[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Project Name",
    "section": "",
    "text": "SL",
    "crumbs": [
      "Problem Definition"
    ]
  },
  {
    "objectID": "index.html#the-why",
    "href": "index.html#the-why",
    "title": "Project Name",
    "section": "The Why",
    "text": "The Why\n(High-level description of the business challenges and client pain points)\n\nProblem Details\nSed elementum convallis quam, sed tempor massa dictum sit amet. Phasellus ultricies ante id massa scelerisque interdum. Vestibulum vitae volutpat felis. Sed metus magna, malesuada vitae odio eu, volutpat aliquam odio. Mauris eget purus ex. Praesent nec gravida lorem. Nam rhoncus bibendum nulla at viverra. Curabitur at diam sem. Pellentesque semper venenatis lorem quis pharetra. Cras venenatis consectetur ante vitae mattis. Etiam in augue vel nunc euismod sodales vitae eu arcu.\nMaecenas tempus ultricies sapien, porta suscipit est facilisis quis. Ut imperdiet massa condimentum sapien lobortis, dictum eleifend enim cursus. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Aenean interdum vel velit non dictum. Suspendisse bibendum neque ut nulla condimentum, quis dictum sem consectetur. Integer quis arcu sem. Sed condimentum dolor sed libero posuere, sed tristique nulla cursus. In hac habitasse platea dictumst. Aenean ullamcorper condimentum risus, at semper tortor consequat eu.\nPellentesque a semper nisl, a vehicula libero. Mauris aliquam porttitor nibh ut porttitor. Praesent eu sem lacinia, volutpat dolor id, interdum enim. Etiam sit amet urna rhoncus, iaculis ex hendrerit, suscipit urna. Duis id porta massa, ac ultrices nulla. Vestibulum ut rutrum lacus, ac vulputate libero. Sed metus massa, maximus ac vulputate nec, lacinia sit amet felis.\n\n\nAdditional Context\nSed elementum convallis quam, sed tempor massa dictum sit amet. Phasellus ultricies ante id massa scelerisque interdum. Vestibulum vitae volutpat felis. Sed metus magna, malesuada vitae odio eu, volutpat aliquam odio. Mauris eget purus ex. Praesent nec gravida lorem. Nam rhoncus bibendum nulla at viverra. Curabitur at diam sem. Pellentesque semper venenatis lorem quis pharetra. Cras venenatis consectetur ante vitae mattis. Etiam in augue vel nunc euismod sodales vitae eu arcu.\nMaecenas tempus ultricies sapien, porta suscipit est facilisis quis. Ut imperdiet massa condimentum sapien lobortis, dictum eleifend enim cursus. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Aenean interdum vel velit non dictum. Suspendisse bibendum neque ut nulla condimentum, quis dictum sem consectetur. Integer quis arcu sem. Sed condimentum dolor sed libero posuere, sed tristique nulla cursus. In hac habitasse platea dictumst. Aenean ullamcorper condimentum risus, at semper tortor consequat eu.\nPellentesque a semper nisl, a vehicula libero. Mauris aliquam porttitor nibh ut porttitor. Praesent eu sem lacinia, volutpat dolor id, interdum enim. Etiam sit amet urna rhoncus, iaculis ex hendrerit, suscipit urna. Duis id porta massa, ac ultrices nulla. Vestibulum ut rutrum lacus, ac vulputate libero. Sed metus massa, maximus ac vulputate nec, lacinia sit amet felis.",
    "crumbs": [
      "Problem Definition"
    ]
  },
  {
    "objectID": "src/qse/01_installation.html",
    "href": "src/qse/01_installation.html",
    "title": "Installation",
    "section": "",
    "text": "import { Tabs, TabItem } from ‘@astrojs/starlight/components’;\n:::note[Useful links]\nhttps://www.ibm.com/docs/en/quantum-safe/quantum-safe-explorer/1.0.x?topic=explorer-installing-quantum-safe-macos\n:::",
    "crumbs": [
      "QSE",
      "Installation"
    ]
  },
  {
    "objectID": "src/qse/01_installation.html#pre-reqs",
    "href": "src/qse/01_installation.html#pre-reqs",
    "title": "Installation",
    "section": "Pre-Reqs",
    "text": "Pre-Reqs\nFor any installation, in order to use the VSCode plugin, you need to install VSCode.\n\nOS: Windows 10 or 11 or MacOS\nRAM: 16 Gigs\nOpenJDK 17\nGit Bash (Windows)\n\n\nInstalling VSCode\n  Grab VSCode for windows from here\nMake sure you download the system installer\n\n\n\nWindows1\n\n\nDouble click it and install it to this following path\n\n\n\nWindows2\n\n\n\n\n\nWindows3\n\n\n \nGrab VSCode for MacOS from here\n\n\n\nMacOS1\n\n\nFile will be located in your Downloads folder.\n \n\n\nInstalling OpenJDK\n \nMicrosoft’s OpenJDK 17 can be found here\nDownload it here\nGo to your Downloads folder and double click the microsoft-jdk-17.0.11-windows-x64.msi file there.\nLet it set the JAVA_HOME var and click Next then click Install.\n\n\n\nWindowsJDK\n\n\n \nMake sure you have openjdk-17 installed.\nbrew install openjdk@17\nIf you have other versions of Java installed, you might need to add this to your path:\necho 'export PATH=\"/opt/homebrew/opt/openjdk@17/bin:$PATH\"' &gt;&gt; ~/.zshrc\n\n. ~/.zshrc",
    "crumbs": [
      "QSE",
      "Installation"
    ]
  },
  {
    "objectID": "src/qse/01_installation.html#downloading-qse",
    "href": "src/qse/01_installation.html#downloading-qse",
    "title": "Installation",
    "section": "Downloading QSE",
    "text": "Downloading QSE\nYou can find the required installation files for your OS of choice on IBM’s Passport Advantage. Or if you’re an IBM Employee, you can find it here.\n\n\n\nQSE1\n\n\nIt’s important to note that these files all use the same name\n \n\n\n\nQSE2\n\n\nExtract them all to the same folder\n\n\n\nQSE3\n\n\nFile listing\nPS C:\\Users\\Administrator\\Downloads\\IIQSE0_1.0.1_MP_EN&gt; tree /F\nFolder PATH listing\nVolume serial number is 1EA9-8239\nC:.\n├───M0GT5EN\n│       ibm-quantum-safe-explorer-1.0.1.vsix\n│\n├───M0GT6EN\n│   │   cli.sh\n│   │   LicenseAcceptance.config\n│   │   version\n│   │\n│   ├───la_home\n│   │       LA_cs\n│   │       LA_de\n│   │       LA_el\n│   │       LA_en\n│   │       LA_es\n│   │       LA_fr\n│   │       LA_in\n│   │       LA_it\n│   │       LA_ja\n│   │       LA_ko\n│   │       LA_lt\n│   │       LA_pl\n│   │       LA_pt\n│   │       LA_ru\n│   │       LA_sl\n│   │       LA_tr\n│   │       LA_zh\n│   │       LA_zh_TW\n│   │       notices\n│   │\n│   ├───lib\n│   │       apiguardian-api-1.1.2.jar\n│   │       commons-cli-1.5.0.jar\n│   │       commons-codec-1.15.jar\n│   │       commons-collections4-4.4.jar\n│   │       commons-compress-1.21.jar\n│   │       commons-io-2.11.0.jar\n│   │       commons-lang3-3.12.0.jar\n│   │       commons-math3-3.6.1.jar\n│   │       commons-text-1.10.0.jar\n│   │       curvesapi-1.07.jar\n│   │       gson-2.10.1.jar\n│   │       hamcrest-core-1.1.jar\n│   │       json-simple-1.1.1.jar\n│   │       jsoup-1.16.1.jar\n│   │       junit-4.10.jar\n│   │       junit-platform-commons-1.9.0.jar\n│   │       license_metric_logger-2.1.2.202009181115.jar\n│   │       log4j-api-2.18.0.jar\n│   │       log4j-core-2.20.0.jar\n│   │       poi-5.2.3.jar\n│   │       poi-ooxml-5.2.3.jar\n│   │       poi-ooxml-lite-5.2.3.jar\n│   │       quantum-safe-sca-tng-0.0.1-SNAPSHOT.jar\n│   │       slf4j-api-1.7.36.jar\n│   │       SparseBitSet-1.2.jar\n│   │       sqlite-jdbc-3.42.0.0.jar\n│   │       tika-core-2.4.1.jar\n│   │       xmlbeans-5.1.1.jar\n│   │\n│   └───swidtag\n│           ibm.com_IBM_Quantum_Safe_Explorer-1.0.1.swidtag\n│\n└───M0GT7EN\n        IBM Quantum Safe Explorer.exe\n\n\n-rw-r--r--@    1 kramerro  staff    35M Jun 26 15:59 IIQSE0_1.0.1_MP_EN(2).zip\n-rw-r--r--@    1 kramerro  staff    73M Jun 26 15:58 IIQSE0_1.0.1_MP_EN.zip\n-rw-r--r--@    1 kramerro  staff   446K Jun 26 15:58 IIQSE0_1.0.1_MP_EN(1).zip\nUnzip them all to the same folder\nFile listing\n➜  QSE git:(ross-updates) ✗ tree\n.\n├── M0GT4EN\n│   └── IBM Quantum Safe Explorer-1.0.1.pkg\n├── M0GT5EN\n│   └── ibm-quantum-safe-explorer-1.0.1.vsix\n└── M0GT6EN\n    ├── LicenseAcceptance.config\n    ├── cli.sh\n    ├── la_home\n    │   ├── LA_cs\n    │   ├── LA_de\n    │   ├── LA_el\n    │   ├── LA_en\n    │   ├── LA_es\n    │   ├── LA_fr\n    │   ├── LA_in\n    │   ├── LA_it\n    │   ├── LA_ja\n    │   ├── LA_ko\n    │   ├── LA_lt\n    │   ├── LA_pl\n    │   ├── LA_pt\n    │   ├── LA_ru\n    │   ├── LA_sl\n    │   ├── LA_tr\n    │   ├── LA_zh\n    │   ├── LA_zh_TW\n    │   └── notices\n    ├── lib\n    │   ├── SparseBitSet-1.2.jar\n    │   ├── apiguardian-api-1.1.2.jar\n    │   ├── commons-cli-1.5.0.jar\n    │   ├── commons-codec-1.15.jar\n    │   ├── commons-collections4-4.4.jar\n    │   ├── commons-compress-1.21.jar\n    │   ├── commons-io-2.11.0.jar\n    │   ├── commons-lang3-3.12.0.jar\n    │   ├── commons-math3-3.6.1.jar\n    │   ├── commons-text-1.10.0.jar\n    │   ├── curvesapi-1.07.jar\n    │   ├── gson-2.10.1.jar\n    │   ├── hamcrest-core-1.1.jar\n    │   ├── json-simple-1.1.1.jar\n    │   ├── jsoup-1.16.1.jar\n    │   ├── junit-4.10.jar\n    │   ├── junit-platform-commons-1.9.0.jar\n    │   ├── license_metric_logger-2.1.2.202009181115.jar\n    │   ├── log4j-api-2.18.0.jar\n    │   ├── log4j-core-2.20.0.jar\n    │   ├── poi-5.2.3.jar\n    │   ├── poi-ooxml-5.2.3.jar\n    │   ├── poi-ooxml-lite-5.2.3.jar\n    │   ├── quantum-safe-sca-tng-0.0.1-SNAPSHOT.jar\n    │   ├── slf4j-api-1.7.36.jar\n    │   ├── sqlite-jdbc-3.42.0.0.jar\n    │   ├── tika-core-2.4.1.jar\n    │   └── xmlbeans-5.1.1.jar\n    ├── swidtag\n    │   └── ibm.com_IBM_Quantum_Safe_Explorer-1.0.1.swidtag\n    └── version\n\n7 directories, 53 files",
    "crumbs": [
      "QSE",
      "Installation"
    ]
  },
  {
    "objectID": "src/qse/01_installation.html#installing-the-services",
    "href": "src/qse/01_installation.html#installing-the-services",
    "title": "Installation",
    "section": "Installing the Services",
    "text": "Installing the Services\n \nPS C:\\Users\\Administrator\\Downloads\\IIQSE0_1.0.1_MP_EN&gt; cd .\\M0GT7EN\\\nPS C:\\Users\\Administrator\\Downloads\\IIQSE0_1.0.1_MP_EN\\M0GT7EN&gt;\n\nPS C:\\Users\\Administrator\\Downloads\\IIQSE0_1.0.1_MP_EN\\M0GT7EN&gt; & '.\\IBM Quantum Safe Explorer.exe'\nThe following window will pop up. Go with all the defaults.\n\n\n\nQSEwindows1\n\n\nWhen installation is complete, the following window will pop up. Click Finish and allow it to start up the service.\n\n\n\nQSEwindows2\n\n\nClick Accept \nOnce it’s running, don’t close the window.\n\n\n\nQSEwindows4\n\n\n\n\nIn our extracted directory, run the following command with sudo and enter your admin password when prompted.\nsudo installer -pkg M0GT4EN/IBM\\ Quantum\\ Safe\\ Explorer-1.0.1.pkg -target /Applications\nPassword:\ninstaller: Package name is IBM Quantum Safe Explorer\ninstaller: Installing at base path /\ninstaller: The install was successful.\nLocate the desktop shortcut for IBM Quantum Safe Explorer and kick it off.\n\n\n\nQSEmacos1\n\n\nYou should see the following windows pop up:  Make sure to click Accept\nThe QSE service should now be running.\n\n\n\nQSEmacos3\n\n\n\nDo not close this window as apparently it DOES NOT continue running in the background.",
    "crumbs": [
      "QSE",
      "Installation"
    ]
  },
  {
    "objectID": "src/qse/01_installation.html#installing-the-ibm-quantum-safe-explorer-cli",
    "href": "src/qse/01_installation.html#installing-the-ibm-quantum-safe-explorer-cli",
    "title": "Installation",
    "section": "Installing the IBM Quantum Safe Explorer CLI",
    "text": "Installing the IBM Quantum Safe Explorer CLI\n \nSimply change directory to where ever you extracted the extension. The cli.sh file will be used to perform the actual scan. Assuming you unzipped from your \\Downloads directory:\nPS C:\\Users\\Administrator\\Downloads\\IIQSE0_1.0.1_MP_EN&gt; cd .\\M0GT6EN\\\n \nSimply change directory to wherever you extracted the extension. The cli.sh file will be used to perform the actual scan. Assuming you unzipped from your ~/Downloads directory:\ncd ~/Downloads/M0GT6EN",
    "crumbs": [
      "QSE",
      "Installation"
    ]
  },
  {
    "objectID": "src/qse/01_installation.html#installing-vscode-plugin",
    "href": "src/qse/01_installation.html#installing-vscode-plugin",
    "title": "Installation",
    "section": "Installing VSCode plugin",
    "text": "Installing VSCode plugin\n\nIf using MacOS, make sure you have the code cli for MacOS enabled. This is how you do that\n\nChange directory to where ever you extracted the extension\n \nPS C:\\Users\\Administrator\\Downloads\\IIQSE0_1.0.1_MP_EN&gt; cd .\\M0GT5EN\\\nInstall the extension\nPS C:\\Users\\Administrator\\Downloads\\IIQSE0_1.0.1_MP_EN\\M0GT5EN&gt; code --install-extension ibm-quantum-safe-explorer-1.0.1.vsix\nInstalling extensions...\nExtension 'ibm-quantum-safe-explorer-1.0.1.vsix' was successfully installed.\n \ncd M0GT5EN\nInstall the extension\ncode --install-extension ibm-quantum-safe-explorer-1.0.1.vsix \nInstalling extensions...\nExtension 'ibm-quantum-safe-explorer-1.0.1.vsix' was successfully installed.\n \nIn your running VSCode instance, click the Extensions icon in the left-hand side bar and select IBM Quantum Safe Explorer\n\n\n\nExtension1\n\n\nClick on the settings for the extension \nPer the current guide, the following settings must be configured:\n\nCurrently as of 1.0.x only localhost is supported as qs-explorer.hostname.\n\nThe following configuration settings should be followed:\nRequired settings:\n\nqs-explorer.hostname – Enter the hostname of the IBM Quantum Safe Explorer API server. Currently only localhost is supported.\nqs-explorer.sourceFileExtensions – Enter the list of source file types to be scanned. For example, .java, .py, and so on.\n\nsourceFileExtensions would be edited in the setting json. They should currently default to the following supported extensions for code scanning:\n    \"qs-explorer.sourceFileExtensions\": [\n        \".java\",\n        \".jar\",\n        \".dart\",\n        \".py\",\n        \".c\"\n    ]",
    "crumbs": [
      "QSE",
      "Installation"
    ]
  },
  {
    "objectID": "src/qse/01_installation.html#uninstalling-the-services",
    "href": "src/qse/01_installation.html#uninstalling-the-services",
    "title": "Installation",
    "section": "Uninstalling the Services",
    "text": "Uninstalling the Services\n\nThere is a bug in v1.0.1 uninstall on Windows that will later cause issues when v2.x is installed, please note the instructions below.\n\nFor Windows, after running the normal “Remove Programs” to uninstall the QSE service, manually delete the qs-explorer folder from the user’s home directory. If this is not done and v2.x is later installed, it will cause the scanning to fail.\nThis is only an issue with the v1.0.1 uninstallation on Windows. This issue has been resolved in the v2.x uninstallation.",
    "crumbs": [
      "QSE",
      "Installation"
    ]
  },
  {
    "objectID": "src/implementation_methodology/stepone-imp.html",
    "href": "src/implementation_methodology/stepone-imp.html",
    "title": "Step One",
    "section": "",
    "text": "Step One Implementation - The How\n(Details the process and techniques used to execute the technical solution)"
  },
  {
    "objectID": "src/qsr/adaptive_proxy/02_configuration.html",
    "href": "src/qsr/adaptive_proxy/02_configuration.html",
    "title": "Configuration",
    "section": "",
    "text": "import { Tabs, TabItem } from ‘@astrojs/starlight/components’;",
    "crumbs": [
      "QSR",
      "Adaptive Proxy",
      "Configuration"
    ]
  },
  {
    "objectID": "src/qsr/adaptive_proxy/02_configuration.html#browser-configuration",
    "href": "src/qsr/adaptive_proxy/02_configuration.html#browser-configuration",
    "title": "Configuration",
    "section": "Browser configuration",
    "text": "Browser configuration\nFor the exercises below, the browser is the client side of the proxied connection. Each browser type can be configured to support quantum-safe kyber encryption. Chrome and Edge support kyber encryption by default, however Firefox does not.\nDepending on the scenario, it could be desirable to either enable or disable kyber encryption for the browser. For example, if the scenario is a legacy encryption client connecting to a quantum-safe kyber encrypted application or site, then the kyber encryption would need to be disabled on the browser. The vice versa would also be true for a scenario where kyber encrypted clients are connecting to a legacy encrypted application or site.\nFor configuration instructions, see Enable Post Quantum Cryptography Support in Web Browsers\n\nAs of Dec 2024, only Chromium-based browsers work in the kyber encrypted front end scenario. Use Chrome (properly configured) for kyber encrypted client connection to the Adaptive Proxy and use Firefox for a legacy encrypted client connection.\n\n\nConfiguring Chrome\nFor quantum-safe kyber encryption through the Adaptive Proxy, you must disable the use of ML-KEM in TLS 1.3 in Chrome. In a chrome tab, open chrome://flags and look for “Use ML-KEM in TLS 1.3”. Select “Disabled” and relaunch the browser.\n\n\n\n41f0f37b-b4dc-4e59-a894-2b7d102a7574\n\n\nThe “TLS 1.3 post-quantum key agreement” flag can remain “Default”, as default enables that flag in Chrome.",
    "crumbs": [
      "QSR",
      "Adaptive Proxy",
      "Configuration"
    ]
  },
  {
    "objectID": "src/qsr/adaptive_proxy/02_configuration.html#adaptive-proxy-configuration",
    "href": "src/qsr/adaptive_proxy/02_configuration.html#adaptive-proxy-configuration",
    "title": "Configuration",
    "section": "Adaptive proxy configuration",
    "text": "Adaptive proxy configuration\nThe adaptive proxy configuration consists of the following files:\n\nadaptive.proxy.location.conf\nadaptive.proxy.upstream.conf\nadaptive.proxy.curvemap\nadaptive.proxy.passthrough.conf",
    "crumbs": [
      "QSR",
      "Adaptive Proxy",
      "Configuration"
    ]
  },
  {
    "objectID": "src/qsr/adaptive_proxy/02_configuration.html#example-setup",
    "href": "src/qsr/adaptive_proxy/02_configuration.html#example-setup",
    "title": "Configuration",
    "section": "Example setup",
    "text": "Example setup\n\nBuild a test site\nSome of the below was pulled out of How To Create a Self-Signed SSL Certificate for Apache in Ubuntu 16.04\n\nPrerequisites\nOn our Linux host (Ubuntu 24.0.4 LTS), let’s install httpd and npm, install corepack, enable yarn, and configure the firewall.\nsudo apt update\n\nsudo apt-get -y install apache2\n\ncurl -fsSL https://deb.nodesource.com/setup_20.x | sudo bash -\n\nsudo apt-get install -y nodejs\n\nsudo corepack enable\n\n\nEnabling mod_ssl\nEnable mod_ssl with the a2enmod command:\nsudo a2enmod ssl\nRestart Apache to activate the module:\nsudo systemctl restart apache2\n\n\nCreate the SSL Certificate\nCreate the SSL key and certificate files with the openssl command:\nsudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/ssl/private/apache-selfsigned.key -out /etc/ssl/certs/apache-selfsigned.crt\nThe entirety of the prompts will look something like this:\nCountry Name (2 letter code) [XX]:US\nState or Province Name (full name) []:Example\nLocality Name (eg, city) [Default City]:Example \nOrganization Name (eg, company) [Default Company Ltd]:Example Inc\nOrganizational Unit Name (eg, section) []:Example Dept\nCommon Name (eg, your name or your server's hostname) []:your_domain_or_ip\nEmail Address []:webmaster@example.com\n\n\nBuild the test site\nYou can build a basic Hello World site or a more complex test site using Astro.\n \nNow let’s build a basic Hello World page for Apache.\nCreate a directory to act as the document root.\nsudo mkdir -p /var/www/html/test-app/build\nOpen a new index.html file with your text editor.\nsudo vi /var/www/html/test-app/build/index.html\nPaste the following into the blank file:\n&lt;h1&gt;Hello world!&lt;/h1&gt;\n \nNow let’s build an Astro site with a Starlight template.\n:::note[Internet access] The following requires access to various public websites. If you are using a proxy that is restrictive, you may run into connection issues. :::\nsudo yarn create astro --install --no-git --typescript relaxed --template starlight /var/www/html/test-app\nThis will create a directory called /var/www/html/test-app\nGo into test-app and update the astro.config.mjs with the following:\nimport { defineConfig } from 'astro/config';\nimport starlight from '@astrojs/starlight';\n\n// https://astro.build/config\nexport default defineConfig({\n    outDir: './build',\n    build: {\n        assets: 'css'  \n    }, \n    integrations: [\n        starlight({\n            title: 'My Docs',\n            social: {\n                github: 'https://github.com/withastro/starlight',\n            },\n            sidebar: [\n                {\n                    label: 'Guides',\n                    items: [\n                        // Each item here is one entry in the navigation menu.\n                        { label: 'Example Guide', slug: 'guides/example' },\n                    ],\n                },{\n                    label: 'Reference',\n                    autogenerate: { directory: 'reference' },\n                },\n            ],\n        }),\n    ],\n});\nNow run the build\nsudo yarn build\n\nThis will build a viable site in /var/www/html/test-app/build  \n\n\nUpdate apache config\nCreate a file called /etc/apache2/sites-available/test-site.conf\nsudo vi /etc/apache2/sites-available/test-site.conf\nSince we’re running this in IBM Fyre (private cloud), we’re going to use the FQDN of our host here. But please use your own host name here that was used to create the self-signed certificate above.\nListen 8080\n\n&lt;VirtualHost *:8080&gt;\n    DocumentRoot \"/var/www/html/test-app/build\"\n    ServerName apache-host01.fyre.ibm.com\n\n    SSLEngine on\n    SSLCertificateFile /etc/ssl/certs/apache-selfsigned.crt\n    SSLCertificateKeyFile /etc/ssl/private/apache-selfsigned.key\n&lt;/VirtualHost&gt;\nEnable the new config\ncd /etc/apache2/sites-enabled\n\nsudo ln -s ../sites-available/test-site.conf\nNow restart apache2\nsudo systemctl restart apache2\n\nVerify the site is now up by navigating to https://apache-host01.fyre.ibm.com:8080.\n\n\n\nQSRAP-config1\n\n\n\nIf you created the “Hello world” test site then you will see a simple site that says “Hello world!” instead of the “Welcome to Starlight” page shown above.\n\nWe set our test site to port 8080.\n\n\n\nDeploy the proxy\nAfter following the instructions to install the Adaptive Proxy here\nEdit the following file /opt/adaptive-proxy/workdir/adaptive.proxy.location.conf\n:::note[On NGINX configuration]\nBy default, the best encryption Apache supports is TLSv1.2, so be sure to include proxy_ssl_protocols TLSv1.2 in the config as shown below.\nThe docker container running Adaptive Proxy will resolve the host name in proxy_pass through the network DNS. It will not use the parent host /etc/hosts file to resolve that host name. So if that name can be pinged without the use of an /etc/hosts update, then it will work. Otherwise, use the IP address instead. :::\n# IBM Confidential\n# PID: 5900-B8I\n# Copyright (c) IBM Corp. 2024\n\n# Nginx Documentation: https://nginx.org/en/docs/http/ngx_http_core_module.html\n# Location directive documentation: https://nginx.org/en/docs/http/ngx_http_core_module.html#location\n# file path in server: /etc/nginx/conf.d/location.conf\n\nlocation ^~ / {\n    proxy_pass https://apache-host01.fyre.ibm.com:8080/;\n    proxy_ssl_protocols TLSv1.2;\n    proxy_set_header Host $host;\n    proxy_set_header X-Real-IP $remote_addr;\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n    proxy_set_header X-Forwarded-Proto $scheme;\n}\n\nStart up the adaptive proxy\ncd /opt/adaptive-proxy/workdir\n\n./runAdaptiveProxyServer.sh\nProxy should come up and our test site should be available now\n\n\n\nQSRAP-config2\n\n\n\nIf you created the “Hello world” test site then you will see a simple site that says “Hello world!” instead of the “Welcome to Starlight” page shown above.\n\nIn our case, we’re pointing the proxy_pass to the FQDN virtual host we configured on our web server.\n:::note[How it works]\nThe magic is the PORT. You must go to the URL at port 8443 which is where the proxy listens.\nIn our case, to get to the proxied site:\nhttps://dockerbot21.fyre.ibm.com:8443/\ndockerbot21 is our docker host in fyre. His FQDN is dockerbot21.fyre.ibm.com\nOur apache host is apache-host01.fyre.ibm.com\n:::\n\n\nVerifying the encryption algorithms\n\nIf you created the “Hello world” test site then you will see a simple site that says “Hello world!” instead of the “Welcome to Starlight” page shown above, but the instructions will be the same.\n\nNow that our site is up and proxied, let’s verify that it’s using a Quantum Safe algorithm for encryption.\n\nCurrently there is no PQC encryption available for any TLS certs available from any legitimate Certificate Authority.\n\nIn a Chrome brower, go to the site and click the three dots.\n\n\n\nQSRAP-config3\n\n\nOpen the Developer Tools\n\n\n\nQSRAP-config4\n\n\nClick the Security panel\n\n\n\nQSRAP-config5\n\n\nHere we can see that X25519Kyber768Draft00 is one of the encryption algorithms and that’s Quantum Safe.",
    "crumbs": [
      "QSR",
      "Adaptive Proxy",
      "Configuration"
    ]
  },
  {
    "objectID": "src/qsr/adaptive_proxy/02_configuration.html#diagram-of-the-proxied-site",
    "href": "src/qsr/adaptive_proxy/02_configuration.html#diagram-of-the-proxied-site",
    "title": "Configuration",
    "section": "Diagram of the Proxied Site",
    "text": "Diagram of the Proxied Site\nd2 sketch pad=50 direction: left dockerbot1: \"dockerbot21:8443\\n(adaptive proxy)\\nTLS encryption\\n happens here\" {   style.fill: transparent   style.font-size: 12   style.font: mono   width: 141   height: 95 } Image: \"\" {   icon: https://icons.terrastruct.com/aws/_General/Users_light-bg.svg   shape: image } Image -&gt; dockerbot1: TLS Square: \"apache-host01\\n(example site)\\nNo TLS\" {   style.fill: transparent   style.font-size: 12   style.font: mono   width: 141   height: 95 } dockerbot1 -&gt; Square: \"No-TLS\\n\" Square -&gt; dockerbot1: \"No-TLS\\n\" dockerbot1 -&gt; Image: TLS",
    "crumbs": [
      "QSR",
      "Adaptive Proxy",
      "Configuration"
    ]
  },
  {
    "objectID": "src/qsr/adaptive_proxy/02_configuration.html#troubleshooting",
    "href": "src/qsr/adaptive_proxy/02_configuration.html#troubleshooting",
    "title": "Configuration",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nAdaptive Proxy docker container will not start\nRun docker ps to ensure that the AP container is up and running. If it’s not check the local logs directory for error.log. There you will see any error messages from the docker container.\nIf you see the following message, it means that from within the docker container, the host name of your site cannot be resolved.\n[emerg] 1#1: host not found in upstream \"quantum-site\" in /etc/nginx/conf.d/adaptive.proxy.location.conf:18\nTo resolve, update adaptive.proxy.location.conf and replace the host name in proxy_pass to use the IP address of the site instead. Start AP again.",
    "crumbs": [
      "QSR",
      "Adaptive Proxy",
      "Configuration"
    ]
  },
  {
    "objectID": "src/qsr/adaptive_proxy/01_installation.html",
    "href": "src/qsr/adaptive_proxy/01_installation.html",
    "title": "Installation",
    "section": "",
    "text": ":::note[Useful links]\nhttps://www.ibm.com/docs/en/quantum-safe/quantum-safe-remediator/1.0.x?topic=proxy-installing-adaptive\n:::",
    "crumbs": [
      "QSR",
      "Adaptive Proxy",
      "Installation"
    ]
  },
  {
    "objectID": "src/qsr/adaptive_proxy/01_installation.html#pre-reqs",
    "href": "src/qsr/adaptive_proxy/01_installation.html#pre-reqs",
    "title": "Installation",
    "section": "Pre-Reqs",
    "text": "Pre-Reqs\n\nFor Fedora, make sure the following packages are installed for cron.\nsudo dnf -y install cronie\n\n\nInstalling Docker\nOn our gateway host we are running Fedora 40 Server:\nsudo dnf remove docker \\\n                  docker-client \\\n                  docker-client-latest \\\n                  docker-common \\\n                  docker-latest \\\n                  docker-latest-logrotate \\\n                  docker-logrotate \\\n                  docker-selinux \\\n                  docker-engine-selinux \\\n                  docker-engine\n\nSet up the repository\nsudo dnf -y install dnf-plugins-core\nsudo dnf config-manager --add-repo https://download.docker.com/linux/fedora/docker-ce.repo\nInstall Docker Engine, containerd, and Docker Compose\nsudo dnf -y install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\nAdd your local user to the docker group\nsudo usermod -G docker,wheel kramerro\nStart up docker\nsudo systemctl start docker\n\n\nInstalling jq\nInstall jq v1.7+\nsudo dnf -y install jq\n\n\nInstalling Java\nInstall sdkman\ncurl -s \"https://get.sdkman.io\" | sudo bash\nLogin as root to source the environment\nsudo su -\nInstall Java 21 (as root)\nsdk install java 21.0.2-open\n\n\nInstalling the proxy\nDownload the Adaptive Proxy from IBM Passport Advantage. If you are an IBM employee you can download internally here. Search for IBM Quantum Safe Remediator Adaptive Proxy.\nOnce downloaded, the file should be named IQSRAP_&lt;version&gt;_Linux_EN.zip\nExtract to your gateway host\n[kramerro@eksa-admin ~]$ unzip IQSRAP_&lt;version&gt;_Linux_EN.zip\nArchive:  IQSRAP_&lt;version&gt;_Linux_EN.zip\n  inflating: M0J90EN/adaptiveproxy_&lt;version&gt;.tar\nMake the adaptive proxy directory in /opt and extract the tar file there\nsudo tar xvf M0J90EN/adaptiveproxy_&lt;version&gt;.tar -C /opt/adaptive-proxy\nNow run the installer script\ncd /opt/adaptive-proxy/workdir\n\nsudo ./install.sh\nAccept the license agreement and allow the installation to continue\nFor all the algorithms, just hit enter to accept the defaults.\nAfter installation, verify that the certificates have been generated\nll certificates/ca\n\n-rw-r--rw- 1 1012 1012  635 Jun 27 11:30 CA_default.crt\n-rw-----w- 1 1012 1012  227 Jun 27 11:30 CA_default.key\n-rw-r--rw- 1 1012 1012   41 Jun 27 11:30 CA_default.srl\n-rw-r--rw- 1 1012 1012 5685 Jun 27 11:30 CA_hybrid.crt\n-rw-----w- 1 1012 1012 5458 Jun 27 11:30 CA_hybrid.key\n-rw-r--rw- 1 1012 1012   41 Jun 27 11:30 CA_hybrid.srl\n-rw-r--rw- 1 1012 1012  631 Jun 27 11:30 CA_legacy.crt\n-rw-----w- 1 1012 1012  227 Jun 27 11:30 CA_legacy.key\n-rw-r--rw- 1 1012 1012   41 Jun 27 11:30 CA_legacy.srl\n-rw-r--rw- 1 1012 1012 5502 Jun 27 11:30 CA_qsc.crt\n-rw-----w- 1 1012 1012 5295 Jun 27 11:30 CA_qsc.key\n-rw-r--rw- 1 1012 1012   41 Jun 27 11:30 CA_qsc.srl\n-rw-r--rw- 1 1012 1012  114 Jun 27 11:30 server_default.crt\n-rw-r--rw- 1 1012 1012  111 Jun 27 11:30 server_hybrid.crt\n-rw-r--rw- 1 1012 1012  111 Jun 27 11:30 server_legacy.crt\n-rw-r--rw- 1 1012 1012  102 Jun 27 11:30 server_qsc.crt\n\nll certificates/leaf\n-rw-r--rw- 1 1012 1012  895 Jun 27 11:30 server_default.crt\n-rw----rw- 1 1012 1012  227 Jun 27 11:30 server_default.key\n-rw-r--rw- 1 1012 1012 5640 Jun 27 11:30 server_hybrid.crt\n-rw----rw- 1 1012 1012 5458 Jun 27 11:30 server_hybrid.key\n-rw-r--rw- 1 1012 1012  891 Jun 27 11:30 server_legacy.crt\n-rw----rw- 1 1012 1012  227 Jun 27 11:30 server_legacy.key\n-rw-r--rw- 1 1012 1012 5458 Jun 27 11:30 server_qsc.crt\n-rw----rw- 1 1012 1012 5295 Jun 27 11:30 server_qsc.key\nFinally verify the images are available\n$ docker images\nREPOSITORY                                                  TAG       IMAGE ID       CREATED         SIZE\nde.icr.io/qs-remediator/adaptive-proxy-provider-ubi-micro   1.0.1     1f345e88fb04   2 weeks ago     530MB\nde.icr.io/qs-remediator/qsc-openssl-provider                1.0.1     f3fe42d98d81   3 months ago    284MB\nhello-world                                                 latest    d2c94e258dcb   17 months ago   13.3kB\nThe Adaptive Proxy supports two patterns of TLS handling:\n\nedge termination: In an edge scenario, Adaptive Proxy terminates the SSL connection from the client and initiates a new or no SSL connection to the backend/upstream servers.\nre-encrypt: In a re-encrypt scenario, Adaptive Proxy decrypts the client request, then encrypts it again when sending to the backend/upstream using the respective backend/upstream server certificate and keys as per the configuration provided.\n\nWe’ll check them both out and see which one works better for our use case.",
    "crumbs": [
      "QSR",
      "Adaptive Proxy",
      "Installation"
    ]
  },
  {
    "objectID": "src/qsr/performance_test_harness/01_installation.html",
    "href": "src/qsr/performance_test_harness/01_installation.html",
    "title": "Installation",
    "section": "",
    "text": "import { Tabs, TabItem } from ‘@astrojs/starlight/components’;",
    "crumbs": [
      "QSR",
      "Performance Test Harness",
      "Installation"
    ]
  },
  {
    "objectID": "src/qsr/performance_test_harness/01_installation.html#pre-reqs",
    "href": "src/qsr/performance_test_harness/01_installation.html#pre-reqs",
    "title": "Installation",
    "section": "Pre-Reqs",
    "text": "Pre-Reqs\n\nDocker\njq\nbc\n\n\nInstalling pre-requisite packages\n \nsudo dnf -y install jq bc\n \nsudo apt-get -y update\nsudo apt-get -y install jq bc\n \nDownload the Performance Test Pack from IBM Passport Advantage:\n\n\n\nQSRPTH1\n\n\nThe file downloaded will likely be named IQSR_1.0_Linux_EN.zip. Extract the file.\nunzip ~/Downloads/IQSR_1.0_Linux_EN.zip\nArchive:  /Users/kramerro/Downloads/IQSR_1.0_Linux_EN.zip\n  inflating: M0J91EN/pqc-performance-test_1.0.0.tar\nUpload the resultant pqc-performance-test_1.0.0.tar to your docker host.\nOn your docker host\nmkdir -p ~/perftest\ntar -xf pqc-performance-test_1.0.0.tar -C ~/perftest/\n\nChange over to the newly extracted directory\ncd ~/perftest/pqc-performance-test/test_utils/build_images/",
    "crumbs": [
      "QSR",
      "Performance Test Harness",
      "Installation"
    ]
  },
  {
    "objectID": "src/qsr/performance_test_harness/01_installation.html#install-the-performance-test-harness",
    "href": "src/qsr/performance_test_harness/01_installation.html#install-the-performance-test-harness",
    "title": "Installation",
    "section": "Install the Performance Test Harness",
    "text": "Install the Performance Test Harness\n:::note[IBM Docs]\nA lot of this stuff is take verbatim from here:\nhttps://www.ibm.com/docs/en/quantum-safe/quantum-safe-remediator/1.0.x?topic=harness-installing-performance-test\n:::\n\nBuilding the images\nExecute the build_images.sh script and select 1 when prompted for the License Agreement.\nShould show the images available now for Docker ```tsx {6-8} docker images\nREPOSITORY TAG IMAGE ID CREATED SIZE adaptive-proxy ubi-9.3 99c3cd5ad80f 2 months ago 277MB adaptive-proxy qscopenssl 6609ea524cdf 2 months ago 282MB localhost:5000/qsc-h2load-provider-22.04.2-1.1.1t 1.0 5a856162a800 8 months ago 427MB localhost:5000/qsc-curl-provider-9.2.0-1.1.1t 1.0 429552679bba 8 months ago 70.3MB localhost:5000/qsc-openssl-provider-9.2.0-1.1.1t 1.0 8b69ff2c49da 8 months ago 77MB\n\n### Configuring the build images\n\n:::note\nThis build config needs to happen for every host ip you want to check. So if you have multiple addresses, add them to the space delimited list below.\n:::\n\nExecute the `build_config.sh` script. We are going to use our test vm that is running our test site for the adaptive proxy.\nSince this is running on IBM Fyre, we have two IP addresses available. We're going to point to both.\n\nBreaking down the below requirements:\n\n- `TARGET_FQDN_LIST` - Should be a space delimited list of ip addresses of the target hosts\n- `TARGET_IP_LIST` - This should also be the same space delimited list of ips from above \n- `TESTFQDN` - This should be the same ip as the target host\n\nFor our test, we will use the ip of our docker host\n\n```tsx\n./build_config.sh\n\nEnter the target FQDN list seperated by space (TARGET_FQDN_LIST): 9.46.72.93\nEnter the target IP list seperated by space (TARGET_IP_LIST): 9.46.72.93\nEnter the test FQDN (TESTFQDN): 9.46.72.93\nThe build will take a while.\nAfter awhile you should see\nPerformance test server build completed successfully.\nLaunching performance test server ...\nLaunching localhost:5000/qsc-perf-server-ubuntu:provider.\nPerformance test server container started RC=0, container id 318e77d157114d84915654f86cc8c3a225c689e9983bb9d38fbc6b5cbdefc22d.\nBuilding PQC client container ...\nBuilding PQC client container ...\nWhen all is done, you should see the running test harness server container running:\ndocker ps -a\n\nCONTAINER ID   IMAGE                                            COMMAND                  CREATED          STATUS                    PORTS                                                                                                  NAMES\ncae3f96543e0   localhost:5000/qsc-perf-server-ubuntu:provider   \"/bin/bash /opt/ngin…\"   29 seconds ago   Up 5 seconds              0.0.0.0:9100-9563-&gt;9100-9563/tcp, 0.0.0.0:9080-&gt;80/tcp, 0.0.0.0:9043-&gt;443/tcp                          qsc-perf-server",
    "crumbs": [
      "QSR",
      "Performance Test Harness",
      "Installation"
    ]
  },
  {
    "objectID": "src/solution_overview/prepare.html",
    "href": "src/solution_overview/prepare.html",
    "title": "Step One",
    "section": "",
    "text": "Step One Solution - The What\n(Summarize the overarching technical solution. Explain the reasoning and strategic considerations behind the solution)"
  },
  {
    "objectID": "src/solution_overview/troubleshooting.html",
    "href": "src/solution_overview/troubleshooting.html",
    "title": "Step Three",
    "section": "",
    "text": "Step Three Solution"
  },
  {
    "objectID": "src/guardium_insights/04_optional_services.html",
    "href": "src/guardium_insights/04_optional_services.html",
    "title": "Optional Services",
    "section": "",
    "text": "These should be relevant to your environment, therefore clustername should be your clustername and same with your hostname. Below this is an example.\nexport clustername=gi-east\nexport region=us-east-1\nexport NAMESPACE=openshift-marketplace\nexport HOSTNAME=apps.gi.thinkforward.work",
    "crumbs": [
      "Guardium Insights",
      "Optional Services"
    ]
  },
  {
    "objectID": "src/guardium_insights/04_optional_services.html#export-the-following-vars.",
    "href": "src/guardium_insights/04_optional_services.html#export-the-following-vars.",
    "title": "Optional Services",
    "section": "",
    "text": "These should be relevant to your environment, therefore clustername should be your clustername and same with your hostname. Below this is an example.\nexport clustername=gi-east\nexport region=us-east-1\nexport NAMESPACE=openshift-marketplace\nexport HOSTNAME=apps.gi.thinkforward.work",
    "crumbs": [
      "Guardium Insights",
      "Optional Services"
    ]
  },
  {
    "objectID": "src/guardium_insights/04_optional_services.html#optional-install-the-alb-ingress-controller",
    "href": "src/guardium_insights/04_optional_services.html#optional-install-the-alb-ingress-controller",
    "title": "Optional Services",
    "section": "(Optional) Install the ALB ingress controller",
    "text": "(Optional) Install the ALB ingress controller\nUnlike NGINX Ingress, ALB generates a load balancer when a external service is created in the cluster. So DNS is not necessary. See example here.\nDownload an IAM policy for the AWS Load Balancer Controller that allows it to make calls to AWS APIs on your behalf.\n\nAs of this writing, the latest version of AWS Load Balancer Controller is v2.9.0\n\ncurl -O https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.9.0/docs/install/iam_policy.json\nCreate an IAM policy using the policy downloaded in the previous step.\naws iam create-policy \\\n    --policy-name AWSLoadBalancerControllerIAMPolicy \\\n    --tags '{\"Key\": \"Product\",\"Value\": \"Guardium\"}' \\\n    --policy-document file://iam_policy.json\nShould return\n{\n    \"Policy\": {\n        \"PolicyName\": \"AWSLoadBalancerControllerIAMPolicy\",\n        \"PolicyId\": \"ANPA3WENOYSATHNEI5OIR\",\n        \"Arn\": \"arn:aws:iam::&lt;ACCOUNT ID&gt;:policy/AWSLoadBalancerControllerIAMPolicy\",\n        \"Path\": \"/\",\n        \"DefaultVersionId\": \"v1\",\n        \"AttachmentCount\": 0,\n        \"PermissionsBoundaryUsageCount\": 0,\n        \"IsAttachable\": true,\n        \"CreateDate\": \"2024-09-30T21:08:03+00:00\",\n        \"UpdateDate\": \"2024-09-30T21:08:03+00:00\",\n        \"Tags\": [\n            {\n                \"Key\": \"Product\",\n                \"Value\": \"Guardium\"\n            }\n        ]\n    }\n}\n\nLet’s export that policy arn as another env var\nexport alb_policy_arn=$(aws iam list-policies --query 'Policies[?PolicyName==`AWSLoadBalancerControllerIAMPolicy`].Arn' --output text)\nExport the role name as a env var. We’re going to append the cluster name to the role name to help identify it in AWS and in case we have multiple clusters in the same account.\nexport alb_role_name=AWSLoadBalancerControllerRole-${clustername}\nCreate a Kubernetes service account named aws-load-balancer-controller in the kube-system namespace for the AWS Load Balancer Controller and annotate the Kubernetes service account with the name of the IAM role.\neksctl create iamserviceaccount \\\n    --cluster ${clustername} \\\n    --namespace kube-system \\\n    --name aws-load-balancer-controller \\\n    --role-name ${alb_role_name} \\\n    --attach-policy-arn ${alb_policy_arn} \\\n    --tags \"Product=Guardium\" \\\n    --approve \\\n    --region ${region}\n\nNow let’s use helm to install the AWS Load Balancer Controller\nInstall the helm repo\nhelm repo add eks https://aws.github.io/eks-charts\nhelm repo update\nNow install the ALB controller\nhelm install aws-load-balancer-controller eks/aws-load-balancer-controller \\\n  -n kube-system \\\n  --set clusterName=${clustername} \\\n  --set serviceAccount.create=false \\\n  --set serviceAccount.name=aws-load-balancer-controller \n\nShould return something like this\nNAME: aws-load-balancer-controller\nLAST DEPLOYED: Mon Sep 30 17:21:43 2024\nNAMESPACE: kube-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nAWS Load Balancer controller installed!\nVerify the ingress class has been created\nkubectl get ingressclass\nshould return\ntsx {2} NAME    CONTROLLER             PARAMETERS   AGE alb     ingress.k8s.aws/alb    &lt;none&gt;       79s nginx   k8s.io/ingress-nginx   &lt;none&gt;       35d\n{/*",
    "crumbs": [
      "Guardium Insights",
      "Optional Services"
    ]
  },
  {
    "objectID": "src/guardium_insights/04_optional_services.html#use-aws-ca-acm-for-certs-optional",
    "href": "src/guardium_insights/04_optional_services.html#use-aws-ca-acm-for-certs-optional",
    "title": "Optional Services",
    "section": "Use AWS CA (ACM) for certs (optional)",
    "text": "Use AWS CA (ACM) for certs (optional)\nIf you plan to use a private CA via AWS ACM instead of something like LetsEncrypt by default, follow these steps\nThe AWS PrivateCA Issuer plugin acts as an addon (see external cert configuration) to cert-manager that signs certificate requests using ACM Private CA.\n\nCreate the namespace\nkubectl create namespace aws-pca-issuer\nAdd the helm repo\nhelm repo add awspca https://cert-manager.github.io/aws-privateca-issuer\nhelm repo update\nhelm install awspca/aws-privateca-issuer --generate-name --namespace aws-pca-issuer\n\n\nCreate the private CA in ACM\n\nThis is using an example from this helpful AWS link\nGenerating and installing the CA cert here\n\n\nIn our example we are using the subdomain apps.gi.thinkforward.work. This will not be the domain you will likely use.\n\nCreate the following file and populate it with appropriate values. This file is an EXAMPLE:\ncat &lt;&lt;EOF &gt; ca_config.txt\n{\n   \"KeyAlgorithm\":\"RSA_2048\",\n   \"SigningAlgorithm\":\"SHA256WITHRSA\",\n   \"Subject\":{\n      \"Country\":\"US\",\n      \"Organization\":\"IBM\",\n      \"OrganizationalUnit\":\"Client Engineering\",\n      \"State\":\"MA\",\n      \"Locality\":\"Cambridge\",\n      \"CommonName\":\"$HOSTNAME\"\n   }\n}\nEOF\nWe’re going to use the default OCSP settings. Now create a revocation file\ncat &lt;&lt;EOF &gt; revoke_config.txt\n{\n   \"OcspConfiguration\":{\n      \"Enabled\":true\n   }\n}\nEOF\nFinally, create the CA\naws acm-pca create-certificate-authority \\\n     --certificate-authority-configuration file://ca_config.txt \\\n     --revocation-configuration file://revoke_config.txt \\\n     --certificate-authority-type \"ROOT\" \\\n     --idempotency-token 01234567 \\\n     --region $region \\\n     --tags Key=Product,Value=Guardium\nThis should return the new CA’s arn.\n{\n    \"CertificateAuthorityArn\": \"arn:aws:acm-pca:us-east-1:748107796891:certificate-authority/00c9d3eb-c837-4cef-917a-fd87ec608a04\"\n}\nLet’s collect that arn into an env var\nexport ca_arn=\"arn:aws:acm-pca:us-east-1:748107796891:certificate-authority/00c9d3eb-c837-4cef-917a-fd87ec608a04\"\n\n\nGenerate and install the CA cert\nGenerate a certificate signing request (CSR).\naws acm-pca get-certificate-authority-csr \\\n     --certificate-authority-arn $ca_arn \\\n     --output text \\\n     --region $region &gt; ca.csr\nUsing the CSR from the previous step as the argument for the –csr parameter, issue the root certificate and export the arn as another env var.\nWe’re going to set the validity to 999 days.\nexport root_cert_arn=$(aws acm-pca issue-certificate \\\n     --certificate-authority-arn $ca_arn \\\n     --csr fileb://ca.csr \\\n     --signing-algorithm SHA256WITHRSA \\\n     --region $region \\\n     --template-arn arn:aws:acm-pca:::template/RootCACertificate/V1 \\\n     --validity Value=999,Type=DAYS \\\n     --output text)\nRetrieve the cert and let’s stick it in cert.pem\naws acm-pca get-certificate \\\n    --certificate-authority-arn $ca_arn \\\n    --certificate-arn $root_cert_arn \\\n  --region $region \\\n    --output text &gt; cert.pem\nFinally import it into the CA\naws acm-pca import-certificate-authority-certificate \\\n    --region $region \\\n    --certificate-authority-arn $ca_arn \\\n    --certificate fileb://cert.pem\nVerify the status of the CA\naws acm-pca describe-certificate-authority \\\n  --region $region \\\n    --certificate-authority-arn $ca_arn \\\n  --query \"CertificateAuthority.Status\" \\\n    --output text\nThis should return ACTIVE.\n\n\nCreate an ACM Private CA\nDownload the CA certificate using the following command.\naws acm-pca get-certificate-authority-certificate \\\n--certificate-authority-arn $ca_arn \\\n--region $region \\\n--output text &gt; cacert.pem\n\n\nSet EKS node permission for ACM Private CA\nAdd the IAM policy to your EKS NodeInstanceRole\nCreate the following file\ncat &lt;&lt;EOF &gt; EKS-CA-NodePerms-policy.json\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"awspcaissuer\",\n            \"Action\": [\n                \"acm-pca:DescribeCertificateAuthority\",\n                \"acm-pca:GetCertificate\",\n                \"acm-pca:IssueCertificate\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"$ca_arn\"\n        }\n    ]\n}\nEOF\nCreate the policy in IAM. This needs to be specific for your cluster using the CA we created.\nExport the following var:\nexport awscapolicy=\"AWSCAPolicy-${clustername}\"\naws iam create-policy \\\n--policy-name ${awscapolicy} \\\n--policy-document file://EKS-CA-NodePerms-policy.json\nLet’s retrieve the policy arn as an env var\nexport ca_policy_arn=$(aws iam list-policies --query 'Policies[?PolicyName==`'${awscapolicy}'`].Arn' --output text)\nLet’s gather our NodeInstance role name\nexport node_instance_role=$(aws iam list-roles --region $region --query 'Roles[?starts_with(RoleName, `eksctl-'${clustername}'`)]|[?contains(RoleName, `NodeInstanceRole`)].RoleName' --output text)\nNow let’s attach the policy to our EKS node role. In our case the role name is gi-eks-nodes-role. This may be different for you.\naws iam attach-role-policy \\\n--role-name \"${node_instance_role}\" \\\n--policy-arn \"${ca_policy_arn}\" \\\n--region \"${region}\"\n\n\nCreate an issuer\nWe’re going to create a namespace scoped issuer for now.\nCreate a yaml file for the issuer\ncat &lt;&lt;EOF &gt; awpca-issuer.yaml\napiVersion: awspca.cert-manager.io/v1beta1\nkind: AWSPCAIssuer\nmetadata:\n  name: acm-issuer\n  namespace: $NAMESPACE\nspec:\n  arn: $ca_arn\n  region: $region\nEOF\nDeploy the AWSPCAIssuer:\nkubectl apply -f awpca-issuer.yaml\n*/}",
    "crumbs": [
      "Guardium Insights",
      "Optional Services"
    ]
  },
  {
    "objectID": "src/guardium_insights/04_optional_services.html#community-cert-manager-optional",
    "href": "src/guardium_insights/04_optional_services.html#community-cert-manager-optional",
    "title": "Optional Services",
    "section": "Community Cert Manager (optional)",
    "text": "Community Cert Manager (optional)\n\nInstalling community Cert Manager\nInstall the helm repo\nhelm repo add jetstack https://charts.jetstack.io\nCreate the namespace\nkubectl create ns cert-manager\nNow install the Community Cert Manager\nhelm install cert-manager jetstack/cert-manager \\\n--namespace cert-manager \\\n--set installCRDs=true \\\n--version v1.16.1",
    "crumbs": [
      "Guardium Insights",
      "Optional Services"
    ]
  },
  {
    "objectID": "src/guardium_insights/04_optional_services.html#configure-security-constraints-optional",
    "href": "src/guardium_insights/04_optional_services.html#configure-security-constraints-optional",
    "title": "Optional Services",
    "section": "Configure Security Constraints (Optional)",
    "text": "Configure Security Constraints (Optional)\nFor some environments there might be security constraints applied to the EKS cluster. Follow these instructions to apply security constraints using OPA gatekeeper.\nIf the operating environment is a production or pre-production enterprise environment, then you can skip this section as it’s meant for development environments.\n\nInstall OPA Gatekeeper\nDeploy OPA Gatekeeper using prebuilt docker images\n\nBy default, the violations audit runs every 60 seconds and will report a maximum of 20 audits in the constraint yaml. If you want to increase the number of violations shown on the constraint yaml to 500 (not recommended to increase further), update the gatekeeper.yaml below before applying and add the following argument to the audit deployment.\nyaml {17}   template:     metadata:       labels:         control-plane: audit-controller         gatekeeper.sh/operation: audit         gatekeeper.sh/system: \"yes\"     spec:       automountServiceAccountToken: true       containers:       - args:         - --operation=audit         - --operation=status         - --operation=mutation-status         - --logtostderr         - --disable-opa-builtin={http.send}         - --disable-cert-rotation         - --constraint-violations-limit=500\n\nkubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper/release-3.17/deploy/gatekeeper.yaml\nCheck the pods in gatekeeper-system namespace\nkubectl get pods -n gatekeeper-system\nThe output will be similar to:\nNAME                                             READY   STATUS    RESTARTS   AGE\ngatekeeper-audit-5bc9b59c57-9d9hc                1/1     Running   0          25s\ngatekeeper-controller-manager-744cdc8556-hxf2n   1/1     Running   0          25s\ngatekeeper-controller-manager-744cdc8556-jn42m   1/1     Running   0          25s\ngatekeeper-controller-manager-744cdc8556-wwrb6   1/1     Running   0          25s\n\n\nInstall kustomize\nTo ease in the deployment of the templates and constraints, install kustomize.\nThe following script detects your OS and downloads the appropriate kustomize binary to your current working directory.\ncurl -s \"https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\"  | bash\n\n\nApply Templates\nThere are 2 parts to OPA gatekeeper policy enforcment, the first part is the ConstraintTemplate. This is where the logic of the policy exists. The templates must be deployed prior to deploying the constraints.\nIf you haven’t yet, clone this github repository.\ngit clone https://github.com/ibm-client-engineering/engineering-journal-quantum-safe.git\nGo to the opa library templates directory.\ncd engineering-journal-quantum-safe/opa/library/templates\nApply the templates using kustomize\nkustomize build . | kubectl apply -f -\nSample output\nconstrainttemplate.templates.gatekeeper.sh/k8sallowedrepos created\nconstrainttemplate.templates.gatekeeper.sh/k8sblockendpointeditdefaultrole created\nconstrainttemplate.templates.gatekeeper.sh/k8sblockloadbalancer created\nconstrainttemplate.templates.gatekeeper.sh/k8sblocknodeport created\nconstrainttemplate.templates.gatekeeper.sh/k8sblockwildcardingress created\nconstrainttemplate.templates.gatekeeper.sh/k8sdisallowanonymous created\nconstrainttemplate.templates.gatekeeper.sh/k8sdisallowedrepos created\nconstrainttemplate.templates.gatekeeper.sh/k8sdisallowedtags created\nconstrainttemplate.templates.gatekeeper.sh/k8sdisallowinteractivetty created\nconstrainttemplate.templates.gatekeeper.sh/k8sexternalips created\nconstrainttemplate.templates.gatekeeper.sh/k8shttpsonly created\nconstrainttemplate.templates.gatekeeper.sh/k8simagedigests created\nconstrainttemplate.templates.gatekeeper.sh/k8spoddisruptionbudget created\nconstrainttemplate.templates.gatekeeper.sh/k8spspautomountserviceaccounttokenpod created\nconstrainttemplate.templates.gatekeeper.sh/k8srequiredresources created\nconstrainttemplate.templates.gatekeeper.sh/k8sstorageclass created\nconstrainttemplate.templates.gatekeeper.sh/k8suniqueingresshost created\nconstrainttemplate.templates.gatekeeper.sh/k8suniqueserviceselector created\n\n\nApply Constraints\nThe 2nd part to OPA gatekeeper policy enforcment is the Constraint. The constraints are a specific application of a template. This instructs the OPA gatekeeper to constrain a resource according to specific parameters and applying one of the templates. For example, a constraint can specify that every namespace must have a specific label applied.\nPart of a constraint definition is the enforcement action. By default, OPA gatekeeper will enforce constraints by denying the action. For the constraints here, we are using dryrun, which does not deny the resources that violate constraints, but rather just logs the violation. This allows us to see what resources are violating the constraints without the resources being denied.\nGo to the opa library constraints directory.\ncd engineering-journal-quantum-safe/opa/library/constraints\nApply the templates using kustomize\nkustomize build . | kubectl apply -f -\nSample output\nk8sallowedrepos.constraints.gatekeeper.sh/repo-is-amazonaws created\nk8sallowedrepos.constraints.gatekeeper.sh/repo-is-openpolicyagent created\nk8sblockendpointeditdefaultrole.constraints.gatekeeper.sh/block-endpoint-edit-default-role created\nk8sblockloadbalancer.constraints.gatekeeper.sh/block-load-balancer created\nk8sblocknodeport.constraints.gatekeeper.sh/block-node-port created\nk8sblockwildcardingress.constraints.gatekeeper.sh/block-wildcard-ingress created\nk8sdisallowanonymous.constraints.gatekeeper.sh/no-anonymous-no-authenticated created\nk8sdisallowinteractivetty.constraints.gatekeeper.sh/no-interactive-tty-containers created\nk8sdisallowedrepos.constraints.gatekeeper.sh/repo-must-not-be-k8s-gcr-io created\nk8sdisallowedtags.constraints.gatekeeper.sh/container-image-must-not-have-latest-tag created\nk8sexternalips.constraints.gatekeeper.sh/external-ips created\nk8shttpsonly.constraints.gatekeeper.sh/ingress-https-only created\nk8simagedigests.constraints.gatekeeper.sh/container-image-must-have-digest created\nk8spspautomountserviceaccounttokenpod.constraints.gatekeeper.sh/psp-automount-serviceaccount-token-pod created\nk8spoddisruptionbudget.constraints.gatekeeper.sh/pod-distruption-budget created\nk8srequiredresources.constraints.gatekeeper.sh/container-must-have-limits-and-requests created\nk8suniqueingresshost.constraints.gatekeeper.sh/unique-ingress-host created\n\n\nList Violations\nUse the following command to list the violations.\nkubectl get constraints\nSample output\nNAME                                                                                     ENFORCEMENT-ACTION   TOTAL-VIOLATIONS\nk8srequiredresources.constraints.gatekeeper.sh/container-must-have-limits-and-requests   dryrun               4\n\nNAME                                                                 ENFORCEMENT-ACTION   TOTAL-VIOLATIONS\nk8suniqueingresshost.constraints.gatekeeper.sh/unique-ingress-host   dryrun               0\n\nNAME                                                                                ENFORCEMENT-ACTION   TOTAL-VIOLATIONS\nk8sdisallowinteractivetty.constraints.gatekeeper.sh/no-interactive-tty-containers   dryrun               0\n\nNAME                                                        ENFORCEMENT-ACTION   TOTAL-VIOLATIONS\nk8shttpsonly.constraints.gatekeeper.sh/ingress-https-only   dryrun               0\n\nNAME                                                                                         ENFORCEMENT-ACTION   TOTAL-VIOLATIONS\nk8sblockendpointeditdefaultrole.constraints.gatekeeper.sh/block-endpoint-edit-default-role   dryrun               0\n\nNAME                                                                       ENFORCEMENT-ACTION   TOTAL-VIOLATIONS\nk8sdisallowedrepos.constraints.gatekeeper.sh/repo-must-not-be-k8s-gcr-io   dryrun               0\n\nNAME                                                                                   ENFORCEMENT-ACTION   TOTAL-VIOLATIONS\nk8sdisallowedtags.constraints.gatekeeper.sh/container-image-must-not-have-latest-tag   dryrun               0\n\nNAME                                                         ENFORCEMENT-ACTION   TOTAL-VIOLATIONS\nk8sblocknodeport.constraints.gatekeeper.sh/block-node-port   dryrun               0\n\n\nViolation Details\nTo see the details of the violations, you can look at the constraint yaml. For example, to see the container-must-have-limits-and-requests constraint violations, run the following command.\nkubectl get k8srequiredresources container-must-have-limits-and-requests -o yaml\nNear the end of the yaml output, you will see violations listed.\n  violations:\n  - enforcementAction: dryrun\n    group: \"\"\n    kind: Pod\n    message: container &lt;manager&gt; does not have &lt;{\"cpu\"}&gt; limits defined\n    name: gatekeeper-controller-manager-865cc64485-kgkkj\n    namespace: gatekeeper-system\n    version: v1\n  - enforcementAction: dryrun\n    group: \"\"\n    kind: Pod\n    message: container &lt;manager&gt; does not have &lt;{\"cpu\"}&gt; limits defined\n    name: gatekeeper-controller-manager-865cc64485-h2x7d\n    namespace: gatekeeper-system\n    version: v1\n  - enforcementAction: dryrun\n    group: \"\"\n    kind: Pod\n    message: container &lt;manager&gt; does not have &lt;{\"cpu\"}&gt; limits defined\n    name: gatekeeper-controller-manager-865cc64485-g4b72\n    namespace: gatekeeper-system\n    version: v1\n  - enforcementAction: dryrun\n    group: \"\"\n    kind: Pod\n    message: container &lt;manager&gt; does not have &lt;{\"cpu\"}&gt; limits defined\n    name: gatekeeper-audit-c794d5f69-s2nmd\n    namespace: gatekeeper-system\n    version: v1\nIf you want a CSV of all of the violations (up to the constraints violation limit set at install), run the following command. Note that the header will be repeated.\nkubectl get constraints -o json | yq -o=json eval '.items[].status.violations' | jq -r '(.[0] | keys_unsorted) as $keys | $keys, map([.[ $keys[] ]])[] | @csv' &gt; violations.csv 2&gt;/dev/null",
    "crumbs": [
      "Guardium Insights",
      "Optional Services"
    ]
  },
  {
    "objectID": "src/guardium_insights/04_optional_services.html#configure-opa-mutations",
    "href": "src/guardium_insights/04_optional_services.html#configure-opa-mutations",
    "title": "Optional Services",
    "section": "Configure OPA Mutations",
    "text": "Configure OPA Mutations\n\nIf you are deploying to an EKS environment that is airgapped and using a private registry, the following mutations would need to be configured for the environment. This is not applicable to an Openshift environment as that can use an ImageContentSourcePolicy\n\nThese mutations essentially work like an Openshift ImageContentSourcePolicy and rewrite the domain paths in pods.\nExport the registry domain. This should be the url of your private registry. my.registry.io is an example.\nexport myprivatereg=my.registry.io\nApply the mutations\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: mutations.gatekeeper.sh/v1alpha1\nkind: AssignImage\nmetadata:\n  name: assign-container-domain\nspec:\n  applyTo:\n  - groups: [ \"\" ]\n    kinds: [ \"Pod\" ]\n    versions: [ \"v1\" ]\n  location: \"spec.containers[name:*].image\"\n  parameters:\n    assignDomain: \"${myprivatereg}\"\n  match:\n    source: \"All\"\n    scope: Namespaced\n    namespaces:\n      - openshift-marketplace\n      - ibm-cert-manager\n    kinds:\n    - apiGroups: [ \"*\" ]\n      kinds: [ \"Pod\" ]\n---\napiVersion: mutations.gatekeeper.sh/v1alpha1\nkind: AssignImage\nmetadata:\n  name: assign-initcontainer-domain\nspec:\n  applyTo:\n  - groups: [ \"\" ]\n    kinds: [ \"Pod\" ]\n    versions: [ \"v1\" ]\n  location: \"spec.initContainers[name:*].image\"\n  parameters:\n    assignDomain: \"${myprivatereg}\"\n  match:\n    source: \"All\"\n    scope: Namespaced\n    namespaces:\n      - openshift-marketplace\n      - ibm-cert-manager\n    kinds:\n    - apiGroups: [ \"*\" ]\n      kinds: [ \"Pod\" ]\n---\napiVersion: mutations.gatekeeper.sh/v1alpha1\nkind: AssignImage\nmetadata:\n  name: assign-ephemeralcontainer-domain\nspec:\n  applyTo:\n  - groups: [ \"\" ]\n    kinds: [ \"Pod\" ]\n    versions: [ \"v1\" ]\n  location: \"spec.ephemeralContainers[name:*].image\"\n  parameters:\n    assignDomain: \"${myprivatereg}\"\n  match:\n    source: \"All\"\n    scope: Namespaced\n    namespaces:\n      - openshift-marketplace\n      - ibm-cert-manager\n    kinds:\n    - apiGroups: [ \"*\" ]\n      kinds: [ \"Pod\" ]\nEOF",
    "crumbs": [
      "Guardium Insights",
      "Optional Services"
    ]
  },
  {
    "objectID": "src/guardium_insights/eks/02_cluster-build.html",
    "href": "src/guardium_insights/eks/02_cluster-build.html",
    "title": "EKS Cluster build",
    "section": "",
    "text": "Links\n\n\n\nSome of these steps come from this documentation for Guardium Insights on EKS\nRun the eksctl command below to create your first cluster and perform the following:",
    "crumbs": [
      "Guardium Insights",
      "EKS",
      "EKS Cluster Build"
    ]
  },
  {
    "objectID": "src/guardium_insights/eks/02_cluster-build.html#creating-the-cluster",
    "href": "src/guardium_insights/eks/02_cluster-build.html#creating-the-cluster",
    "title": "EKS Cluster build",
    "section": "Creating the cluster",
    "text": "Creating the cluster\n\n\n\n\n\n\nOn cluster names\n\n\n\nWe’re deploying in us-east-1 and we are naming our cluster gi-east. This name will be exclusive to this cluster, so gi-east is an example here. You aren’t bound to using gi-east as your name.\n\n\nLet’s set some env vars to make our lives easier.\nexport clustername=gi-east\nexport region=us-east-1\neksctl create cluster \\\n--name ${clustername} \\\n--version 1.31 \\\n--region ${region} \\\n--zones ${region}a,${region}b \\\n--nodegroup-name guardium-workers \\\n--node-type m6i.4xlarge \\\n--with-oidc \\\n--nodes 5 \\\n--nodes-min 5 \\\n--nodes-max 6 \\\n--node-zones ${region}a \\\n--tags \"Product=Guardium\" \\\n--managed\nAssociate an IAM oidc provider with the cluster if you didn’t include --with-oidc above.\neksctl utils associate-iam-oidc-provider --region=${region} --cluster=${clustername} --approve",
    "crumbs": [
      "Guardium Insights",
      "EKS",
      "EKS Cluster Build"
    ]
  },
  {
    "objectID": "src/guardium_insights/eks/02_cluster-build.html#configure-kubectl",
    "href": "src/guardium_insights/eks/02_cluster-build.html#configure-kubectl",
    "title": "EKS Cluster build",
    "section": "Configure kubectl",
    "text": "Configure kubectl\nOnce the cluster is up, add it to your kube config. eksctl will probably do this for you.\naws eks update-kubeconfig --name ${clustername} --region ${region}\n\n\n\n\n\n\nDeploying Optional OPA Security Constraints\n\n\n\nIf you plan to use the Gatekeeper OPA on your cluster, follow the instructions here",
    "crumbs": [
      "Guardium Insights",
      "EKS",
      "EKS Cluster Build"
    ]
  },
  {
    "objectID": "src/guardium_insights/eks/02_cluster-build.html#install-the-ebs-driver-to-the-cluster",
    "href": "src/guardium_insights/eks/02_cluster-build.html#install-the-ebs-driver-to-the-cluster",
    "title": "EKS Cluster build",
    "section": "Install the EBS driver to the cluster",
    "text": "Install the EBS driver to the cluster\nWe install the EBS CSI driver as this gives us access to GP3 block storage.\nDownload the example ebs iam policy\ncurl -o iam-policy-example-ebs.json https://raw.githubusercontent.com/kubernetes-sigs/aws-ebs-csi-driver/master/docs/example-iam-policy.json\nCreate the policy. You can change AmazonEKS_EBS_CSI_Driver_Policy to a different name, but if you do, make sure to change it in later steps too.\naws iam create-policy \\\n--policy-name AmazonEKS_EBS_CSI_Driver_Policy \\\n--tags '{\"Key\": \"Product\",\"Value\": \"Guardium\"}' \\\n--policy-document file://iam-policy-example-ebs.json\nOutput should be similar to below\n{\n    \"Policy\": {\n        \"PolicyName\": \"AmazonEKS_EBS_CSI_Driver_Policy\",\n        \"PolicyId\": \"ANPA24LVTCGN5YOUAVX2V\",\n        \"Arn\": \"arn:aws:iam::&lt;ACCOUNT ID&gt;:policy/AmazonEKS_EBS_CSI_Driver_Policy\",\n        \"Path\": \"/\",\n        \"DefaultVersionId\": \"v1\",\n        \"AttachmentCount\": 0,\n        \"PermissionsBoundaryUsageCount\": 0,\n        \"IsAttachable\": true,\n        \"CreateDate\": \"2023-04-19T14:17:03+00:00\",\n        \"UpdateDate\": \"2023-04-19T14:17:03+00:00\",\n        \"Tags\": [\n            {\n                \"Key\": \"Product\",\n                \"Value\": \"Guardium\"\n            }\n        ]\n    }\n}\n\nLet’s export the returned arn as a env VAR for further use\nexport ebs_driver_policy_arn=$(aws iam list-policies --query 'Policies[?PolicyName==`AmazonEKS_EBS_CSI_Driver_Policy`].Arn' --output text)\nNow let’s export the rolename we are going to create as a env var. We’re going to append the cluster name to the role to differentiate in case we have multiple clusters in this account. You can share policies, but you cannot share roles.\nexport ebs_driver_role_name=AmazonEKS_EBS_CSI_DriverRole-${clustername}\nCreate the service account\neksctl create iamserviceaccount \\\n  --name ebs-csi-controller-sa \\\n  --namespace kube-system \\\n  --cluster ${clustername} \\\n  --attach-policy-arn $ebs_driver_policy_arn \\\n  --approve \\\n  --region=${region} \\\n  --tags \"Product=Guardium\" \\\n  --role-only \\\n  --role-name ${ebs_driver_role_name}\nLet’s export the created role arn as another env var\nexport ebs_driver_role_arn=$(aws iam list-roles --query 'Roles[?RoleName==`AmazonEKS_EBS_CSI_DriverRole-'$clustername'`].Arn' --output text)\nCreate the addon for the cluster\neksctl create addon \\\n--name aws-ebs-csi-driver \\\n--cluster ${clustername} \\\n--service-account-role-arn $ebs_driver_role_arn \\\n--region=${region} \\\n--force\nCreate the following StorageClass yaml to use gp3\ncat &lt;&lt;EOF |kubectl apply -f -\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: ebs-gp3-sc\nprovisioner: ebs.csi.aws.com\nparameters:\n  type: gp3\n  fsType: ext4\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\nEOF\n\nVerifying EBS\nRun the following command\nkubectl apply -f - &lt;&lt;EOF \napiVersion: v1 \nkind: PersistentVolumeClaim \nmetadata: \n  name: block-pvc \nspec: \n  storageClassName: ebs-gp3-sc\n  accessModes: \n    - ReadWriteOnce \n  resources: \n    requests: \n      storage: 1Gi \n--- \napiVersion: v1 \nkind: Pod \nmetadata: \n  name: mypod \nspec: \n  containers: \n    - name: myfrontend \n      image: nginx \n      volumeMounts: \n        - mountPath: \"/var/www/html\" \n          name: mypd \n  volumes: \n    - name: mypd \n      persistentVolumeClaim: \n        claimName: block-pvc \nEOF\nVerify the PVC was created\nkubectl get pvc\nNAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE\nblock-pvc   Bound    pvc-67193212-3e10-46ab-a0a4-2834e3560c4a   1Gi        RWO            ebs-gp3-sc     &lt;unset&gt;                 7s\nYou should see the bound status of the above pvc.\nDelete the test pod and pvc\nkubectl delete -f - &lt;&lt;EOF\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: block-pvc\nspec:\n  storageClassName: ebs-gp3-sc\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  containers:\n    - name: myfrontend\n      image: nginx\n      volumeMounts:\n        - mountPath: \"/var/www/html\"\n          name: mypd\n  volumes:\n    - name: mypd\n      persistentVolumeClaim:\n        claimName: block-pvc\nEOF",
    "crumbs": [
      "Guardium Insights",
      "EKS",
      "EKS Cluster Build"
    ]
  },
  {
    "objectID": "src/guardium_insights/eks/02_cluster-build.html#enable-efs-on-the-cluster",
    "href": "src/guardium_insights/eks/02_cluster-build.html#enable-efs-on-the-cluster",
    "title": "EKS Cluster build",
    "section": "Enable EFS on the cluster",
    "text": "Enable EFS on the cluster\nBy default when we create a cluster with eksctl it defines and installs gp2 storage class which is backed by Amazon’s EBS (elastic block storage). After that we installed the EBS CSI driver to support gp3. However, both gp2 and gp3 are both block storage. They will not support RWX in our cluster. We need to install an EFS storage class.\n\nCreate IAM policy\nDownload the IAM policy document from GitHub. You can also view the policy document\ncurl -o iam-policy-example-efs.json https://raw.githubusercontent.com/kubernetes-sigs/aws-efs-csi-driver/master/docs/iam-policy-example.json\nCreate the policy. You can change AmazonEKS_EFS_CSI_Driver_Policy to a different name, but if you do, make sure to change it in later steps too.\naws iam create-policy \\\n--policy-name AmazonEKS_EFS_CSI_Driver_Policy \\\n--tags '{\"Key\": \"Product\",\"Value\": \"Guardium\"}' \\\n--policy-document file://iam-policy-example-efs.json\nOutput should be similar to below\n{\n    \"Policy\": {\n        \"PolicyName\": \"AmazonEKS_EFS_CSI_Driver_Policy\",\n        \"PolicyId\": \"ANPA3WENOYSA6LSFRSZ6U\",\n        \"Arn\": \"arn:aws:iam::803455550593:policy/AmazonEKS_EFS_CSI_Driver_Policy\",\n        \"Path\": \"/\",\n        \"DefaultVersionId\": \"v1\",\n        \"AttachmentCount\": 0,\n        \"PermissionsBoundaryUsageCount\": 0,\n        \"IsAttachable\": true,\n        \"CreateDate\": \"2024-08-23T19:57:13+00:00\",\n        \"UpdateDate\": \"2024-08-23T19:57:13+00:00\",\n        \"Tags\": [\n            {\n                \"Key\": \"Product\",\n                \"Value\": \"Guardium\"\n            }\n        ]\n    }\n}\nLet’s export that policy arn as another env var\nexport efs_driver_policy_arn=$(aws iam list-policies --query 'Policies[?PolicyName==`AmazonEKS_EFS_CSI_Driver_Policy`].Arn' --output text)\n\n\nCreate IAM role\nNow let’s export the rolename we are going to create as a env var. We’re going to append the cluster name to the role to differentiate in case we have multiple clusters in this account. You can share policies, but you cannot share roles.\nexport efs_driver_role_name=AmazonEKS_EFS_CSI_DriverRole-${clustername}\nCreate an IAM role and attach the IAM policy to it. Annotate the Kubernetes service account with the IAM role ARN and the IAM role with the Kubernetes service account name.\neksctl create iamserviceaccount \\\n    --cluster ${clustername} \\\n    --namespace kube-system \\\n    --name efs-csi-controller-sa \\\n    --role-name ${efs_driver_role_name} \\\n    --attach-policy-arn $efs_driver_policy_arn \\\n    --tags \"Product=Guardium\" \\\n    --approve \\\n    --region ${region}\nOnce created, check the iam service account is created running the following command.\neksctl get iamserviceaccount --cluster ${clustername} --region ${region}\nShould return\nNAMESPACE   NAME            ROLE ARN\nkube-system ebs-csi-controller-sa   arn:aws:iam::803455550593:role/AmazonEKS_EBS_CSI_DriverRole\nkube-system efs-csi-controller-sa   arn:aws:iam::803455550593:role/AmazonEKS_EFS_CSI_DriverRole\n\n\nInstall EFS CSI driver\nNow we just need our add-on registry address. This can be found here: https://docs.aws.amazon.com/eks/latest/userguide/add-ons-images.html\n\n\n\n\n\n\nNote\n\n\n\nThe add-on registry address is per region. So based on the URL above, since our region is us-east-1, then our registry address would be 602401143452.dkr.ecr.us-east-1.amazonaws.com/eks/aws-efs-csi-driver\n\n\nLet’s install the driver add-on to our clusters. We’re going to use helm for this.\nhelm repo add aws-efs-csi-driver https://kubernetes-sigs.github.io/aws-efs-csi-driver/\n\nhelm repo update\nInstall a release of the driver using the Helm chart. Replace the repository address with the cluster’s container image address.\nhelm upgrade -i aws-efs-csi-driver aws-efs-csi-driver/aws-efs-csi-driver \\\n    --namespace kube-system \\\n    --set image.repository=602401143452.dkr.ecr.us-east-1.amazonaws.com/eks/aws-efs-csi-driver \\\n    --set controller.serviceAccount.create=false \\\n    --set controller.serviceAccount.name=efs-csi-controller-sa\nVerify that it installed correctly with this command\nkubectl get pod -n kube-system -l \"app.kubernetes.io/name=aws-efs-csi-driver,app.kubernetes.io/instance=aws-efs-csi-driver\"\nShould return something like\nNAME                                  READY   STATUS    RESTARTS   AGE\nefs-csi-controller-7fc77768fc-2swkw   3/3     Running   0          2m13s\nefs-csi-controller-7fc77768fc-c69rb   3/3     Running   0          2m13s\nefs-csi-node-ccxns                    3/3     Running   0          2d17h\nefs-csi-node-k52pt                    3/3     Running   0          2d17h\nefs-csi-node-r8nbm                    3/3     Running   0          2d17h\n\n\nCreate the EFS Filesystem\nNow we need to create the filesystem in EFS so we can use it\nExport the following variables.\nGet our VPC ID\nvpc_id=$(aws eks describe-cluster \\\n    --name $clustername \\\n    --query \"cluster.resourcesVpcConfig.vpcId\" \\\n    --region $region \\\n    --output text)\nRetrieve the CIDR range for your cluster’s VPC and store it in a variable for use in a later step.\ncidr_range=$(aws ec2 describe-vpcs \\\n    --vpc-ids $vpc_id \\\n    --query \"Vpcs[].CidrBlock\" \\\n    --output text \\\n    --region $region)\nCreate a security group with an inbound rule that allows inbound NFS traffic for your Amazon EFS mount points.\nsecurity_group_id=$(aws ec2 create-security-group \\\n    --group-name EFS4FileNetSecurityGroup-${clustername} \\\n    --description \"EFS security group for Guardium Insight cluster ${clustername}\" \\\n    --vpc-id $vpc_id \\\n    --region $region \\\n    --output text)\nCreate an inbound rule that allows inbound NFS traffic from the CIDR for your cluster’s VPC.\naws ec2 authorize-security-group-ingress \\\n    --group-id $security_group_id \\\n    --protocol tcp \\\n    --port 2049 \\\n    --region $region \\\n    --cidr $cidr_range\nCreate a file system.\nfile_system_id=$(aws efs create-file-system \\\n    --region $region \\\n    --encrypted \\\n    --tags '{\"Key\": \"Product\",\"Value\": \"Guardium\"}' \\\n    --performance-mode generalPurpose \\\n    --query 'FileSystemId' \\\n    --output text)\nCreate mount targets.\nDetermine the IDs of the subnets in your VPC and which Availability Zone the subnet is in.\naws ec2 describe-subnets \\\n    --filters \"Name=vpc-id,Values=$vpc_id\" \\\n    --query 'Subnets[*].{SubnetId: SubnetId,AvailabilityZone: AvailabilityZone,CidrBlock: CidrBlock}' \\\n    --region $region \\\n    --output table\nShould output something like the following\n----------------------------------------------------------------------\n|                           DescribeSubnets                          |\n+------------------+--------------------+----------------------------+\n| AvailabilityZone |     CidrBlock      |         SubnetId           |\n+------------------+--------------------+----------------------------+\n|  us-east-1c      |  192.168.64.0/19   |  subnet-08c33dce5e63c82dc  |\n|  us-east-1b      |  192.168.32.0/19   |  subnet-0f7a2b449320cc1e6  |\n|  us-east-1a      |  192.168.0.0/19    |  subnet-0ec499ae3eae19eb0  |\n|  us-east-1b      |  192.168.128.0/19  |  subnet-04f3d465138687333  |\n|  us-east-1a      |  192.168.96.0/19   |  subnet-0bc4d31344c60c113  |\n|  us-east-1c      |  192.168.160.0/19  |  subnet-0bee6fc06187cafd1  |\n+------------------+--------------------+----------------------------+\nAdd mount targets for the subnets that your nodes are in.\nRun the following command:\nfor subnet in $(aws ec2 describe-subnets --filters \"Name=vpc-id,Values=$vpc_id\" --query 'Subnets[*].{SubnetId: SubnetId,AvailabilityZone: AvailabilityZone,CidrBlock: CidrBlock}' --region $region --output text | awk '{print $3}') ; do aws efs create-mount-target --file-system-id $file_system_id --region $region --subnet-id $subnet --security-groups $security_group_id ; done\nThis wraps the below command in a for loop that will iterate through your subnet ids.\naws efs create-mount-target \\\n    --file-system-id $file_system_id \\\n    --region $region \\\n    --subnet-id &lt;SUBNETID&gt; \\\n    --security-groups $security_group_id\n\n\nCreate EFS Storage Class\nCreate a storage class for dynamic provisioning\nLet’s get our filesystem ID if we don’t already have it above. However if you ran the above steps, $file_system_id should already be defined.\naws efs describe-file-systems \\\n--query \"FileSystems[*].FileSystemId\" \\\n--region $region \\\n--output text\n\nfs-071439ffb7e10b67b\n\n\n\n\n\n\nImportant\n\n\n\nIf you did not export the $file_system_id variable, make sure the filesystem id you use in the below command is the filesystem id that was returned to you above!\n\n\nCreate the storage class\ncat &lt;&lt;EOF | envsubst | kubectl apply -f -\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: efs-sc\nparameters:\n  uid: \"0\"\n  gid: \"0\"\n  directoryPerms: \"777\"\n  fileSystemId: ${file_system_id}\n  provisioningMode: efs-ap\nprovisioner: efs.csi.aws.com\nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\nEOF\n\n\n\n\n\n\nSetting Default Storage Class\n\n\n\nSet one of the EFS storage classes as the default storage class only if you intend on primarily using EFS. Otherwise set block storage as default.\nIf using EFS as primary storage\nkubectl patch storageclass efs-sc -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}'\nIf using EBS(block) as primary storage\nkubectl patch storageclass ebs-gp3-sc -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}'\n\n\nFinally, verify they are both there\nkubectl get sc\nNAME               PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\nebs-gp3-sc         ebs.csi.aws.com         Delete          WaitForFirstConsumer   false                  6d18h\nefs-sc (default)   efs.csi.aws.com         Delete          Immediate              false                  16h\ngp2                kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false                  6d23h\n\n\nVerify EFS\nRun the following command to create a pod and a PVC using the default EFS storage class\nkubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: block-pvc\nspec:\n  storageClassName: efs-sc\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  containers:\n    - name: myfrontend\n      image: nginx\n      volumeMounts:\n        - mountPath: \"/var/www/html\"\n          name: mypd\n  volumes:\n    - name: mypd\n      persistentVolumeClaim:\n        claimName: block-pvc\nEOF\nRunning the following command should verify the PVC was successfully created and bound\nkubectl get pvc\nNAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE\nblock-pvc   Bound    pvc-dc711246-02a1-4a9d-b428-a2476b17dd8c   1Gi        RWO            efs-sc         &lt;unset&gt;                 4s\nNow we can delete our test pod and pvc\nkubectl delete -f - &lt;&lt;EOF\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: block-pvc\nspec:\n  storageClassName: efs-sc\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  containers:\n    - name: myfrontend\n      image: nginx\n      volumeMounts:\n        - mountPath: \"/var/www/html\"\n          name: mypd\n  volumes:\n    - name: mypd\n      persistentVolumeClaim:\n        claimName: block-pvc\nEOF",
    "crumbs": [
      "Guardium Insights",
      "EKS",
      "EKS Cluster Build"
    ]
  },
  {
    "objectID": "src/guardium_insights/eks/02_cluster-build.html#install-nginx-controller",
    "href": "src/guardium_insights/eks/02_cluster-build.html#install-nginx-controller",
    "title": "EKS Cluster build",
    "section": "Install NGINX Controller",
    "text": "Install NGINX Controller\n\n\n\n\n\n\nOn Ingresses\n\n\n\nIf you plan on using AWS ALB for ingress, follow the directions here\n\n\nLet’s install the NGINX helm chart\nhelm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm repo update\nCreate the namespace for NGINX\nkubectl create ns ingress-nginx\n\n\n\n\n\n\nPublic vs Private ingress\n\n\n\nBelow is for a public facing ingress. Yours might need to be internal only so you would set the aws-load-balancer-scheme to internal if it needs to be internal.\n\n\nhelm install ingress-nginx ingress-nginx/ingress-nginx \\\n--set-string controller.service.annotations.'service\\.beta\\.kubernetes\\.io/aws-load-balancer-backend-protocol'=tcp \\\n--set-string controller.service.annotations.'service\\.beta\\.kubernetes\\.io/aws-load-balancer-cross-zone-load-balancing-enabled'=\"true\" \\\n--set-string controller.service.annotations.'service\\.beta\\.kubernetes\\.io/aws-load-balancer-scheme'=internet-facing \\\n--set-string controller.service.annotations.\"service\\.beta\\.kubernetes\\.io/aws-load-balancer-nlb-target-type\"=ip \\\n--set-string controller.service.annotations.'service\\.beta\\.kubernetes\\.io/aws-load-balancer-type'=nlb \\\n--namespace=ingress-nginx\nRun the following command to verify that a load balancer was assigned\nkubectl get service --namespace ingress-nginx ingress-nginx-controller\nNAME                       TYPE           CLUSTER-IP       EXTERNAL-IP                                                                     PORT(S)                      AGE\ningress-nginx-controller   LoadBalancer   10.100.204.215   a904f47d90a8c4961ae389b155359368-aa0f265b8588c710.elb.us-east-1.amazonaws.com   80:30870/TCP,443:31883/TCP   61s\n\nVerify the NGINX deployment\nVerify the deployment\nCommand:\nkubectl get ingressclass\nExample output:\nNAME    CONTROLLER             PARAMETERS   AGE\nnginx   k8s.io/ingress-nginx   &lt;none&gt;       2m43s",
    "crumbs": [
      "Guardium Insights",
      "EKS",
      "EKS Cluster Build"
    ]
  },
  {
    "objectID": "src/guardium_insights/eks/02_cluster-build.html#install-the-operator-lifecycle-manager",
    "href": "src/guardium_insights/eks/02_cluster-build.html#install-the-operator-lifecycle-manager",
    "title": "EKS Cluster build",
    "section": "Install the Operator Lifecycle Manager",
    "text": "Install the Operator Lifecycle Manager\n\n\n\n\n\n\nPermissions for OLM\n\n\n\nThe default method of installing OLM is to use the operator-sdk command line tool. This method creates ClusterRole and ClusterRoleBinding resources that require wildcard permissions at the cluster level.\nIf the security on your cluster is configured to not allow wildcard permissions at the cluster level, proceed with the “Custom OLM” section below to install OLM without wildcard permissions.\n\n\n\noperator-sdkCustom OLM\n\n\nRun the following command using the operator-sdk\noperator-sdk olm install\n\n\nDownload (Save link as…) the customized crds.yaml and olm.yaml for OLM v0.28.0. Keep in mind, this is a point in time customization for GI 3.5.0 and OLM v0.28.0.\nCreate the CRDs\nkubectl create -f crds.yaml\nEnsure that CRDs were applied.\nkubectl wait --for=condition=Established -f crds.yaml\nCreate the OLM resources\nkubectl create -f olm.yaml\nWait for deployments to be ready.\nkubectl rollout status -w deployment/olm-operator --namespace=\"olm\"\nkubectl rollout status -w deployment/catalog-operator --namespace=\"olm\"\n\n\n\nVerify the installation\nkubectl get csv -n olm | grep packageserver\npackageserver   Package Server   0.28.0               Succeeded\nSet the OLM global namespade to use openshift-marketplace\noc set env deploy/catalog-operator GLOBAL_CATALOG_NAMESPACE=openshift-marketplace -n olm\nWe further require to change the global namespace for the packageserver as well. To do so, run the following command:\nkubectl patch csv packageserver -n olm --type='json' -p='[\n  {\n    \"op\": \"replace\",\n    \"path\": \"/spec/install/spec/deployments/0/spec/template/spec/containers/0/command/5\",\n    \"value\": \"openshift-marketplace\"\n  }\n]'\nVerify the global-namespace has been set\nkubectl get deploy packageserver -n olm -o yaml | grep -A 6 \"command:\"\nShould return\n      - command:\n        - /bin/package-server\n        - -v=4\n        - --secure-port\n        - \"5443\"\n        - --global-namespace\n1        - openshift-marketplace\n\n1\n\nThis is the value we care about",
    "crumbs": [
      "Guardium Insights",
      "EKS",
      "EKS Cluster Build"
    ]
  },
  {
    "objectID": "src/guardium_insights/eks/02_cluster-build.html#create-the-required-namespace",
    "href": "src/guardium_insights/eks/02_cluster-build.html#create-the-required-namespace",
    "title": "EKS Cluster build",
    "section": "Create the required Namespace",
    "text": "Create the required Namespace\nkubectl create ns openshift-marketplace\nSet our context to that namespace and export it as an env var\nexport NAMESPACE=openshift-marketplace\n\nkubectl config set-context --current --namespace $NAMESPACE",
    "crumbs": [
      "Guardium Insights",
      "EKS",
      "EKS Cluster Build"
    ]
  },
  {
    "objectID": "src/guardium_insights/eks/02_cluster-build.html#install-the-ibm-cert-manager",
    "href": "src/guardium_insights/eks/02_cluster-build.html#install-the-ibm-cert-manager",
    "title": "EKS Cluster build",
    "section": "Install the IBM Cert Manager",
    "text": "Install the IBM Cert Manager\n\n\n\n\n\n\nOn Cert Managers\n\n\n\nIf you cannot use the IBM Cert Manager, follow the directions here\n\n\nIf you followed the directions here you should have the ibm-guardium-insights case file downloaded and extracted.\nChange to the following directory\ncd ibm-guardium-data-security-center/inventory/install/files/support/eks\nCreate the namespace and then run the installation script\nkubectl create namespace ibm-cert-manager\nchmod +x ibm-cert-manager.sh\n./ibm-cert-manager.sh\nGive it a few minutes and then verify the cert manager is up\nkubectl get po -n ibm-cert-manager\nNAME                                                              READY   STATUS      RESTARTS   AGE\ncd6e1c2b84458a4f431f49f499a919d28af0b23693e36f2fc53bc1f2c3hw5pw   0/1     Completed   0          2m1s\ncert-manager-cainjector-85777f77cc-hbzg7                          1/1     Running     0          102s\ncert-manager-controller-957bc947-kg5wx                            1/1     Running     0          102s\ncert-manager-webhook-5586d798f-lcsln                              1/1     Running     0          102s\nibm-cert-manager-catalog-rcbvc                                    1/1     Running     0          2m12s\nibm-cert-manager-operator-64fc5b4644-rnpqg                        1/1     Running     0          108s\nkubectl get csv -n ibm-cert-manager\nNAME                               DISPLAY            VERSION   REPLACES   PHASE\nibm-cert-manager-operator.v4.2.7   IBM Cert Manager   4.2.7                Succeeded",
    "crumbs": [
      "Guardium Insights",
      "EKS",
      "EKS Cluster Build"
    ]
  },
  {
    "objectID": "src/guardium_insights/eks/02_cluster-build.html#configure-dns-resolution-optional",
    "href": "src/guardium_insights/eks/02_cluster-build.html#configure-dns-resolution-optional",
    "title": "EKS Cluster build",
    "section": "Configure DNS resolution (Optional)",
    "text": "Configure DNS resolution (Optional)\nIf you are following the instrutions in this guide using the “insecure” hostname sections to avoid registering domains, it’s possible to use Route 53 in AWS set up explicit DNS resolution for the specific host routes needed.\nGo to AWS Route 53 and click Hosted zones heading. Then click the Create hosted zone button. This will open the hosted zone form.\nFor the domain use\napps.gi.guardium-insights.com\nFor type, select Private hosted zone\nFor the VPCs to associate with the hosted zone, select the proper region, then select the VPC that was created for your cluster.\n\nThen click Create Hosted zone button to create the hosted zone.\nThis will take you to the hosted zone details page. Now it’s time to create some records for DNS resolution. But first we need the values to use for the internal routing.\nFind the external cluster IP address of the load balancer under EXTERNAL-IP.\nkubectl get service --namespace ingress-nginx ingress-nginx-controller\nNAME                       TYPE           CLUSTER-IP      EXTERNAL-IP                                                                     PORT(S)                      AGE\ningress-nginx-controller   LoadBalancer   10.100.24.243   a091398910dbf4ad8aeb2f3f0e864311-916cbf853e32c1d5.elb.us-east-2.amazonaws.com   80:31369/TCP,443:30370/TCP   20h\nCapture that EXTERNAL-IP value for the next steps.\nBack on the hosted zone details page, click the Create record button.\nWe want to create a CNAME record that routes *.apps.gi.guardium-insights.com to the external cluster IP address captured above.\n\n\n\nimage\n\n\nClick Create Records.",
    "crumbs": [
      "Guardium Insights",
      "EKS",
      "EKS Cluster Build"
    ]
  },
  {
    "objectID": "src/guardium_insights/eks/02_cluster-build.html#install-foundational-services",
    "href": "src/guardium_insights/eks/02_cluster-build.html#install-foundational-services",
    "title": "EKS Cluster build",
    "section": "Install Foundational Services",
    "text": "Install Foundational Services\n\n\n\n\n\n\nEntitlement\n\n\n\nAs noted here make sure you have retrieved your entitlement key for the next step\n\n\nExport your entitlement key as an env var\nexport IBMKEY=\"&lt;entitlement key&gt;\"\nChange to the following directory\ncd ibm-guardium-data-security-center/inventory/install/files/support/eks\nAs of this writing, we are using Guardium Data Security center v3.6.0 which uses bundle version 2.6.0.\nExport the following vars\nexport REPLACE_NAMESPACE=openshift-marketplace\nexport NAMESPACE=openshift-marketplace\nexport ICS_INVENTORY_SETUP=ibmCommonServiceOperatorSetup\nexport ICS_SIZE=starterset\nexport IBMPAK_LAUNCH_SKIP_PREREQ_CHECK=true\nexport CP_REPO_USER=\"cp\"\nexport CP_REPO_PASS=${IBMKEY}\nexport NAMESPACE=\"openshift-marketplace\"\nexport CASE_NAME=ibm-guardium-data-security-center\nexport CASE_VERSION=2.6.0\nexport LOCAL_CASE_DIR=$HOME/.ibm-pak/data/cases/$CASE_NAME/$CASE_VERSION\nSave the CASE bundle locally\noc ibm-pak get $CASE_NAME \\\n--version $CASE_VERSION \\\n--skip-verify\nThis will download the CASE bundle to $HOME/.ibm-pak/data/cases/ibm-guardium-data-security-center/2.6.0\n\n\n\n\n\n\nOn DNS\n\n\n\nWhether you’re using Route 53 or external DNS, a CNAME must be created if you’re using NGINX as your Ingress Controller.\nFor our example setup, we are working with the thinkforward.work domain. So we set the domain in our remote dns to apps.gi.thinkforward.work. Other users may need to configure their FQDN in AWS Route 53.\nThe important part is there must be a wildcard subdomain. It should point to our loadbalancer.\nRetrieve the loadbalancer ip with this command:\nkubectl get service --namespace ingress-nginx ingress-nginx-controller\nShould return bash {2} NAME                       TYPE           CLUSTER-IP       EXTERNAL-IP                                                                     PORT(S)                      AGE ingress-nginx-controller   LoadBalancer   10.100.129.192   k8s-ingressn-ingressn-7e06ec0c6b-01363c775f8b7ff9.elb.us-east-1.amazonaws.com   80:32113/TCP,443:31457/TCP   24h\nOur external IP above is k8s-ingressn-ingressn-7e06ec0c6b-01363c775f8b7ff9.elb.us-east-1.amazonaws.com\nSo for us, our actual DNS record should be like this:\nCNAME    *.gi-east.apps.thinkforward.work  -&gt;  k8s-ingressn-ingressn-7e06ec0c6b-01363c775f8b7ff9.elb.us-east-1.amazonaws.com\nThe * wildcard ensures that any subdomain generated by GI will map back to that loadbalancer.\n\n\nExport the hostname as an env var. It’s important to note that your domain must begin with apps. If you have multiple clusters, a good practices would be apps.&lt;CLUSTERNAME&gt;.&lt;DOMAIN&gt;.\nexport HOSTNAME=apps.gi.thinkforward.work\n\n\n\n\n\n\nUsing an insecure hostname\n\n\n\nFor development purposes only, you can avoid registering a domain name and setting up certificates by using your local hosts file to redirect traffic later in this guide. If taking this route, you can use the following:\nexport HOSTNAME=apps.gi.guardium-insights.com\n\n\nRun the following ibm-pak command to install the Foundational Services catalogs\n\n\n\n\n\n\nPrivate Registries for Catalog Sources\n\n\n\nThis assumes you’ve mirrored all required GI and CPFS images to your private registry. Export your private registry as an env var (my.registry.io is an example.)\nexport myprivatereg=my.registry.io\nSet the following vars:\nexport ARGS=\"--registry ${myprivatereg} --recursive --inputDir ${LOCAL_CASE_DIR}\"\nElse set it to\nexport ARGS=\"--registry icr.io --recursive --inputDir ${LOCAL_CASE_DIR}\"\n\n\n:::cautionRequired for Air Gapped installations You will need to install the Gatekeeper OPA AND configure mutations if using a private registry! :::\noc ibm-pak launch $CASE_NAME \\\n   --version $CASE_VERSION \\\n   --action install-catalog \\\n   --inventory $ICS_INVENTORY_SETUP \\\n   --namespace $NAMESPACE \\\n   --args \"${ARGS}\"\nNow run the following ibm-pak command to install the Foundational Services operators\n\n\n\n\n\n\nPrivate Registries for Foundational Services Operators\n\n\n\nThis assumes you’ve mirrored all required GI and CPFS images to your private registry and exported the $myprivatereg as an env var.\nSet the following vars:\nexport ARGS=\"--size ${ICS_SIZE} --ks_flag true --hostname ${HOSTNAME} --registry ${myprivatereg} --user yourprivreguser --pass yourprivateregpass --secret yoursecretkey --recursive --inputDir ${LOCAL_CASE_DIR}\"\nomit --user, --pass, --secret if you did not set these on your private repo.\nElse set it this to use the public and entitled registries:\nexport ARGS=\"--size ${ICS_SIZE} --ks_flag true --hostname ${HOSTNAME} --registry cp.icr.io --user ${CP_REPO_USER} --pass ${CP_REPO_PASS} --secret ibm-entitlement-key --recursive --inputDir ${LOCAL_CASE_DIR}\"\n\n\n\n\n\n\n\n\nRequired for Air Gapped installations\n\n\n\nYou will need to install the Gatekeeper OPA AND configure mutations if using a private registry!\n\n\noc ibm-pak launch $CASE_NAME \\\n   --version $CASE_VERSION \\\n   --action install-operator \\\n   --inventory $ICS_INVENTORY_SETUP \\\n   --namespace $NAMESPACE \\\n   --args \"$ARGS\"\n\n\n\n\n\n\nAirgapping CPFS extras\n\n\n\nIf you are performing an airgapped install, you must perform the following steps:\nAfter kicking off the install-operator above, open another terminal and export your private registry as an env var (my.registry.io is an example).\nexport myprivatereg=my.registry.io\nNow wait for the cloud-native-postgreql-image-list config map to be created.\nkubectl get cm cloud-native-postgresql-image-list -w\n\nNAME                                 DATA   AGE\ncloud-native-postgresql-image-list   6      25h\nWhen it appears, run the following\nkubectl get cm cloud-native-postgresql-image-list -o yaml | sed -E \"s/cp.icr.io|\\bicr.io/$myprivatereg/\" &gt; cloud-native-postgresql-image-list-patch.yaml\nThen patch the config map.\nkubectl patch cm cloud-native-postgresql-image-list --patch-file cloud-native-postgresql-image-list-patch.yaml\nNow wait for the cloud-native-postgresql.v1.18.12 ClusterServiceVersion to be created\nkubectl get csv cloud-native-postgresql.v1.18.12 -w\nWhen it appears, run the following:\nkubectl get csv cloud-native-postgresql.v1.18.12 -o yaml | egrep -v \"generation:|resourceVersion:\" | sed -E \"s/cp.icr.io|\\bicr.io/$myprivatereg/\" | sed -e '/^status:/,$ d' &gt; cloud-native-postgresql.v1.18.12-patch.yaml\nPatch the CSV\nkubectl patch csv cloud-native-postgresql.v1.18.12 --type merge --patch-file cloud-native-postgresql.v1.18.12-patch.yaml\nNow you can return to your previous terminal to wait for the installation to complete.\n\n\nThis may take a little while to run.\n\nVerify Foundational Services are properly installed\nkubectl get csv | grep ibm\nOutput should look like this:\nibm-cert-manager-operator.v4.2.11             IBM Cert Manager                       4.2.11                                          Succeeded\nibm-common-service-operator.v4.6.6            IBM Cloud Pak foundational services    4.6.6                                           Succeeded\nibm-commonui-operator.v4.4.5                  Ibm Common UI                          4.4.5                                           Succeeded\nibm-events-operator.v5.0.1                    IBM Events Operator                    5.0.1                                           Succeeded\nibm-iam-operator.v4.5.5                       IBM IM Operator                        4.5.5                                           Succeeded\nibm-zen-operator.v5.1.8                       IBM Zen Service                        5.1.8                                           Succeeded\n\n\nVerify that the operand requests have completed and installed\nkubectl get opreq\nOutput should look like this:\nNAME                          AGE     PHASE     CREATED AT\ncommon-service                4m41s   Running   2024-10-14T20:21:04Z\nibm-iam-request               4m9s    Running   2024-10-14T20:21:36Z\npostgresql-operator-request   4m8s    Running   2024-10-14T20:21:37Z\nVerify the policies have all been created\nkubectl get netpol\nOutput should look like this:\nNAME                                     POD-SELECTOR                                     AGE\naccess-to-audit-svc                      component=zen-audit                              4m1s\naccess-to-common-web-ui                  k8s-app=common-web-ui                            4m19s\naccess-to-edb-postgres                   k8s.enterprisedb.io/cluster                      4m16s\naccess-to-edb-postgres-webhooks          app.kubernetes.io/name=cloud-native-postgresql   3m30s\naccess-to-ibm-common-service-operator    name=ibm-common-service-operator                 3m27s\naccess-to-ibm-nginx                      component=ibm-nginx                              3m58s\naccess-to-icp-mongodb                    app=icp-mongodb                                  4m13s\naccess-to-platform-auth-service          k8s-app=platform-auth-service                    4m10s\naccess-to-platform-identity-management   k8s-app=platform-identity-management             4m5s\naccess-to-platform-identity-provider     k8s-app=platform-identity-provider               4m3s\naccess-to-usermgmt                       component=usermgmt                               3m55s\naccess-to-volumes                        icpdsupport/app=volumes                          3m42s\naccess-to-zen-core                       component=zen-core                               3m50s\naccess-to-zen-core-api                   component=zen-core-api                           3m52s\naccess-to-zen-meta-api                   app.kubernetes.io/instance=ibm-zen-meta-api      3m24s\naccess-to-zen-minio                      component=zen-minio                              3m46s\naccess-to-zen-watchdog                   component=zen-watchdog                           3m39s\nallow-iam-config-job                     component=iam-config-job                         3m34s\nallow-webhook-access-from-apiserver      &lt;none&gt;                                           3m21s\nVerify NGINX ingresses have been created\n\n{/*\n\nInstall Kubernetes Ingresses\nExport the following values. As above, the example FQDN we are using is apps.gi.thinkforward.work. Yours may be different depending on what you’ve set in your DNS or Route 53.\nexport REPLACE_NAMESPACE=openshift-marketplace\nexport HOSTNAME=apps.gi.thinkforward.work\n\n\n\n\n\n\nUsing an insecure hostname\n\n\n\nFor development purposes only, you can avoid registering a domain name and setting up certificates by using your local hosts file to redirect traffic later in this guide. If taking this route, you can use the following:\nexport REPLACE_NAMESPACE=openshift-marketplace\nexport HOSTNAME=apps.gi.guardium-insights.com\n\n\nRun the Configmap creation with the following commands\nwhile [[ $(kubectl get secret platform-oidc-credentials -ojsonpath='{.data.WLP_CLIENT_SECRET}' | base64 -d | wc -c) -eq 0 ]] ; do echo Waiting 30 seconds for platform-oidc-credentials to be created with the correct content; sleep 30; done\nclient_id=\"$(kubectl get secret platform-oidc-credentials -o yaml |grep 'WLP_CLIENT_ID:' | sed 's/WLP_CLIENT_ID: * //' | awk '{print $1}')\"\nclient_id=`echo $client_id|base64 --decode`\necho $client_id\n\ncat cpfs-ingress-template.yaml | sed 's#REPLACE_CLIENT_ID#'$client_id'#g' | sed 's#REPLACE_NAMESPACE#'$NAMESPACE'#g' | sed 's#REPLACE_HOSTNAME#'$HOSTNAME'#g' | kubectl apply -f -\n*/}\n\n\nVerify the ingress creation\nkubectl get ingress\nOutput should look like this:\nNAME                         CLASS   HOSTS                                                         ADDRESS                                                                               PORTS   AGE\ncncf-common-web-ui           nginx   cp-console-openshift-marketplace.apps.gi.thinkforward.work    k8s-ingressn-ingressn-7e06ec0c6b-01363c775f8b7ff9.elb.us-east-1.amazonaws.com         80      13s\ncncf-id-mgmt                 nginx   cp-console-openshift-marketplace.apps.gi.thinkforward.work    k8s-ingressn-ingressn-7e06ec0c6b-01363c775f8b7ff9.elb.us-east-1.amazonaws.com         80      13s\ncncf-platform-auth           nginx   cp-console-openshift-marketplace.apps.gi.thinkforward.work    k8s-ingressn-ingressn-7e06ec0c6b-01363c775f8b7ff9.elb.us-east-1.amazonaws.com         80      13s\ncncf-platform-id-auth        nginx   cp-console-openshift-marketplace.apps.gi.thinkforward.work    k8s-ingressn-ingressn-7e06ec0c6b-01363c775f8b7ff9.elb.us-east-1.amazonaws.com         80      13s\ncncf-platform-id-provider    nginx   cp-console-openshift-marketplace.apps.gi.thinkforward.work    k8s-ingressn-ingressn-7e06ec0c6b-01363c775f8b7ff9.elb.us-east-1.amazonaws.com         80      13s\ncncf-platform-login          nginx   cp-console-openshift-marketplace.apps.gi.thinkforward.work    k8s-ingressn-ingressn-7e06ec0c6b-01363c775f8b7ff9.elb.us-east-1.amazonaws.com         80      13s\ncncf-platform-oidc           nginx   cp-console-openshift-marketplace.apps.gi.thinkforward.work    k8s-ingressn-ingressn-7e06ec0c6b-01363c775f8b7ff9.elb.us-east-1.amazonaws.com         80      13s\ncncf-saml-ui-callback        nginx   cp-console-openshift-marketplace.apps.gi.thinkforward.work    k8s-ingressn-ingressn-7e06ec0c6b-01363c775f8b7ff9.elb.us-east-1.amazonaws.com         80      13s\ncncf-social-login-callback   nginx   cp-console-openshift-marketplace.apps.gi.thinkforward.work    k8s-ingressn-ingressn-7e06ec0c6b-01363c775f8b7ff9.elb.us-east-1.amazonaws.com         80      13s\n\n\n\n\n\n\nUsing an insecure hostname\n\n\n\nIf you used an insecure hostname above, configure your local workstation to redirect traffic to the external IP of the ingress.\nFind the public hostname address of the ingress under EXTERNAL-IP.\nkubectl get service --namespace ingress-nginx ingress-nginx-controller\nNAME                       TYPE           CLUSTER-IP      EXTERNAL-IP                                                                     PORT(S)                      AGE\ningress-nginx-controller   LoadBalancer   10.100.24.243   a091398910dbf4ad8aeb2f3f0e864311-916cbf853e32c1d5.elb.us-east-2.amazonaws.com   80:31369/TCP,443:30370/TCP   20h\nFind the public IP address of the of the public hostname.\nnslookup a091398910dbf4ad8aeb2f3f0e864311-916cbf853e32c1d5.elb.us-east-2.amazonaws.com\nServer:         10.255.255.254\nAddress:        10.255.255.254#53\n\nNon-authoritative answer:\nName:   a091398910dbf4ad8aeb2f3f0e864311-916cbf853e32c1d5.elb.us-east-2.amazonaws.com\nAddress: 13.58.11.46\nName:   a091398910dbf4ad8aeb2f3f0e864311-916cbf853e32c1d5.elb.us-east-2.amazonaws.com\nAddress: 3.19.41.78\nNotice that there are 2 public IP addresses associated with the hostname. This is because that hostname is on a load balancer. For our purposes, you only need one of those IP addresses. We will use 13.58.11.46 here.\nOpen your hosts file on your workstation where you will be using the browser to connect. For Windows that is under C:\\Windows\\System32\\drivers\\etc\\hosts. For Linux that is under /etc/hosts.\nAdd a line that redirects network traffic from the insecure hostname to the public IP address and save the file.\n13.58.11.46   cp-console-openshift-marketplace.apps.gi.guardium-insights.com\nAfter installing Guardium Insights (next step), another ingress will be created that will need to be added to the local hosts file. Add that additional line now, even though that hostname will not redirect properly until after Guardium Insights is installed.\n13.58.11.46   guardium.apps.gi.guardium-insights.com\n\n\n\n\nVerify Ingress\nVerify that the common webui is now up and available by going to the link above (this will reflect whatever you set for your domain). In our case it is cp-console-openshift-marketplace.apps.gi.thinkforward.work\n\n\n\n\n\n\nUsing an insecure hostname\n\n\n\nIf you used an insecure hostname above, use cp-console-openshift-marketplace.apps.gi.guardium-insights.com to verify.\n\n\nYou can retrieve the cpadmin password with the following\nkubectl get secret platform-auth-idp-credentials -o jsonpath='{.data.admin_password}' | base64 -d\nThis will return the password for the cpadmin user. You can then use cpadmin to login.",
    "crumbs": [
      "Guardium Insights",
      "EKS",
      "EKS Cluster Build"
    ]
  },
  {
    "objectID": "src/guardium_insights/eks/02_cluster-build.html#install-kubernetes-ingresses",
    "href": "src/guardium_insights/eks/02_cluster-build.html#install-kubernetes-ingresses",
    "title": "EKS Cluster build",
    "section": "Install Kubernetes Ingresses",
    "text": "Install Kubernetes Ingresses\nExport the following values. As above, the example FQDN we are using is apps.gi.thinkforward.work. Yours may be different depending on what you’ve set in your DNS or Route 53.\nexport REPLACE_NAMESPACE=openshift-marketplace\nexport HOSTNAME=apps.gi.thinkforward.work\n\n\n\n\n\n\nUsing an insecure hostname\n\n\n\nFor development purposes only, you can avoid registering a domain name and setting up certificates by using your local hosts file to redirect traffic later in this guide. If taking this route, you can use the following:\nexport REPLACE_NAMESPACE=openshift-marketplace\nexport HOSTNAME=apps.gi.guardium-insights.com\n\n\nRun the Configmap creation with the following commands\nwhile [[ $(kubectl get secret platform-oidc-credentials -ojsonpath='{.data.WLP_CLIENT_SECRET}' | base64 -d | wc -c) -eq 0 ]] ; do echo Waiting 30 seconds for platform-oidc-credentials to be created with the correct content; sleep 30; done\nclient_id=\"$(kubectl get secret platform-oidc-credentials -o yaml |grep 'WLP_CLIENT_ID:' | sed 's/WLP_CLIENT_ID: * //' | awk '{print $1}')\"\nclient_id=`echo $client_id|base64 --decode`\necho $client_id\n\ncat cpfs-ingress-template.yaml | sed 's#REPLACE_CLIENT_ID#'$client_id'#g' | sed 's#REPLACE_NAMESPACE#'$NAMESPACE'#g' | sed 's#REPLACE_HOSTNAME#'$HOSTNAME'#g' | kubectl apply -f -\n*/}",
    "crumbs": [
      "Guardium Insights",
      "EKS",
      "EKS Cluster Build"
    ]
  },
  {
    "objectID": "src/guardium_insights/ec2/03-upi_install.html",
    "href": "src/guardium_insights/ec2/03-upi_install.html",
    "title": "OpenShift Installation",
    "section": "",
    "text": "sudo yum update -y; sudo yum install git -y\n\n\n\ngit clone https://github.com/ibm-client-engineering/engineering-journal-quantum-safe\n\n\n\nRun the ‘setup_bastion’ script which will install additional tools, and configure an HTTP server to host the Bootstrap ignition file.\ncd $HOME/aws-openshift410-cloudformation-noIAM-noR53/utils\n./setup_bastion.sh\n\n\n\nsudo hostnamectl set-hostname registry.$DOMAIN.com\nsudo reboot"
  },
  {
    "objectID": "src/guardium_insights/ec2/03-upi_install.html#nfs-provisioner-setup",
    "href": "src/guardium_insights/ec2/03-upi_install.html#nfs-provisioner-setup",
    "title": "OpenShift Installation",
    "section": "NFS Provisioner setup",
    "text": "NFS Provisioner setup\n\nIn a user managed instance of OCP on AWS, the EFS provisioner from AWS is not available. In our case we will be provisioning an EFS share, and then installing the NFS provioner to the cluster.2a\n\n\nCreating the EFS filesystem\nCreate an AWS Elastic File System with the following commands:\nexport CREATION_TOKEN='ibm-wxai-token'\naws efs create-file-system --creation-token ${CREATION_TOKEN} --encrypted --backup --performance-mode generalPurpose --throughput-mode elastic --region us-east-2 --tags Key=key,Value=value Key=key1,Value=value1\nSet the Subnet IDs and Worker Security Group ID variables:\nexport SUBNET_ID_1=\"US-EAST-2a Subnet ID\"\nexport SUBNET_ID_2=\"US-EAST-2b Subnet ID\"\nexport SUBNET_ID_3=\"US-EAST-2c Subnet ID\"\nexport WORKER_SG=\"Worker Security Group ID\"\nexport EFS_ID=$(aws efs describe-file-systems --creation-token ${CREATION_TOKEN} | jq '.FileSystems[] | .\"FileSystemId\"'| tr -d '\"')\n\nCreate mount targets:\nRun the following commands to create mount targets that include the three subnets used by the worker nodes:\naws efs create-mount-target --file-system-id ${EFS_ID} --subnet-id  ${SUBNET_ID_1} --security-group ${WORKER_SG_ID} --region us-east-2\naws efs create-mount-target --file-system-id ${EFS_ID} --subnet-id  ${SUBNET_ID_2} --security-group ${WORKER_SG_ID} --region us-east-2\naws efs create-mount-target --file-system-id ${EFS_ID} --subnet-id  ${SUBNET_ID_3} --security-group ${WORKER_SG_ID} --region us-east-2\n\n\n\nDeploying the provisioner with a yaml\n\nDownload nfs-provisioner.yml\nDownload the nfs-provisioner.yml file from here:\nhttps://raw.githubusercontent.com/ibm-client-engineering/engineering-journal-quantum-safe/main/public/scripts/nfs_provisioner.yml\nConfirm that the “EFS_ID” is properly set from the previous step:\necho $EFS_ID\nThen update the ‘nfs_provisioner.yml’ to use the new EFS ID:\nsed -i \"s/EFS_ID/${EFS_ID}/g\" nfs_provisioner.yml\n\n\nApply nfs_provisioner.yml\nNow create the nfs_provisioner in the cluster:\noc create -f nfs_provisioner.yml\n\n\n\nDeploying the provisioner with helm\n\nInstalling helm\nVerify and/or install openssl\nsudo dnf -y install openssl\nRun the following commands to install helm to the bastion host\ncurl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod +x get_helm.sh\n./get_helm.sh\n\nDownloading https://get.helm.sh/helm-v3.14.3-linux-amd64.tar.gz\nVerifying checksum... Done.\nPreparing to install helm into /usr/local/bin\nhelm installed into /usr/local/bin/helm\nVerify helm is available\nhelm\n\nThe Kubernetes package manager\n\nCommon actions for Helm:\n\n- helm search:    search for charts\n- helm pull:      download a chart to your local directory to view\n- helm install:   upload the chart to Kubernetes\n- helm list:      list releases of charts\n\nEnvironment variables:\n\n| Name                               | Description                                                                                                |\n|------------------------------------|------------------------------------------------------------------------------------------------------------|\n| $HELM_CACHE_HOME                   | set an alternative location for storing cached files.                                                      |\n| $HELM_CONFIG_HOME                  | set an alternative location for storing Helm configuration.                                                |\n| $HELM_DATA_HOME                    | set an alternative location for storing Helm data.                                                         |\n| $HELM_DEBUG                        | indicate whether or not Helm is running in Debug mode                                                      |\n| $HELM_DRIVER                       | set the backend storage driver. Values are: configmap, secret, memory, sql.                                |\n| $HELM_DRIVER_SQL_CONNECTION_STRING | set the connection string the SQL storage driver should use.                                               |\n| $HELM_MAX_HISTORY                  | set the maximum number of helm release history.                                                            |\n| $HELM_NAMESPACE                    | set the namespace used for the helm operations.                                                            |\n| $HELM_NO_PLUGINS                   | disable plugins. Set HELM_NO_PLUGINS=1 to disable plugins.                                                 |\n| $HELM_PLUGINS                      | set the path to the plugins directory                                                                      |\n| $HELM_REGISTRY_CONFIG              | set the path to the registry config file.                                                                  |\n| $HELM_REPOSITORY_CACHE             | set the path to the repository cache directory                                                             |\n| $HELM_REPOSITORY_CONFIG            | set the path to the repositories file.                                                                     |\n| $KUBECONFIG                        | set an alternative Kubernetes configuration file (default \"~/.kube/config\")                                |\n| $HELM_KUBEAPISERVER                | set the Kubernetes API Server Endpoint for authentication                                                  |\n| $HELM_KUBECAFILE                   | set the Kubernetes certificate authority file.                                                             |\n| $HELM_KUBEASGROUPS                 | set the Groups to use for impersonation using a comma-separated list.                                      |\n| $HELM_KUBEASUSER                   | set the Username to impersonate for the operation.                                                         |\n| $HELM_KUBECONTEXT                  | set the name of the kubeconfig context.                                                                    |\n| $HELM_KUBETOKEN                    | set the Bearer KubeToken used for authentication.                                                          |\n| $HELM_KUBEINSECURE_SKIP_TLS_VERIFY | indicate if the Kubernetes API server's certificate validation should be skipped (insecure)                |\n| $HELM_KUBETLS_SERVER_NAME          | set the server name used to validate the Kubernetes API server certificate                                 |\n| $HELM_BURST_LIMIT                  | set the default burst limit in the case the server contains many CRDs (default 100, -1 to disable)         |\n| $HELM_QPS                          | set the Queries Per Second in cases where a high number of calls exceed the option for higher burst values |\n\nHelm stores cache, configuration, and data based on the following configuration order:\n\n- If a HELM_*_HOME environment variable is set, it will be used\n- Otherwise, on systems supporting the XDG base directory specification, the XDG variables will be used\n- When no other location is set a default location will be used based on the operating system\n\nBy default, the default directories depend on the Operating System. The defaults are listed below:\n\n| Operating System | Cache Path                | Configuration Path             | Data Path               |\n|------------------|---------------------------|--------------------------------|-------------------------|\n| Linux            | $HOME/.cache/helm         | $HOME/.config/helm             | $HOME/.local/share/helm |\n| macOS            | $HOME/Library/Caches/helm | $HOME/Library/Preferences/helm | $HOME/Library/helm      |\n| Windows          | %TEMP%\\helm               | %APPDATA%\\helm                 | %APPDATA%\\helm          |\n\nUsage:\n  helm [command]\n\nAvailable Commands:\n  completion  generate autocompletion scripts for the specified shell\n  create      create a new chart with the given name\n  dependency  manage a chart's dependencies\n  env         helm client environment information\n  get         download extended information of a named release\n  help        Help about any command\n  history     fetch release history\n  install     install a chart\n  lint        examine a chart for possible issues\n  list        list releases\n  package     package a chart directory into a chart archive\n  plugin      install, list, or uninstall Helm plugins\n  pull        download a chart from a repository and (optionally) unpack it in local directory\n  push        push a chart to remote\n  registry    login to or logout from a registry\n  repo        add, list, remove, update, and index chart repositories\n  rollback    roll back a release to a previous revision\n  search      search for a keyword in charts\n  show        show information of a chart\n  status      display the status of the named release\n  template    locally render templates\n  test        run tests for a release\n  uninstall   uninstall a release\n  upgrade     upgrade a release\n  verify      verify that a chart at the given path has been signed and is valid\n  version     print the client version information\n\nFlags:\n      --burst-limit int                 client-side default throttling limit (default 100)\n      --debug                           enable verbose output\n  -h, --help                            help for helm\n      --kube-apiserver string           the address and the port for the Kubernetes API server\n      --kube-as-group stringArray       group to impersonate for the operation, this flag can be repeated to specify multiple groups.\n      --kube-as-user string             username to impersonate for the operation\n      --kube-ca-file string             the certificate authority file for the Kubernetes API server connection\n      --kube-context string             name of the kubeconfig context to use\n      --kube-insecure-skip-tls-verify   if true, the Kubernetes API server's certificate will not be checked for validity. This will make your HTTPS connections insecure\n      --kube-tls-server-name string     server name to use for Kubernetes API server certificate validation. If it is not provided, the hostname used to contact the server is used\n      --kube-token string               bearer token used for authentication\n      --kubeconfig string               path to the kubeconfig file\n  -n, --namespace string                namespace scope for this request\n      --qps float32                     queries per second used when communicating with the Kubernetes API, not including bursting\n      --registry-config string          path to the registry config file (default \"/home/kramerro/.config/helm/registry/config.json\")\n      --repository-cache string         path to the file containing cached repository indexes (default \"/home/kramerro/.cache/helm/repository\")\n      --repository-config string        path to the file containing repository names and URLs (default \"/home/kramerro/.config/helm/repositories.yaml\")\n\nUse \"helm [command] --help\" for more information about a command.\n\n\n\nInstall to OCP\nFrom the OC cli:\noc new-project nfs-provisioner\n\nNow using project \"nfs-provisioner\" on server \"https://api.wxai.cpdu8vscs.ibmworkshops.com:6443\".\n\nYou can add applications to this project with the 'new-app' command. For example, try:\n\n    oc new-app rails-postgresql-example\n\nto build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:\n\n    kubectl create deployment hello-node --image=registry.k8s.io/e2e-test-images/agnhost:2.43 -- /agnhost serve-hostname\nThis will automatically put you in that project.\nNow set up the SCCs\noc apply -f - &lt;&lt;EOF\nallowHostDirVolumePlugin: true\nallowHostIPC: false\nallowHostNetwork: false\nallowHostPID: false\nallowHostPorts: false\nallowPrivilegeEscalation: true\nallowPrivilegedContainer: false\nallowedCapabilities: null\napiVersion: security.openshift.io/v1\ndefaultAddCapabilities: null\nfsGroup:\n  type: RunAsAny\ngroups: []\nkind: SecurityContextConstraints\nmetadata:\n  annotations:\n    kubernetes.io/description: 'hostmount-anyuid provides all the features of the\n      restricted SCC but allows host mounts and any UID by a pod.  This is primarily\n      used by the persistent volume recycler. WARNING: this SCC allows host file system\n      access as any UID, including UID 0.  Grant with caution.'\n  name: nfs-storage-hostmount-anyuid\nreadOnlyRootFilesystem: false\nrequiredDropCapabilities:\n- MKNOD\nrunAsUser:\n  type: RunAsAny\nseLinuxContext:\n  type: MustRunAs\nsupplementalGroups:\n  type: RunAsAny\nusers:\n- system:serviceaccount:nfs-provisioner:nfs-subdir-external-provisioner\nvolumes:\n- configMap\n- downwardAPI\n- emptyDir\n- hostPath\n- nfs\n- persistentVolumeClaim\n- projected\n- secret\nEOF\noutput:\nsecuritycontextconstraints.security.openshift.io/nfs-storage-hostmount-anyuid created\n\n\nInstall the helm chart and run the helm install\nThe &lt;EFS URL&gt; should be returned by the aws cli creation of the EFS storage filesystem.\nhelm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/\nhelm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\\n    --namespace nfs-provisioner \\\n    --set nfs.server=&lt;EFS URL&gt;\\\n    --set nfs.path=/ \\\n    --set storageClass.defaultClass=true\nVerify the pods are up\noc get pods\nEXAMPLE RETURNED OUTPUT HERE\nMake sure the storage class now exists and is set to default\noc get sc\nVerify that the storage will provision with the following test deployment\noc apply -f - &lt;&lt;EOF\nkind: Pod\napiVersion: v1\nmetadata:\n  name: test-pod\nspec:\n  containers:\n  - name: test-pod\n    image: busybox:stable\n    command:\n      - \"/bin/sh\"\n    args:\n      - \"-c\"\n      - \"touch /mnt/SUCCESS && exit 0 || exit 1\"\n    volumeMounts:\n      - name: nfs-pvc\n        mountPath: \"/mnt\"\n  restartPolicy: \"Never\"\n  volumes:\n    - name: nfs-pvc\n      persistentVolumeClaim:\n        claimName: test-claim\n---\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: test-claim\nspec:\n  storageClassName: nfs-client\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Mi\nEOF"
  },
  {
    "objectID": "src/guardium_insights/ec2/03-upi_install.html#change-cluster-domain",
    "href": "src/guardium_insights/ec2/03-upi_install.html#change-cluster-domain",
    "title": "OpenShift Installation",
    "section": "Change Cluster Domain",
    "text": "Change Cluster Domain\n\nGenerate new self-signed certificate\n\nGenerate CA certs\nopenssl genrsa -out ca.key 2048\n\nopenssl req -new -x509 -days 365 -key ca.key -subj \"/C=CN/ST=GD/L=SZ/O=Acme, Inc./CN=Acme Root CA\" -out ca.crt\n\n\n\nGenerate Server certs\nGenerate ‘server.csr’\nopenssl req -newkey rsa:2048 -nodes -keyout server.key -subj \"/C=CN/ST=GD/L=SZ/O=Acme, Inc./CN=*.{BASE_DOMAIN}\" -out server.csr\nGenerate ‘server.crt’\nopenssl x509 -req -extfile &lt;(printf \"subjectAltName=DNS:*.{BASE_DOMAIN}\") -days 365 -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt\n\n\nUpdate the cluster:\nCreate the new secret which will contain the cert and key:\noc create secret tls custom-cert --cert=server.crt --key=server.key -n openshift-config\nUpdate the ingress:\noc edit ingresses.config/cluster -o yaml\nAdd the following under ‘spec:’\n  componentRoutes:\n  - hostname: console.{NEW_URL}\n    name: console\n    namespace: openshift-console\n    servingCertKeyPairSecret:\n      name: custom-cert\n  - hostname: oauth.{NEW_URL}\n    name: oauth-openshift\n    namespace: openshift-authentication\n    servingCertKeyPairSecret:\n      name: custom-cert"
  },
  {
    "objectID": "src/guardium_insights/ec2/03-upi_install.html#increase-primary-disk-size-on-worker-nodes",
    "href": "src/guardium_insights/ec2/03-upi_install.html#increase-primary-disk-size-on-worker-nodes",
    "title": "OpenShift Installation",
    "section": "Increase Primary Disk size on worker nodes:",
    "text": "Increase Primary Disk size on worker nodes:\n\nRun the following bash one-liner to increase the primary disk on all worker nodes to 500GB:\n\naws ec2 describe-instances --query 'Reservations[*].Instances[*].[InstanceId,Tags[?Key==`Name`].Value|[0],BlockDeviceMappings[0].Ebs.VolumeId]' --output text | grep worker | awk '{print $3}' | while read volume_id; do aws ec2 modify-volume --volume-id $volume_id --size 500; done\n\nLog into the node with following command:\n\noc debug node/&lt;node_name&gt;\n\nOnce in the node, run the following:\n\nchroot /host\nthen:\nsudo lsblk\nThe output should look like this:\n# sudo lsblk\nNAME        MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS\nnvme1n1     259:0    0     1T  0 disk\nnvme0n1     259:1    0   500G  0 disk\n|-nvme0n1p1 259:2    0     1M  0 part\n|-nvme0n1p2 259:3    0   127M  0 part\n|-nvme0n1p3 259:4    0   384M  0 part /boot\n`-nvme0n1p4 259:5    0 239.5G  0 part /var/lib/kubelet/pods/555d6f90-41fd-49d2-8aad-fa7293a924e4/volume-subpaths/app-config-override/wd-discovery-cnm-api/2\n                                      /var/lib/kubelet/pods/57d28f73-d355-4092-8e52-6b6aeec28bd5/volume-subpaths/clouseau-config/search/4\n                                      /var/lib/kubelet/pods/fcb4b5ec-ea1a-42c7-908c-a027cf885ca1/volume-subpaths/db2wh-cm/zen-database-core/1\n                                      /var/lib/kubelet/pods/fcb4b5ec-ea1a-42c7-908c-a027cf885ca1/volume-subpaths/db2oltp-cm/zen-database-core/0\n                                      /var/lib/kubelet/pods/5e35fe3f-7e4d-4729-b3dd-b9553ffd73f6/volume-subpaths/nginx-conf/monitoring-plugin/1\n                                      /var\n                                      /sysroot/ostree/deploy/rhcos/var\n                                      /sysroot\n                                      /usr\n                                      /etc\n                                      /\n\n\nFind the part on the disk that you wish to increase, in my case it was ‘nvme0n1p4’.\n\nNow we extend the partition, by targeting the disk (Example: /dev/nvme0n1) and the partition (Example: 4)\nsudo growpart /dev/nvme0n1 4\n\nCheck the disk sizes again:\n\nsudo lsblk\nThis is what my output looks like now:\nNAME        MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS\nnvme1n1     259:0    0     1T  0 disk\nnvme0n1     259:1    0   500G  0 disk\n|-nvme0n1p1 259:2    0     1M  0 part\n|-nvme0n1p2 259:3    0   127M  0 part\n|-nvme0n1p3 259:4    0   384M  0 part /boot\n`-nvme0n1p4 259:5    0 499.5G  0 part /var/lib/kubelet/pods/555d6f90-41fd-49d2-8aad-fa7293a924e4/volume-subpaths/app-config-override/wd-discovery-cnm-api/2\n                                      /var/lib/kubelet/pods/57d28f73-d355-4092-8e52-6b6aeec28bd5/volume-subpaths/clouseau-config/search/4\n                                      /var/lib/kubelet/pods/fcb4b5ec-ea1a-42c7-908c-a027cf885ca1/volume-subpaths/db2wh-cm/zen-database-core/1\n                                      /var/lib/kubelet/pods/fcb4b5ec-ea1a-42c7-908c-a027cf885ca1/volume-subpaths/db2oltp-cm/zen-database-core/0\n                                      /var/lib/kubelet/pods/5e35fe3f-7e4d-4729-b3dd-b9553ffd73f6/volume-subpaths/nginx-conf/monitoring-plugin/1\n                                      /var\n                                      /sysroot/ostree/deploy/rhcos/var\n                                      /sysroot\n                                      /usr\n                                      /etc\n                                      /\n\nLast step is to extend the file system:\n\nsudo xfs_growfs -d /"
  },
  {
    "objectID": "src/guardium_insights/ec2/03-upi_install.html#add-users-to-openshift-using-htpassword",
    "href": "src/guardium_insights/ec2/03-upi_install.html#add-users-to-openshift-using-htpassword",
    "title": "OpenShift Installation",
    "section": "Add users to OpenShift using HTPassword",
    "text": "Add users to OpenShift using HTPassword\n\nInstall HTTP Tools:\nyum install httpd-tools\n\n\nCreate a HTPassword File:\nRun the following command to create the HTPassword file that will be used by OpenShift.\nhtpassword -c &lt;file_name&gt; &lt;username&gt;\nAfter running the command, you will be prompted for a password and then asked to confirm password.\n\n\nCreate the HTPassword secret:\noc create secret generic htpasswd-secret --from-file=htpasswd=&lt;file_name&gt; -n openshift-config\n\n\n\nEdit the OpenShift OAuth settings:\noc edit oauth cluster\nThen add the following to the file:\nspec:\n  identityProviders:\n  - name: my_htpasswd_provider\n    mappingMethod: claim\n    type: HTPasswd\n    htpasswd:\n      fileData:\n        name: htpasswd-secret\nAfter this has been added, try opening Openshift in a private browser, and select ‘HTPasswd’. Then enter the username and password."
  },
  {
    "objectID": "src/key-takeaway.html",
    "href": "src/key-takeaway.html",
    "title": "Key Takeaways",
    "section": "",
    "text": "Best Practices\n(Capture the main takeaways and results of the project)",
    "crumbs": [
      "Key Takeaways"
    ]
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "General Overview",
    "section": "",
    "text": "POCs are built in a development account in AWS.\nOCP is not approved for production environments.\nSpecial considerations are required for CoreOS ami\nClient adheres to role based access for all their environments.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#security-posture-assumptions",
    "href": "overview.html#security-posture-assumptions",
    "title": "General Overview",
    "section": "",
    "text": "POCs are built in a development account in AWS.\nOCP is not approved for production environments.\nSpecial considerations are required for CoreOS ami\nClient adheres to role based access for all their environments.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#aws-role-perms",
    "href": "overview.html#aws-role-perms",
    "title": "General Overview",
    "section": "AWS Role perms",
    "text": "AWS Role perms\nQuantum Safe Posture Management is supported on either Red Hat Openshift or on AWS EKS.\n\n{/* ### Policies required for installing OCP\nThe following policies are necessary to install\n\n\nRequired EC2 permissions for installation\n\n    ec2:AuthorizeSecurityGroupEgress\n    ec2:AuthorizeSecurityGroupIngress\n    ec2:CopyImage\n    ec2:CreateNetworkInterface\n    ec2:AttachNetworkInterface\n    ec2:CreateSecurityGroup\n    ec2:CreateTags\n    ec2:CreateVolume\n    ec2:DeleteSecurityGroup\n    ec2:DeleteSnapshot\n    ec2:DeleteTags\n    ec2:DeregisterImage\n    ec2:DescribeAccountAttributes\n    ec2:DescribeAddresses\n    ec2:DescribeAvailabilityZones\n    ec2:DescribeDhcpOptions\n    ec2:DescribeImages\n    ec2:DescribeInstanceAttribute\n    ec2:DescribeInstanceCreditSpecifications\n    ec2:DescribeInstances\n    ec2:DescribeInstanceTypes\n    ec2:DescribeInternetGateways\n    ec2:DescribeKeyPairs\n    ec2:DescribeNatGateways\n    ec2:DescribeNetworkAcls\n    ec2:DescribeNetworkInterfaces\n    ec2:DescribePrefixLists\n    ec2:DescribeRegions\n    ec2:DescribeRouteTables\n    ec2:DescribeSecurityGroups\n    ec2:DescribeSubnets\n    ec2:DescribeTags\n    ec2:DescribeVolumes\n    ec2:DescribeVpcAttribute\n    ec2:DescribeVpcClassicLink\n    ec2:DescribeVpcClassicLinkDnsSupport\n    ec2:DescribeVpcEndpoints\n    ec2:DescribeVpcs\n    ec2:GetEbsDefaultKmsKeyId\n    ec2:ModifyInstanceAttribute\n    ec2:ModifyNetworkInterfaceAttribute\n    ec2:RevokeSecurityGroupEgress\n    ec2:RevokeSecurityGroupIngress\n    ec2:RunInstances\n    ec2:TerminateInstances\n\n\n\nRequired permissions for creating network resources during installation\n\n    ec2:AllocateAddress\n    ec2:AssociateAddress\n    ec2:AssociateDhcpOptions\n    ec2:AssociateRouteTable\n    ec2:AttachInternetGateway\n    ec2:CreateDhcpOptions\n    ec2:CreateInternetGateway\n    ec2:CreateNatGateway\n    ec2:CreateRoute\n    ec2:CreateRouteTable\n    ec2:CreateSubnet\n    ec2:CreateVpc\n    ec2:CreateVpcEndpoint\n    ec2:ModifySubnetAttribute\n    ec2:ModifyVpcAttribute\n\nIf you use an existing VPC, your account does not require these permissions for creating network resources.\n\n\n\n\nRequired Elastic Load Balancing permissions (ELB) for installation\n\nelasticloadbalancing:AddTags\nelasticloadbalancing:ApplySecurityGroupsToLoadBalancer\nelasticloadbalancing:AttachLoadBalancerToSubnets\nelasticloadbalancing:ConfigureHealthCheck\nelasticloadbalancing:CreateLoadBalancer\nelasticloadbalancing:CreateLoadBalancerListeners\nelasticloadbalancing:DeleteLoadBalancer\nelasticloadbalancing:DeregisterInstancesFromLoadBalancer\nelasticloadbalancing:DescribeInstanceHealth\nelasticloadbalancing:DescribeLoadBalancerAttributes\nelasticloadbalancing:DescribeLoadBalancers\nelasticloadbalancing:DescribeTags\nelasticloadbalancing:ModifyLoadBalancerAttributes\nelasticloadbalancing:RegisterInstancesWithLoadBalancer\nelasticloadbalancing:SetLoadBalancerPoliciesOfListener\n\n\n\nRequired Elastic Load Balancing permissions (ELBv2) for installation\n\n    elasticloadbalancing:AddTags\n    elasticloadbalancing:CreateListener\n    elasticloadbalancing:CreateLoadBalancer\n    elasticloadbalancing:CreateTargetGroup\n    elasticloadbalancing:DeleteLoadBalancer\n    elasticloadbalancing:DeregisterTargets\n    elasticloadbalancing:DescribeListeners\n    elasticloadbalancing:DescribeLoadBalancerAttributes\n    elasticloadbalancing:DescribeLoadBalancers\n    elasticloadbalancing:DescribeTargetGroupAttributes\n    elasticloadbalancing:DescribeTargetHealth\n    elasticloadbalancing:ModifyLoadBalancerAttributes\n    elasticloadbalancing:ModifyTargetGroup\n    elasticloadbalancing:ModifyTargetGroupAttributes\n    elasticloadbalancing:RegisterTargets\n\n\n\nRequired IAM permissions for installation\n\n    iam:AddRoleToInstanceProfile\n    iam:CreateInstanceProfile\n    iam:CreateRole\n    iam:DeleteInstanceProfile\n    iam:DeleteRole\n    iam:DeleteRolePolicy\n    iam:GetInstanceProfile\n    iam:GetRole\n    iam:GetRolePolicy\n    iam:GetUser\n    iam:ListInstanceProfilesForRole\n    iam:ListRoles\n    iam:ListUsers\n    iam:PassRole\n    iam:PutRolePolicy\n    iam:RemoveRoleFromInstanceProfile\n    iam:SimulatePrincipalPolicy\n    iam:TagRole\n\nIf you have not created a load balancer in your AWS account, the IAM user also requires the iam:CreateServiceLinkedRole permission.\n\n\n\n\nRequired Route 53 permissions for installation\n\n    route53:ChangeResourceRecordSets\n    route53:ChangeTagsForResource\n    route53:CreateHostedZone\n    route53:DeleteHostedZone\n    route53:GetChange\n    route53:GetHostedZone\n    route53:ListHostedZones\n    route53:ListHostedZonesByName\n    route53:ListResourceRecordSets\n    route53:ListTagsForResource\n    route53:UpdateHostedZoneComment\n\n\n\nIf S3 buckets are required, add these permissions.\n\n    s3:CreateBucket\n    s3:DeleteBucket\n    s3:GetAccelerateConfiguration\n    s3:GetBucketAcl\n    s3:GetBucketCors\n    s3:GetBucketLocation\n    s3:GetBucketLogging\n    s3:GetBucketPolicy\n    s3:GetBucketObjectLockConfiguration\n    s3:GetBucketRequestPayment\n    s3:GetBucketTagging\n    s3:GetBucketVersioning\n    s3:GetBucketWebsite\n    s3:GetEncryptionConfiguration\n    s3:GetLifecycleConfiguration\n    s3:GetReplicationConfiguration\n    s3:ListBucket\n    s3:PutBucketAcl\n    s3:PutBucketTagging\n    s3:PutEncryptionConfiguration\n\n\n\nS3 permissions that cluster Operators require if S3 is needed\n\n    s3:DeleteObject\n    s3:GetObject\n    s3:GetObjectAcl\n    s3:GetObjectTagging\n    s3:GetObjectVersion\n    s3:PutObject\n    s3:PutObjectAcl\n    s3:PutObjectTagging\n\n\n\nRequired permissions to delete base cluster resources for cleanups\n\n    autoscaling:DescribeAutoScalingGroups\n    ec2:DeletePlacementGroup\n    ec2:DeleteNetworkInterface\n    ec2:DeleteVolume\n    elasticloadbalancing:DeleteTargetGroup\n    elasticloadbalancing:DescribeTargetGroups\n    iam:DeleteAccessKey\n    iam:DeleteUser\n    iam:ListAttachedRolePolicies\n    iam:ListInstanceProfiles\n    iam:ListRolePolicies\n    iam:ListUserPolicies\n    s3:DeleteObject\n    s3:ListBucketVersions\n    tag:GetResources\n\n\n\nRequired permissions to delete network resources for cleanups\n\n    ec2:DeleteDhcpOptions\n    ec2:DeleteInternetGateway\n    ec2:DeleteNatGateway\n    ec2:DeleteRoute\n    ec2:DeleteRouteTable\n    ec2:DeleteSubnet\n    ec2:DeleteVpc\n    ec2:DeleteVpcEndpoints\n    ec2:DetachInternetGateway\n    ec2:DisassociateRouteTable\n    ec2:ReleaseAddress\n    ec2:ReplaceRouteTableAssociation\n\nIf you use an existing VPC, your account does not require these permissions to delete network resources. Instead, your account only requires the tag:UntagResources permission to delete network resources.\n\n\n\n\nRequired permissions to delete a cluster with shared instance roles for cleanups\n\niam:UntagRole\n\n\n\nAdditional IAM and S3 permissions that are required to create manifests\n\n    iam:DeleteAccessKey\n    iam:DeleteUser\n    iam:DeleteUserPolicy\n    iam:GetUserPolicy\n    iam:ListAccessKeys\n    iam:PutUserPolicy\n    iam:TagUser\n    s3:PutBucketPublicAccessBlock\n    s3:GetBucketPublicAccessBlock\n    s3:PutLifecycleConfiguration\n    s3:HeadBucket\n    s3:ListBucketMultipartUploads\n    s3:AbortMultipartUpload\n\nIf you are managing your cloud provider credentials with mint mode, the IAM user also requires the iam:CreateAccessKey and iam:CreateUser permissions.\n\n\n\n\nOptional permissions for instance and quota checks for installation\n\n    ec2:DescribeInstanceTypeOfferings\n    servicequotas:ListAWSDefaultServiceQuotas\n\n\n\nOptional permissions for the cluster owner account when installing a cluster on a shared VPC\n\n    sts:AssumeRole\n\n\nFor more details on the IAM user requirements, see https://docs.openshift.com/container-platform/4.15/installing/installing_aws/installing-aws-account.html#installing-aws-account\n\n*/}",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#engineering-infrastructure-requirements",
    "href": "overview.html#engineering-infrastructure-requirements",
    "title": "General Overview",
    "section": "Engineering infrastructure requirements",
    "text": "Engineering infrastructure requirements\nPer engineering doc:\n\nOpenShift Container Platform\nIBM Cloud Pak Foundational Services\ncluster-admin privileges are required\n\nBelow sizing are the base requirements for a extra small starter deployment.\n\n\n\n\n\n\n\n\n\n\n\nFlavor\nCount\nvCPU\nRAM\nLocal Storage\nRole\n\n\n\n\nc5.2xlarge\n3\n24 (8 cores x Count)\n48G (16G x Count)\n100Gb EBS\n* Control Plane\n\n\nm6i.4xlarge\n4\n64 (16 cores x Count)\n256G (64G x Count)\n250Gb EBS, 200Gb EBS (additional)\nCompute\n\n\nr6i.8xlarge\n1\n32\n256G\n250Gb EBS, 10Tb EBS (additional)\nDB2\n\n\nTotals\n8\n128\n560G\n11903Gb\n\n\n\n\n\n\n\n\n\n\nControl Plane\n\n\n\nIf deploying to EKS on AWS, you don’t need to be concerned with the control plane sizing. Only compute.\n\n\n\n{/* ## Open Questions\n\nPre-Reqs call for 4 vcpu and 8 gig for the infra/bastion node. Can it get by with 2 vcpu? Reasoning: There isn’t many options available in AWS that offer 4x8. Mostly 2x1, 2x4, 2x\nWorker nodes require an extra 200Gb disk. What is this required for?\nCan this work with EFS? */}",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#storage-considerations",
    "href": "overview.html#storage-considerations",
    "title": "General Overview",
    "section": "Storage considerations:",
    "text": "Storage considerations:\nclient primarily uses EBS and EFS.\n\nNetwork attached (SAN) SSD at 3000 MB/sec IO (required)\nAdding ODF into the mix would require adding more larger storage nodes\n\n\n{/* Other considerations:\n\n - Less bloated version of ODF */}",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#components",
    "href": "overview.html#components",
    "title": "General Overview",
    "section": "Components",
    "text": "Components\nThese are some components of this solution that are available from IBM Passport Advantage:\n\nBill of Materials\n\nIBM Quantum Safe Remediator (QSR)\n\nAdaptive Proxy\nPerformance Test Harness\n\nIBM Quantum Safe Explorer (QSR)\nQSPM\nIBM Quantum Safe Advisor (QSA)\n\n\nConstraints\n\nQSR\n\nRequires docker or podman\nUsing the Adaptive Proxy it can act as a dynamic TLS termination point and re-encrypts connections with quantum-safe encryption\n\nQSE\n\nRuns as a local service on a user’s desktop.\nRapid deployment.\nSupports MacOS and Windows services.\nHas a CLI to locally interact with the local service.\nVSCode plugin to automatically scan and identify vulnerable pre-quantum algorithms in a user’s code\n\nQSA\n\nTBD\n\nQSPM\n\nCurrent specs require OCP cluster or EKS",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "src/guardium_insights/ec2/02-preparation.html",
    "href": "src/guardium_insights/ec2/02-preparation.html",
    "title": "Preparation",
    "section": "",
    "text": "Use this URL to download your RedHat Pull Secret: https://console.redhat.com/openshift/install/pull-secret\n\nIf your organization does not have an existing RedHat account, you can create a RedHat trial account for a temporary OCP deployment (60 days). Instructions here under the expandable section “Obtain a RedHat Trial Account”",
    "crumbs": [
      "Guardium Insights",
      "EC2",
      "Preparation"
    ]
  },
  {
    "objectID": "src/guardium_insights/ec2/02-preparation.html#deployment-steps",
    "href": "src/guardium_insights/ec2/02-preparation.html#deployment-steps",
    "title": "Preparation",
    "section": "Deployment Steps",
    "text": "Deployment Steps\n\nCreate OCPInstall Role\nDownload the OCPInstallRole.yaml\nNote: This template assumes that roles BootNodeRole and DevOps-Role already exist. - Download DefaultRoles.yaml and apply if needed - Edit the OCPInstall role yaml and add account or add substitution and set role names appropriately - Set optional tags on the command\nCreate the role by running the following command:\naws cloudformation deploy --stack-name OCPInstall-role-1 --template-file OCPInstallRole.yaml --capabilities CAPABILITY_NAMED_IAM --tags *add Key=Value tag here*\n\n\nCreate LambdaExecution Role\nDownload the LambdaExecutionRole.yaml Create the role by running the following command:\naws cloudformation deploy --stack-name LambdaExecutionRole --template-file LambdaExecutionRole.yaml --capabilities CAPABILITY_NAMED_IAM --tags *add Key=Value tag here*\n\n\nDeploy CloudFormation templates using AWS CLI\nIf you’re creating a new VPC\nexport region=us-east-2 # This should be whatever region you are deploying in\nexport CLUSTERNAME=GI-OCP # This should be whatever you want to call your cluster\nDownload the following file:\nvpc-template.yaml\nCreate a file called vpc-params.json\ncat &lt;&lt; EOF &gt; vpc-params.json\n[\n  {\n    \"ParameterKey\": \"ClusterName\", \n    \"ParameterValue\": \"${CLUSTERNAME}\" \n  },\n  {\n    \"ParameterKey\": \"AvailabilityZones\", \n    \"ParameterValue\": \"${region}a,${region}b,${region}c\"\n  },\n  {\n    \"ParameterKey\": \"CreateNATGateways\", \n    \"ParameterValue\": \"true\" \n  },\n  {\n    \"ParameterKey\": \"CreatePublicSubnets\", \n    \"ParameterValue\": \"true\" \n  },\n  {\n    \"ParameterKey\": \"CreatePrivateSubnets\", \n    \"ParameterValue\": \"true\" \n  },\n  {\n    \"ParameterKey\": \"NumberOfAZs\",\n    \"ParameterValue\": \"3\"\n  },\n  {\n    \"ParameterKey\": \"PrivateSubnet1CIDR\",\n    \"ParameterValue\": \"192.168.0.0/19\"\n  },\n  {\n    \"ParameterKey\": \"PrivateSubnet2CIDR\",\n    \"ParameterValue\": \"192.168.32.0/19\"\n  },\n  {\n    \"ParameterKey\": \"PrivateSubnet3CIDR\",\n    \"ParameterValue\": \"192.168.64.0/19\"\n  },\n  {\n    \"ParameterKey\": \"PrivateSubnet1Tag1\",\n    \"ParameterValue\": \"Name=${CLUSTERNAME}-private-subnet-1\"\n  },\n  {\n    \"ParameterKey\": \"PrivateSubnet2Tag1\",\n    \"ParameterValue\": \"Name=${CLUSTERNAME}-private-subnet-2\"\n  },\n  {\n    \"ParameterKey\": \"PrivateSubnet3Tag1\",\n    \"ParameterValue\": \"Name=${CLUSTERNAME}-private-subnet-3\"\n  },\n  {\n    \"ParameterKey\": \"PublicSubnet1CIDR\",\n    \"ParameterValue\": \"192.168.96.0/21\"\n  },\n  {\n    \"ParameterKey\": \"PublicSubnet1Tag1\",\n    \"ParameterValue\": \"Name=${CLUSTERNAME}-public-subnet-1\"\n  },\n  {\n    \"ParameterKey\": \"PublicSubnet2CIDR\",\n    \"ParameterValue\": \"192.168.104.0/21\"\n  },\n  {\n    \"ParameterKey\": \"PublicSubnet2Tag1\",\n    \"ParameterValue\": \"Name=${CLUSTERNAME}-public-subnet-2\"\n  },\n  {\n    \"ParameterKey\": \"PublicSubnet3CIDR\",\n    \"ParameterValue\": \"192.168.112.0/21\"\n  },\n  {\n    \"ParameterKey\": \"PublicSubnet3Tag1\",\n    \"ParameterValue\": \"Name=${CLUSTERNAME}-public-subnet-3\"\n  },\n  {\n    \"ParameterKey\": \"VPCCIDR\",\n    \"ParameterValue\": \"192.168.0.0/16\"\n  },\n  {\n    \"ParameterKey\": \"VPCDNSNameResolution\",\n    \"ParameterValue\": \"true\"\n  },\n  {\n    \"ParameterKey\": \"VPCDNSNameCreation\",\n    \"ParameterValue\": \"true\"\n  }\n]\nEOF\nUsing the OCPInstall role arn, run the following command to create the VPC:\naws cloudformation create-stack \\\n--stack-name ${CLUSTERNAME} \\\n--template-body file://vpc-template.yaml \\\n--parameters file://vpc-params.json\nUsing the OCPInstall role arn, run the following command to start the main cloudformation deployment:\n\nIf you are developing, add --disable-rollback flag to the following command to disable the automatic rollback and deletion of the bastion node on failures. This will allow you to troubleshoot in the event that the stack does not complete successfully.\n\naws cloudformation deploy \\\n--stack-name stack-deployment-1 \\\n--template-file cluster.yaml \\\n--parameter-overrides file://parameters-override.yaml \\\n--capabilities CAPABILITY_NAMED_IAM \\\n--tags *add Key=Value tag here* \\\n--role-arn arn:aws:iam::&lt;ACCOUNT&gt;:role/OCPInstall\nCheck the AWS Console to see when the cloudformation template has progressed far enough that the bootnode is online.\nUsing the ssh key from the Key Pair name used in parameters-override.yaml, ssh to the bootnode. The status of the install will be found in the log files under ~ec2-user/cpd-status/logs.\n\nIf the install fails but some resources were already created, some clean up might be required in addition to deleting the stack. Find and delete the following before running the installation again.\n\nDNS A records on the hosted Zones\nEC2 instances (control plane and worker nodes)\nEC2 load balancers\nEFS file systems\n\n\n\n\nMonitoring\n\nSSM into bootnode\nAdd AmazonSSMManagedInstanceCore policy to role used to execute the cloudform template and the user/role that will be connecting to the instance.\nThe cloudform template creates a boot node that will begin executing commands. One set of commands installs, enables, and starts amazon-ssm-agent. It may take up to 20 minutes before this agent comes online in the boot node.\nOnce the instance has started the ssm agent a connection can be initiated with following command:\naws ssm start-session --target $InstanceID\nOnce a connection has been opened, you may need to change users to the ‘ec2-user’. This can be accomplished with the following commands:\nBecome root\nsudo su\nBecome ec2-user\nsu ec2-user\nYou will now be able to review deployment logs.\n\n\n Fixing aws command in SSM \n\nSSM does not work exactly the same as SSH. If you intend to use any additional commands, such as aws, then you need to do the following:\nCheck the output of running the aws command, If there is an error message like this:\n[47863] Error loading Python lib '/usr/bin/libpython3.11.so.1.0': dlopen: /usr/bin/libpython3.11.so.1.0: cannot open shared object file: No such file or directory\nAnother possible error message:\n$ aws\nPython path configuration:\n  PYTHONHOME = '/usr/bin'\n  PYTHONPATH = (not set)\n  program name = '/usr/bin/aws'\n  isolated = 0\n  environment = 0\n  user site = 0\n  safe_path = 0\n  import site = 0\n  is in build tree = 0\n  stdlib dir = ''\n  sys._base_executable = '/usr/bin/aws'\n  sys.base_prefix = ''\n  sys.base_exec_prefix = ''\n  sys.platlibdir = 'lib'\n  sys.executable = '/usr/bin/aws'\n  sys.prefix = ''\n  sys.exec_prefix = ''\n  sys.path = [\n    '/usr/bin/base_library.zip',\n    '/usr/bin/lib-dynload',\n    '/usr/bin',\n  ]\nFatal Python error: init_fs_encoding: failed to get the Python codec of the filesystem encoding\nPython runtime state: core initialized\nModuleNotFoundError: No module named 'encodings'\n\nCurrent thread 0x00007fed39a06c00 (most recent call first):\n  &lt;no Python frame&gt;\nYou may not have the correct $PATH.\nIncorrect \\(PATH:\n```\\) echo $PATH /home/ec2-user/.local/bin:/home/ec2-user/bin:/usr/bin:/usr/sbin\n\nHow to Correct $PATH:\n\nexport PATH=“/home/ec2-user/.local/bin:/home/ec2-user/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin”\n\nHow to persist the change to $PATH:\n\necho ‘export PATH=“\\(HOME/.local/bin:\\)HOME/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin”’ &gt;&gt; .bashrc\n\nConfirm this change works:\n\n$ echo \\(PATH\n/home/ec2-user/.local/bin:/home/ec2-user/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin\\) aws\nusage: aws [options]   [ …] [parameters] To see help text, you can run:\naws help aws  help aws   help\naws: error: the following arguments are required: command\n\n&lt;/details&gt;\n\n#### Monitor the deployment\n\nCheck what folders exist in the ec2-home directory. if \"cpd-status\" has not been created yet, then wait a few minutes. Once \"cpd-status\" directory appears, run the following command:\n\ntail -f ~/cpd-status/log/cloud-pak-deployer.log ```\nThis command will show the log file from the cp-deployer process.",
    "crumbs": [
      "Guardium Insights",
      "EC2",
      "Preparation"
    ]
  },
  {
    "objectID": "src/guardium_insights/ec2/01-prereqs.html",
    "href": "src/guardium_insights/ec2/01-prereqs.html",
    "title": "Prerequisites",
    "section": "",
    "text": "Installing OpenShift and Cloud Pak for Data requires specific access, resources and considerations on AWS. In this documentation, we are assuming a CloudFormation install on user-provisioned infrastructure.",
    "crumbs": [
      "Guardium Insights",
      "EC2",
      "Prerequisites"
    ]
  },
  {
    "objectID": "src/guardium_insights/ec2/01-prereqs.html#redhat-requirements",
    "href": "src/guardium_insights/ec2/01-prereqs.html#redhat-requirements",
    "title": "Prerequisites",
    "section": "RedHat Requirements",
    "text": "RedHat Requirements\n\nRedHat OCP Account and Pull Secret\nThis is the required account used for the OCP licensing. Use this URL to download your RedHat Pull Secret: https://console.redhat.com/openshift/install/pull-secret\nIf your organization does not own an OCP license, a trial license can be used for OCP and is valid for 60 days. Steps listed in the expandable section below.\n\n\nObtain a RedHat Trial Account\n\nGo to www.redhat.com and click on “Log In”.\n\n\n\nredhat_login\n\n\nThen click on “Register for a Red Hat Account”.\n\n\n\nredhat_register\n\n\nProvide the requested information and “Create Account”.\nYou will receive a verification email. Click on the link in the email to confirm.\nNow go to www.openshift.com and log in with your RedHat account. After logging in you will see this:\n\n\n\nopenshift_try_it\n\n\nClick on “Try It” which will bring you to a page of different OpenShift versions to try, find the following version and click “Start your Trial”:\n\n\n\nopenshift_start_trial\n\n\nA new page will load and you will need to enter all the required information before clicking “Submit”.\nYou can now log into https://console.redhat.com/openshift/install/pull-secret and download the pull-secret.",
    "crumbs": [
      "Guardium Insights",
      "EC2",
      "Prerequisites"
    ]
  },
  {
    "objectID": "src/guardium_insights/ec2/01-prereqs.html#ibm-requirements",
    "href": "src/guardium_insights/ec2/01-prereqs.html#ibm-requirements",
    "title": "Prerequisites",
    "section": "IBM Requirements",
    "text": "IBM Requirements\nGuardium Insights is part of IBM’s entitled software program. In order to download and install it you will need your IBM entitlement key which can be retrieved here.\nYou will need your valid IBM id in order to login.",
    "crumbs": [
      "Guardium Insights",
      "EC2",
      "Prerequisites"
    ]
  },
  {
    "objectID": "src/guardium_insights/ec2/01-prereqs.html#aws-requirements",
    "href": "src/guardium_insights/ec2/01-prereqs.html#aws-requirements",
    "title": "Prerequisites",
    "section": "AWS Requirements",
    "text": "AWS Requirements\n\nIAM User Requirements\nIf deploying using an IAM user, below are the requirements, roles, and steps.\nEach Amazon Web Services (AWS) account contains a root user account that is based on the email address you used to create the account. This is a highly-privileged account, and it is recommended to use it for only initial account and billing configuration, creating an initial set of users, and securing the account. Before you install OpenShift Container Platform, create a secondary IAM administrative user. As you complete the Creating an IAM User in Your AWS Account procedure in the AWS documentation, set the following options:\n\nSpecify the IAM user name and select Programmatic access.\nAttach the AdministratorAccess policy to ensure that the account has sufficient permission to create the cluster. This policy provides the cluster with the ability to grant credentials to each OpenShift Container Platform component. The cluster grants the components only the credentials that they require.\n\n\nWhile it is possible to create a policy that grants the all of the required AWS permissions and attach it to the user, this is not the preferred option. The cluster will not have the ability to grant additional credentials to individual components, so the same credentials are used by all components.\n\n\nOptional: Add metadata to the user by attaching tags.\nConfirm that the user name that you specified is granted the AdministratorAccess policy.\nRecord the access key ID and secret access key values. You must use these values when you configure your local machine to run the installation program.\n\n\nYou cannot use a temporary session token that you generated while using a multi-factor authentication device to authenticate to AWS when you deploy a cluster. The cluster continues to use your current AWS credentials to create AWS resources for the entire life of the cluster, so you must use key-based, long-term credentials.\n\nYour IAM user must have the permission tag:GetResources in the region us-east-1 to delete the base cluster resources. As part of the AWS API requirement, the OpenShift Container Platform installation program performs various actions in this region.\n\nWhen you attach the AdministratorAccess policy to the IAM user that you create in Amazon Web Services (AWS), you grant that user all of the required permissions.\n\nTo deploy all components of an OpenShift Container Platform cluster, the IAM user requires the following permissions:\n\nRequired Permissions (if not using Administrator Access)\n\n\nRequired EC2 permissions for installation\n\nec2:AuthorizeSecurityGroupEgress\n\nec2:AuthorizeSecurityGroupIngress\n\nec2:CopyImage\n\nec2:CreateNetworkInterface\n\nec2:AttachNetworkInterface\n\nec2:CreateSecurityGroup\n\nec2:CreateTags\n\nec2:CreateVolume\n\nec2:DeleteSecurityGroup\n\nec2:DeleteSnapshot\n\nec2:DeleteTags\n\nec2:DeregisterImage\n\nec2:DescribeAccountAttributes\n\nec2:DescribeAddresses\n\nec2:DescribeAvailabilityZones\n\nec2:DescribeDhcpOptions\n\nec2:DescribeImages\n\nec2:DescribeInstanceAttribute\n\nec2:DescribeInstanceCreditSpecifications\n\nec2:DescribeInstances\n\nec2:DescribeInstanceTypes\n\nec2:DescribeInternetGateways\n\nec2:DescribeKeyPairs\n\nec2:DescribeNatGateways\n\nec2:DescribeNetworkAcls\n\nec2:DescribeNetworkInterfaces\n\nec2:DescribePrefixLists\n\nec2:DescribeRegions\n\nec2:DescribeRouteTables\n\nec2:DescribeSecurityGroups\n\nec2:DescribeSubnets\n\nec2:DescribeTags\n\nec2:DescribeVolumes\n\nec2:DescribeVpcAttribute\n\nec2:DescribeVpcClassicLink\n\nec2:DescribeVpcClassicLinkDnsSupport\n\nec2:DescribeVpcEndpoints\n\nec2:DescribeVpcs\n\nec2:GetEbsDefaultKmsKeyId\n\nec2:ModifyInstanceAttribute\n\nec2:ModifyNetworkInterfaceAttribute\n\nec2:RevokeSecurityGroupEgress\n\nec2:RevokeSecurityGroupIngress\n\nec2:RunInstances\n\nec2:TerminateInstances\n\n\n\nRequired permissions for creating network resources during installation\n\nec2:AllocateAddress\n\nec2:AssociateAddress\n\nec2:AssociateDhcpOptions\n\nec2:AssociateRouteTable\n\nec2:AttachInternetGateway\n\nec2:CreateDhcpOptions\n\nec2:CreateInternetGateway\n\nec2:CreateNatGateway\n\nec2:CreateRoute\n\nec2:CreateRouteTable\n\nec2:CreateSubnet\n\nec2:CreateVpc\n\nec2:CreateVpcEndpoint\n\nec2:ModifySubnetAttribute\n\nec2:ModifyVpcAttribute\n\nIf you use an existing VPC, your account does not require these permissions for creating network resources.\n\n\n\n\nRequired Elastic Load Balancing permissions (ELB) for installation\n\nelasticloadbalancing:AddTags\nelasticloadbalancing:ApplySecurityGroupsToLoadBalancer\nelasticloadbalancing:AttachLoadBalancerToSubnets\nelasticloadbalancing:ConfigureHealthCheck\nelasticloadbalancing:CreateLoadBalancer\nelasticloadbalancing:CreateLoadBalancerListeners\nelasticloadbalancing:DeleteLoadBalancer\nelasticloadbalancing:DeregisterInstancesFromLoadBalancer\nelasticloadbalancing:DescribeInstanceHealth\nelasticloadbalancing:DescribeLoadBalancerAttributes\nelasticloadbalancing:DescribeLoadBalancers\nelasticloadbalancing:DescribeTags\nelasticloadbalancing:ModifyLoadBalancerAttributes\nelasticloadbalancing:RegisterInstancesWithLoadBalancer\nelasticloadbalancing:SetLoadBalancerPoliciesOfListener\n\n\n\nRequired Elastic Load Balancing permissions (ELBv2) for installation\n\nelasticloadbalancing:AddTags\n\nelasticloadbalancing:CreateListener\n\nelasticloadbalancing:CreateLoadBalancer\n\nelasticloadbalancing:CreateTargetGroup\n\nelasticloadbalancing:DeleteLoadBalancer\n\nelasticloadbalancing:DeregisterTargets\n\nelasticloadbalancing:DescribeListeners\n\nelasticloadbalancing:DescribeLoadBalancerAttributes\n\nelasticloadbalancing:DescribeLoadBalancers\n\nelasticloadbalancing:DescribeTargetGroupAttributes\n\nelasticloadbalancing:DescribeTargetHealth\n\nelasticloadbalancing:ModifyLoadBalancerAttributes\n\nelasticloadbalancing:ModifyTargetGroup\n\nelasticloadbalancing:ModifyTargetGroupAttributes\n\nelasticloadbalancing:RegisterTargets\n\n\n\nRequired IAM permissions for installation\n\niam:AddRoleToInstanceProfile\n\niam:CreateInstanceProfile\n\niam:CreateRole\n\niam:DeleteInstanceProfile\n\niam:DeleteRole\n\niam:DeleteRolePolicy\n\niam:GetInstanceProfile\n\niam:GetRole\n\niam:GetRolePolicy\n\niam:GetUser\n\niam:ListInstanceProfilesForRole\n\niam:ListRoles\n\niam:ListUsers\n\niam:PassRole\n\niam:PutRolePolicy\n\niam:RemoveRoleFromInstanceProfile\n\niam:SimulatePrincipalPolicy\n\niam:TagRole\n\nIf you have not created a load balancer in your AWS account, the IAM user also requires the iam:CreateServiceLinkedRole permission.\n\n\n\n\nRequired Route 53 permissions for installation\n\nroute53:ChangeResourceRecordSets\n\nroute53:ChangeTagsForResource\n\nroute53:CreateHostedZone\n\nroute53:DeleteHostedZone\n\nroute53:GetChange\n\nroute53:GetHostedZone\n\nroute53:ListHostedZones\n\nroute53:ListHostedZonesByName\n\nroute53:ListResourceRecordSets\n\nroute53:ListTagsForResource\n\nroute53:UpdateHostedZoneComment\n\n\n\nRequired S3 permissions for installation\n\ns3:CreateBucket\n\ns3:DeleteBucket\n\ns3:GetAccelerateConfiguration\n\ns3:GetBucketAcl\n\ns3:GetBucketCors\n\ns3:GetBucketLocation\n\ns3:GetBucketLogging\n\ns3:GetBucketPolicy\n\ns3:GetBucketObjectLockConfiguration\n\ns3:GetBucketRequestPayment\n\ns3:GetBucketTagging\n\ns3:GetBucketVersioning\n\ns3:GetBucketWebsite\n\ns3:GetEncryptionConfiguration\n\ns3:GetLifecycleConfiguration\n\ns3:GetReplicationConfiguration\n\ns3:ListBucket\n\ns3:PutBucketAcl\n\ns3:PutBucketTagging\n\ns3:PutEncryptionConfiguration\n\n\n\nS3 permissions that cluster Operators required\n\ns3:DeleteObject\n\ns3:GetObject\n\ns3:GetObjectAcl\n\ns3:GetObjectTagging\n\ns3:GetObjectVersion\n\ns3:PutObject\n\ns3:PutObjectAcl\n\ns3:PutObjectTagging\n\n\n\nRequired permissions to delete base cluster resources\n\nautoscaling:DescribeAutoScalingGroups\n\nec2:DeletePlacementGroup\n\nec2:DeleteNetworkInterface\n\nec2:DeleteVolume\n\nelasticloadbalancing:DeleteTargetGroup\n\nelasticloadbalancing:DescribeTargetGroups\n\niam:DeleteAccessKey\n\niam:DeleteUser\n\niam:ListAttachedRolePolicies\n\niam:ListInstanceProfiles\n\niam:ListRolePolicies\n\niam:ListUserPolicies\n\ns3:DeleteObject\n\ns3:ListBucketVersions\n\ntag:GetResources\n\n\n\nRequired permissions to delete network resources\n\nec2:DeleteDhcpOptions\n\nec2:DeleteInternetGateway\n\nec2:DeleteNatGateway\n\nec2:DeleteRoute\n\nec2:DeleteRouteTable\n\nec2:DeleteSubnet\n\nec2:DeleteVpc\n\nec2:DeleteVpcEndpoints\n\nec2:DetachInternetGateway\n\nec2:DisassociateRouteTable\n\nec2:ReleaseAddress\n\nec2:ReplaceRouteTableAssociation\n\nIf you use an existing VPC, your account does not require these permissions to delete network resources. Instead, your account only requires the tag:UntagResources permission to delete network resources.\n\n\n\n\nRequired permissions to delete a cluster with shared instance roles\n\niam:UntagRole\n\n\n\nAdditional IAM and S3 permissions that are required to create manifests\n\niam:DeleteAccessKey\n\niam:DeleteUser\n\niam:DeleteUserPolicy\n\niam:GetUserPolicy\n\niam:ListAccessKeys\n\niam:PutUserPolicy\n\niam:TagUser\n\ns3:PutBucketPublicAccessBlock\n\ns3:GetBucketPublicAccessBlock\n\ns3:PutLifecycleConfiguration\n\ns3:HeadBucket\n\ns3:ListBucketMultipartUploads\n\ns3:AbortMultipartUpload\n\nIf you are managing your cloud provider credentials with mint mode, the IAM user also requires the iam:CreateAccessKey and iam:CreateUser permissions.\n\n\n\n\nOptional permissions for instance and quota checks for installation\n\nec2:DescribeInstanceTypeOfferings\n\nservicequotas:ListAWSDefaultServiceQuotas\n\n\n\nOptional permissions for the cluster owner account when installing a cluster on a shared VPC\n\nsts:AssumeRole\n\n\nFor more details on the IAM user requirements, see https://docs.openshift.com/container-platform/4.16/installing/installing_aws/installing-aws-account.html#installing-aws-account\n\n\n\nAWS Role Permissions Checker\nBash script that can be run on linux or OS X. Requires AWS CLI to be installed and configured.\n\nScript user will need the following permissions:\n\niam:SimulatePrincipalPolicy\niam:GetPolicy\niam:GetPolicyVersion\n\n\nDownload the script\nOr run the following commands:\nwget /assets/script/check-permissions.sh\nchmod +x check-permission.sh\nTo run the script you will need to download the “permissions.txt” file.\nDownload permissions.txt\nScript Syntax\n./check-permissions.sh &lt;ROLE&gt; &lt;PERMISSIONS FILE&gt;\nThe permissions file is long and will take a few minutes for the script to work through. The output from the script is a file named “missing_permissions.txt”.\n\n\n\nRequired AWS Infrastructure Components\nTo install OpenShift Container Platform on user-provisioned infrastructure in Amazon Web Services (AWS), you must manually create both the machines and their supporting infrastructure. For more information about the integration testing for different platforms, see the OpenShift Container Platform 4.x Tested Integrations page.\nBy using the provided CloudFormation templates, you can create stacks of AWS resources that represent the following components:\n\nAn AWS Virtual Private Cloud (VPC)\nNetworking and load balancing components\nSecurity groups and roles\nAn OpenShift Container Platform bootstrap node\nOpenShift Container Platform control plane nodes\nAn OpenShift Container Platform compute node\n\nAlternatively, you can manually create the components or you can reuse existing infrastructure that meets the cluster requirements. Review the CloudFormation templates for more details about how the components interrelate.\nOther infrastructure components:\n\nA VPC\nDNS entries\n\nLoad balancers (classic or network) and listeners\nA public and a private Route 53 zone\nSecurity groups\nIAM roles\nS3 buckets\n\n\nRequired VPC Components\nYou must provide a suitable VPC and subnets that allow communication to your machines:\n\n\n\n\n\n\n\n\nComponent\nAWS type\nDescription\n\n\n\n\nVPC\nAWS::EC2::VPC AWS::EC2::VPCEndpoint\nYou must provide a public VPC for the cluster to use. The VPC uses an endpoint that references the route tables for each subnet to improve communication with the registry that is hosted in S3.\n\n\nPublic subnets\nAWS::EC2::Subnet AWS::EC2::SubnetNetworkAclAssociation\nYour VPC must have public subnets for between 1 and 3 availability zones and associate them with appropriate Ingress rules.\n\n\nInternet gateway\nAWS::EC2::InternetGateway AWS::EC2::VPCGatewayAttachment AWS::EC2::RouteTable AWS::EC2::Route AWS::EC2::SubnetRouteTableAssociation AWS::EC2::NatGateway AWS::EC2::EIP\nYou must have a public internet gateway, with public routes, attached to the VPC. In the provided templates, each public subnet has a NAT gateway with an EIP address. These NAT gateways allow cluster resources, like private subnet instances, to reach the internet and are not required for some restricted network or proxy scenarios.\n\n\nNetwork access control\nAWS::EC2::NetworkAcl AWS::EC2::NetworkAclEntry\nRequired Ports: 80, 443, 22, 1024-65535, 0-65535\n\n\nPrivate subnets\nAWS::EC2::Subnet AWS::EC2::RouteTable AWS::EC2::SubnetRouteTableAssociation\nYour VPC can have private subnets. The provided CloudFormation templates can create private subnets for between 1 and 3 availability zones. If you use private subnets, you must provide appropriate routes and tables for them.\n\n\n\n\n\nRequired DNS and load balancing components\nYour DNS and load balancer configuration needs to use a public hosted zone and can use a private hosted zone similar to the one that the installation program uses if it provisions the cluster’s infrastructure. You must create a DNS entry that resolves to your load balancer. An entry for api.cluster_name.domain must point to the external load balancer, and an entry for api-int.cluster_name.domain must point to the internal load balancer.\nThe cluster also requires load balancers and listeners for port 6443, which are required for the Kubernetes API and its extensions, and port 22623, which are required for the Ignition config files for new machines. The targets will be the control plane nodes. Port 6443 must be accessible to both clients external to the cluster and nodes within the cluster. Port 22623 must be accessible to nodes within the cluster.\n\n\n\n\n\n\n\n\nComponent\nAWS type\nDescription\n\n\n\n\nDNS\nAWS::Route53::HostedZone\nThe hosted zone for your internal DNS.\n\n\nPublic load balancer\nAWS::ElasticLoadBalancingV2::LoadBalancer\nThe load balancer for your public subnets.\n\n\nExternal API server record\nAWS::Route53::RecordSetGroup\nAlias records for the external API server.\n\n\nExternal listener\nAWS::ElasticLoadBalancingV2::Listener\nA listener on port 6443 for the external load balancer.\n\n\nExternal target group\nAWS::ElasticLoadBalancingV2::TargetGroup\nThe target group for the external load balancer.\n\n\nPrivate load balancer\nAWS::ElasticLoadBalancingV2::LoadBalancer\nThe load balancer for your private subnets.\n\n\nInternal API server record\nAWS::Route53::RecordSetGroup\nAlias records for the internal API server.\n\n\nInternal listener\nAWS::ElasticLoadBalancingV2::Listener\nA listener on port 22623 for the internal load balancer.\n\n\nInternal target group\nAWS::ElasticLoadBalancingV2::TargetGroup\nThe target group for the internal load balancer.\n\n\nInternal listener\nAWS::ElasticLoadBalancingV2::Listener\nA listener on port 6443 for the internal load balancer.\n\n\nInternal target group\nAWS::ElasticLoadBalancingV2::TargetGroup\nThe target group for the internal load balancer.\n\n\n\n\n\nRequired Security groups\nThe control plane and worker machines require access to the following ports:\n\n\n\n\n\n\n\n\n\nGroup\nType\nIP Protocol\nPort range\n\n\n\n\nMasterSecurityGroup\nAWS::EC2::SecurityGroup\nicmp\n0\n\n\n\n\ntcp\n22\n\n\n\n\ntcp\n6443\n\n\n\n\ntcp\n22623\n\n\nWorkerSecurityGroup\nAWS::EC2::SecurityGroup\nicmp\n0\n\n\n\n\ntcp\n22\n\n\nBootstrapSecurityGroup\nAWS::EC2::SecurityGroup\ntcp\n22\n\n\n\n\ntcp\n19531\n\n\n\n\n\nControl plane Ingress\nThe control plane machines require the following Ingress groups. Each Ingress group is a AWS::EC2::SecurityGroupIngress resource.\n\n\n\n\n\n\n\n\n\nGroup\nType\nIP Protocol\nPort range\n\n\n\n\nMasterSecurityGroup\nAWS::EC2::SecurityGroup\nicmp\n0\n\n\n\n\ntcp\n22\n\n\n\n\ntcp\n6443\n\n\n\n\ntcp\n22623\n\n\nWorkerSecurityGroup\nAWS::EC2::SecurityGroup\nicmp\n0\n\n\n\n\ntcp\n22\n\n\nBootstrapSecurityGroup\nAWS::EC2::SecurityGroup\ntcp\n22\n\n\n\n\ntcp\n19531\n\n\n\n\n\nWorker Ingress\nThe worker machines require the following Ingress groups. Each Ingress group is a AWS::EC2::SecurityGroupIngress resource.\n\n\n\n\n\n\n\n\n\nGroup\nType\nIP Protocol\nPort range\n\n\n\n\nMasterSecurityGroup\nAWS::EC2::SecurityGroup\nicmp\n0\n\n\n\n\ntcp\n22\n\n\n\n\ntcp\n6443\n\n\n\n\ntcp\n22623\n\n\nWorkerSecurityGroup\nAWS::EC2::SecurityGroup\nicmp\n0\n\n\n\n\ntcp\n22\n\n\nBootstrapSecurityGroup\nAWS::EC2::SecurityGroup\ntcp\n22\n\n\n\n\ntcp\n19531\n\n\n\n\n\nCluster machines\nYou need AWS::EC2::Instance objects for the following machines:\n\nA bootstrap machine. This machine is required during installation, but you can remove it after your cluster deploys.\nThree control plane machines. The control plane machines are not governed by a control plane machine set.\nCompute machines. You must create at least two compute machines, which are also known as worker machines, during installation. These machines are not governed by a compute machine set.\n\n\nFor more information on OCP on AWS, see https://docs.openshift.com/container-platform/4.16/installing/installing_aws/installing-aws-user-infra.html The “Minimum requirements” section does not consider the watsonx.ai and related services, and is accounted for in this documentation\n\n\n\n\nParameters for CloudFormation Template\nThe following parameters must be gathered from the AWS infrastructure in order to use the CloudFormation template:\n\nBootNodeAccessCIDR:\nvpc_id:\naws_region:\n\nExample: us-east-2\n\nmachine_cidr:\n\nExample: 10.2.1.0/24\n\nopenshift_cluster_network_cidr:\n\nExample: 10.128.0.0/14\n\nsubnet_ids: :::note For subnets, at least one set of 3 (Public or Private) subnets must be used. :::\n\nPublic\n\nPublic subnet1\nPublic subnet2\nPublic subnet3\n\nPrivate\n\nPrivate subnet1\nPrivate subnet2\nPrivate subnet3\n\n\nhosted_zone_id:\n\nExample: Hosted_Zone_Id\n\nimage_registry:\nname: Registy\nregistry_host_name:\n\nExample: registry.example.com\n\nregistry_port:\n\nExample: 5000\n\nregistry_insecure:\n\nExample: false\n\nregistry_trusted_ca_secret:\n\nExample: cpd463-ca-bundle\n\nhttp_proxy:\nhttps_proxy:\nno_proxy:\nKeyPairName: :::note Key Pair generation instructions here :::\nRedhatPullSecret: :::note RedHat Pull Secret instructions here :::\nDomainName:\nClusterName:",
    "crumbs": [
      "Guardium Insights",
      "EC2",
      "Prerequisites"
    ]
  },
  {
    "objectID": "src/guardium_insights/eks/01_pre-reqs.html",
    "href": "src/guardium_insights/eks/01_pre-reqs.html",
    "title": "Pre-Reqs",
    "section": "",
    "text": "IBM Entitlement Key\n\n\n\nIt’s important to note that you will need an IBM Entitlement Key to install the IBM Foundational Services as well as Guardium Insights.\nThis can be retrieved from here",
    "crumbs": [
      "Guardium Insights",
      "EKS",
      "Prerequisites"
    ]
  },
  {
    "objectID": "src/guardium_insights/eks/01_pre-reqs.html#installing-required-cli-tools",
    "href": "src/guardium_insights/eks/01_pre-reqs.html#installing-required-cli-tools",
    "title": "Pre-Reqs",
    "section": "Installing required cli tools",
    "text": "Installing required cli tools\n\nWindows Exec Path\nIf your environment is Windows, let’s set a path for some of these executables to run from:\nRight click on the Windows badge at the lower left hand slide and select System\n\nScroll down and select Advanced system settings\n\nSelect Environment Variables\n\nUnder the env variables, select Path under System variables and hit Edit\n\nClick New and let’s create a new directory under C: called Bin\n\nOnce that is saved, create the new directory under C:\n\nC:\\Bin is where we will drop some of our executables.\n\n\nPackage managers\n\nMacOSWindows\n\n\nHomebrew for MacOS\nInstall homebrew with the following command\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\nInstall Gnu-Sed\nMacOS and Linux use different versions of sed. Let’s install Gnu-Sed for Mac with Homebrew as it will make some commands easier.\nbrew install gnu-sed\n\n\n\nScoop for Windows\nThis installs the scoop package manager. This just makes it easier to install some of the pre-reqs in Windows.\nRun the following commands in a powershell window one line at a time:\nSet-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser\niex \"& {$(irm get.scoop.sh)} -RunAsAdmin\"\n\n\n\n\n\nInstalling AWS CLI\n\nMacOSWindows\n\n\ncurl \"https://awscli.amazonaws.com/AWSCLIV2.pkg\" -o \"AWSCLIV2.pkg\"\nInstall it with sudo (to use for all users)\nsudo installer -pkg ./AWSCLIV2.pkg -target /\n\n\nmsiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi\n\n\n\nNow let’s configure our client env\naws configure\nAnswer all the questions with the info you got for your account.\n\n\nInstalling helm\n\nMacOSWindows\n\n\ncurl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash\n\n\nTo install helm to windows with scoop\nscoop install helm\n\n\n\n\n\nInstalling kubectl\n\nMacOSWindows\n\n\nInstall kubectl with\nbrew install kubectl\n\n\nIn a powershell window, download the kubectl binary from Amazon S3\ncurl.exe -OL https://s3.us-west-2.amazonaws.com/amazon-eks/1.30.2/2024-07-12/bin/windows/amd64/kubectl.exe\nCopy the binary to C:\\Bin\n\n\n\n\n\nInstalling eksctl\n\nMacOSWindows\n\n\nTo install eksctl we’re going to use homebrew\nbrew tap weaveworks/tap\n\nbrew install weaveworks/tap/eksctl\n\n\nIn a powershell window:\nscoop install eksctl\nVerify the installation\neksctl info\n\neksctl version: 0.187.0\nkubectl version: v1.30.2-eks-1552ad0\nOS: windows\n\n\n\n\n\nInstalling Openshift CLI\n\n\n\n\n\n\nNote\n\n\n\nthe oc cli is required to work with the IBM Cloud Pak plugin. This plugin is needed for part of the GI installation.\nIt also needs to be 4.10.x\nWe are installing 4.10.67\n\n\n\nMacOSWindows\n\n\nIf /usr/local/bin doesn’t exist, create the directory path.\nsudo mkdir -p /usr/local/bin\nThen download the oc tar gzip and install.\ncurl -fsSL -o openshift-client-mac.tar.gz https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.10.67/openshift-client-mac.tar.gz\nsudo tar -C /usr/local/bin -zxf openshift-client-mac.tar.gz oc\nThis should install client version 4.10.67 to /usr/local/bin.\nVerify this with:\noc version --client\nClient Version: 4.10.67\n\n\nIn a powershell window, download the openshift-cli tar gz.\ncurl.exe -OL https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.10.67/openshift-client-windows.zip\nExtract the zip file\nExpand-Archive -LiteralPath 'C:\\Users\\Administrator\\openshift-client-windows.zip' -DestinationPath 'C:\\Bin'\nVerify it works\noc version --client\n\nClient Version: 4.10.67\nVerify the installation in a powershell window\n\n\n\n\n\nInstalling oc-ibm_pak\noc-ibm_pak is a plugin for the oc cli for managing cloud-paks.\n\nMacOSWindows\n\n\nDownload the plugin with curl. At this writing, version 1.15.1 is the most recent.\ncurl -L https://github.com/IBM/ibm-pak/releases/download/v1.15.1/oc-ibm_pak-darwin-amd64.tar.gz -o oc-ibm_pak-darwin-amd64.tar.gz\nExtract and copy the plugin to /usr/local/bin\ntar -zxf oc-ibm_pak-darwin-amd64.tar.gz\nsudo cp oc-ibm_pak-darwin-amd64 /usr/local/bin/oc-ibm_pak\nVerify the installation\noc ibm-pak --help\n\n\nIn a powershell window, download the plugin with curl.exe. At this writing, version 1.15.1 is the most recent.\ncurl.exe -OL https://github.com/IBM/ibm-pak/releases/download/v1.15.1/oc-ibm_pak-windows-amd64.tar.gz -o oc-ibm_pak-windows-amd64.tar.gz\nExtract the plugin\ntar -xvf oc-ibm_pak-windows-amd64.tar.gz\nCopy the plugin to our exec path\nCopy-Item oc-ibm_pak-windows-amd64 C:\\Bin\\oc-ibm_pak.exe\nVerify the installation\noc ibm-pak --help\n\n\n\n\n\nInstall IBM Case CLI\n\nMacOSWindows\n\n\nDownload the latest release of the Case CLI\nTBD\nExtract it and move it to /usr/local/bin\nTBD\nVerify that it’s available\n\n\n\nTBD\n\n\n\n\n\nInstalling the Operator-SDK\n\nMacOSWindows\n\n\nInstall the operator-sdk with homebrew\nbrew install operator-sdk\nVerify the installation\noperator-sdk version\n\noperator-sdk version: \"v1.35.0\", commit: \"e95abdbd5ccb7ca0fd586e0c6f578e491b0a025b\", kubernetes version: \"v1.28.0\", go version: \"go1.21.11\", GOOS: \"darwin\", GOARCH: \"arm64\"\n\n\nWindows installation for the operator-sdk is a little more complicated…\nInstall golang\nscoop install go\n\n\n\n\n\nDownloading the case file\nRetrieve the ibm-guardium-data-security-center case file. As of this writing the latest is 2.6.0.\n\nMacOSWindows\n\n\nwget https://github.com/IBM/cloud-pak/raw/master/repo/case/ibm-guardium-data-security-center/2.6.0/ibm-guardium-data-security-center-2.6.0.tgz\nExtract the downloaded casefile in your directory\ntar zxf ibm-guardium-data-security-center-2.6.0.tgz\nThis should create a directory called ibm-guardium-data-security-center\n\n\nTBD",
    "crumbs": [
      "Guardium Insights",
      "EKS",
      "Prerequisites"
    ]
  },
  {
    "objectID": "src/guardium_insights/03_gi-install.html",
    "href": "src/guardium_insights/03_gi-install.html",
    "title": "Guardium Insights Installation",
    "section": "",
    "text": ":::note[Useful Links] A lot of content cribbed from here\nYou will need your IBM Entitlement Key for the next steps. You should already have it available at this point, but it can also be retrieved from here :::",
    "crumbs": [
      "Guardium Insights",
      "Guardium Insights Installation"
    ]
  },
  {
    "objectID": "src/guardium_insights/03_gi-install.html#obtaining-the-case-bundle-for-gi",
    "href": "src/guardium_insights/03_gi-install.html#obtaining-the-case-bundle-for-gi",
    "title": "Guardium Insights Installation",
    "section": "Obtaining the CASE bundle for GI",
    "text": "Obtaining the CASE bundle for GI\nExport the following env vars. This assumes we have been installing in the openshift-marketplace namespace.\nAs of this writing, we are using Guardium Insights version 3.5.0 which uses the 2.5.0 bundle file.\nexport CP_REPO_USER=\"cp\"\nexport CP_REPO_PASS=&lt;Your Password / Entitlement Key to cp.icr.io&gt;\nexport NAMESPACE=\"openshift-marketplace\"\nexport CLUSTERNAME=\"&lt;your-cluster-name&gt;\"\nexport CASE_NAME=ibm-guardium-data-security-center\nexport CASE_VERSION=2.6.0\nexport LOCAL_CASE_DIR=$HOME/.ibm-pak/data/cases/$CASE_NAME/$CASE_VERSION\nexport GI_INVENTORY_SETUP=install\nexport IBMPAK_LAUNCH_SKIP_PREREQ_CHECK=true\nSave the CASE bundle locally\noc ibm-pak get $CASE_NAME \\\n--version $CASE_VERSION \\\n--skip-verify\nThis will download the CASE bundle to $HOME/.ibm-pak/data/cases/ibm-guardium-data-security-center/2.6.0",
    "crumbs": [
      "Guardium Insights",
      "Guardium Insights Installation"
    ]
  },
  {
    "objectID": "src/guardium_insights/03_gi-install.html#install-the-gi-operator",
    "href": "src/guardium_insights/03_gi-install.html#install-the-gi-operator",
    "title": "Guardium Insights Installation",
    "section": "Install the GI Operator",
    "text": "Install the GI Operator\nSet the context to the correct namespace\nkubectl config set-context --current --namespace=${NAMESPACE}\n\nCollect the node names\nIn most installations you can label specific nodes to be used for DB2. In our case, we are just going to label all the compute nodes as part of the pre-install.\nNODES=$(kubectl get nodes -o jsonpath='{.items[*].metadata.name}')\nNODES=$(echo $NODES | tr ' ' ',')\necho $NODES\nShould return output similar to this\nip-192-168-10-208.ec2.internal,ip-192-168-17-40.ec2.internal,ip-192-168-23-220.ec2.internal\n\n\nRun the pre-install\noc ibm-pak launch $CASE_NAME \\\n   --version $CASE_VERSION \\\n   --inventory $GI_INVENTORY_SETUP \\\n   --action pre-install \\\n   --namespace $NAMESPACE \\\n   --args \"-n ${NAMESPACE} -h ${NODES} -t false -l true -e true\"\nThis should return the following\nWelcome to the CASE launcher\nAttempting to retrieve and extract the CASE from the specified location\n[✓] CASE has been retrieved and extracted\nAttempting to validate the CASE\n[✓] CASE has been successfully validated\nAttempting to locate the launch inventory item, script, and action in the specified CASE\n[✓] Found the specified launch inventory item, action, and script for the CASE\nSkipping CASE defined prerequisite checks for the specified action\nExecuting inventory item install, action preInstall : launch.sh\n-------------Installing dependent GI preinstall: /var/folders/tp/zkf5_qld1wn_ms51xys8d0m00000gn/T/extractedCASELauncher1330610821/ibm-guardium-insights-------------\nPRE-INSTALL VALUES:\n -n openshift-marketplace -h ip-192-168-10-208.ec2.internal,ip-192-168-17-40.ec2.internal,ip-192-168-23-220.ec2.internal -t false -l true -e true\nWarning : One or more optional parameters not passed, default values will be used\nOpenSSL is working with parameters -pbkdf2\n#####IBM Guardium Insights Pre-installation: Starting Preparation#####\nThe system properties of the nodes\nSkipping due to NON_OCP being true\nThe system properties of the nodes\nSkipping the system properties due to NON_OCP being true\nnode/ip-192-168-10-208.ec2.internal labeled\nnode/ip-192-168-17-40.ec2.internal labeled\nnode/ip-192-168-23-220.ec2.internal labeled\nSkipping data node(s) tainting.\nNode ip-192-168-10-208.ec2.internal already labelled.\nNode ip-192-168-17-40.ec2.internal already labelled.\nNode ip-192-168-23-220.ec2.internal already labelled.\n##### IBM Guardium Insights Pre-installation: Custom SCC creation#####\nSkipping custom scc creation\n#####IBM Guardium Insights Pre-installation: Ingress Certificate Recreation#####\nOverwrite existing secrets mode: no\n--------------------------------------------------------------\nStarting: IBM Guardium Insights: Ingress creation script.\nSkipping certificate generation. Certificate creation is handled by the Guardium Insights Deployment.\nCompleted: IBM Guardium Insights : Ingress creation script.\n--------------------------------------------------------------\n[✓] CASE launch script completed successfully\n\n\nVerify the node labeling\nkubectl get nodes --no-headers -o custom-columns=\":metadata.name\" | xargs -I {} sh -c 'oc describe node {} | grep db2 | grep -v $NAMESPACE'\nShould return\n                    icp4data=database-db2wh\n                    icp4data=database-db2wh\n                    icp4data=database-db2wh\n\n\nInstall the operator catalogs\noc ibm-pak launch $CASE_NAME \\\n   --version $CASE_VERSION \\\n   --inventory $GI_INVENTORY_SETUP \\\n   --action install-catalog \\\n   --namespace $NAMESPACE \\\n   --args \"--inputDir ${LOCAL_CASE_DIR}\"\nVerify the Catalog Sources were installed\nkubectl get catsrc\nOutput should look like this:\ntsx {3-5} NAME                                     DISPLAY                                  TYPE   PUBLISHER   AGE cloud-native-postgresql-catalog          Cloud Native Postgresql Catalog          grpc   IBM         3d21h ibm-db2uoperator-catalog                 IBM Db2U Catalog                         grpc   IBM         3m18s ibm-guardium-insights-operator-catalog   IBM Security Guardium Insights Catalog   grpc   IBM         2m31s ibm-redis-cp-operator-catalog            IBM Redis CP Catalog                     grpc   IBM         2m37s opencloud-operators                      IBMCS Operators                          grpc   IBM         3d21h\n\n\nInstall the Operators\noc ibm-pak launch $CASE_NAME \\\n   --version $CASE_VERSION \\\n   --inventory $GI_INVENTORY_SETUP \\\n   --action install-operator \\\n   --namespace ${NAMESPACE} \\\n   --args \"--registry cp.icr.io --user ${CP_REPO_USER} --pass ${CP_REPO_PASS} --secret ibm-entitlement-key --inputDir ${LOCAL_CASE_DIR}\"\nVerify the installation of the operators\nkubectl get sub\nOutput should look like this:\ntsx {4,6,10} NAME                                          PACKAGE                          SOURCE                                   CHANNEL cloud-native-postgresql                       cloud-native-postgresql          cloud-native-postgresql-catalog          stable ibm-common-service-operator                   ibm-common-service-operator      opencloud-operators                      v4.6 ibm-db2uoperator-catalog-subscription         db2u-operator                    ibm-db2uoperator-catalog                 v110509.0 ibm-events-operator                           ibm-events-operator              opencloud-operators                      v3 ibm-guardium-insights-operator-subscription   ibm-guardium-insights-operator   ibm-guardium-insights-operator-catalog   v3.6 ibm-idp-config-ui-operator                    ibm-commonui-operator-app        opencloud-operators                      v4.4 ibm-im-operator                               ibm-iam-operator                 opencloud-operators                      v4.5 ibm-platformui-operator                       ibm-zen-operator                 opencloud-operators                      v4.4 ibm-redis-cp-operator-catalog-subscription    ibm-redis-cp                     ibm-redis-cp-operator-catalog            v1.1 operand-deployment-lifecycle-manager-app      ibm-odlm                         opencloud-operators                      v4.3",
    "crumbs": [
      "Guardium Insights",
      "Guardium Insights Installation"
    ]
  },
  {
    "objectID": "src/guardium_insights/03_gi-install.html#deploying-an-instance-of-guardium-insights",
    "href": "src/guardium_insights/03_gi-install.html#deploying-an-instance-of-guardium-insights",
    "title": "Guardium Insights Installation",
    "section": "Deploying an instance of Guardium Insights",
    "text": "Deploying an instance of Guardium Insights\n:::note[Useful Links] Content heavily borrowed from here\n:::\n\nConfigure DNS resolution (Optional)\nIf you previously used AWS Route 53 to configure DNS resoluion as part of an insecure hostname configuration, then follow these instructions to add additional routing that is needed for the Guardium Insights deployment.\nIn AWS Route 53, open the previously created private hosted zone created during cluster build.\nAs we did previously, first find the private cluster IP address of the load balancer under CLUSTER-IP.\nkubectl get service --namespace ingress-nginx ingress-nginx-controller\nNAME                       TYPE           CLUSTER-IP      EXTERNAL-IP                                                                     PORT(S)                      AGE\ningress-nginx-controller   LoadBalancer   10.100.24.243   a091398910dbf4ad8aeb2f3f0e864311-916cbf853e32c1d5.elb.us-east-2.amazonaws.com   80:31369/TCP,443:30370/TCP   20h\nCapture that CLUSTER-IP value for the next steps.\nBack on the hosted zone details page, click the Create record button.\nWe want to create a record that routes guardium.apps.gi.guardium-insights.com to the private IP address captured above.\n\n\n\nimage\n\n\nClick Create Records.\n\n\nGuardium CR\nThis is a usable example. Copy this to a new yaml file. The name should be the cluster-name and the namespace should be openshift-marketplace.\nWe want to set the following values in this file\n  name: gi-eks\n  namespace: openshift-marketplace\n\nAnd under ingress set the hostname (guardium.apps.gi.thinkforward.work in our example) and make sure the FQDN is the one we set.\n      ingress:\n        hostName: guardium.apps.gi.thinkforward.work\n        domainName: apps.gi.thinkforward.work\n:::note[Using an insecure hostname] If using the insecure hostname, use the following values instead.\n      ingress:\n        hostName: guardium.apps.gi.guardium-insights.com\n        domainName: apps.gi.guardium-insights.com\n:::\nAlso we will be setting the RWO storage classes to use the gp3 block storage class we created ebs-gp3-sc\nThis will install an instance with the sizing set to values-small\ngdsc-custom-eks.yaml\ncat &lt;&lt;EOF &gt; gi-custom-eks.yaml\napiVersion: gi.ds.isc.ibm.com/v1\nkind: GuardiumDataSecurityCenter\nmetadata:\n  name: ${CLUSTERNAME}\n  namespace: ${NAMESPACE}\nspec:\n  version: 3.6.0\n  license:\n    accept: true\n    licenseType: \"L-QABB-9QRLFB\"\n  guardiumGlobal:\n    deploymentOptions:\n      prometheusType: disabled\n    fsGroupGid: 0\n    networkPolicyPodToPod:\n      enabled: false\n    backupsupport:\n      enabled: \"false\"\n      name: backup-pvc-support\n    image:\n      insightsPullSecret: ibm-entitlement-key\n      repository: cp.icr.io/cp/ibm-guardium-data-security-center\n    size: values-small\n    instance:\n      ingress:\n        hostName: guardium.${HOSTNAME}\n        domainName: ${HOSTNAME}\n      ics:\n        namespace: ${NAMESPACE}\n        registry: common-service\n    storageClassName: efs-sc\n    storageClassNameRWO: ebs-gp3-sc\n    deploySettings:\n      nonOCP: true\n      ensureDb2InstallPlacement: true\n  capabilities:\n  - name: quantum-safe\n    enabled: true\n    configurations: {}\n  - name: platform\n    enabled: true\n    configurations:\n      ssh-service:\n        serviceAnnotations:\n          service.beta.kubernetes.io/aws-load-balancer-internal: \"false\"\n      dependency-security:\n        networkPolicy:\n          egresses:\n            enabled: false\n            egress-required-allow:\n              egress:\n                - ports:\n                    - protocol: UDP\n                      port: 443\n                    - protocol: TCP\n                      port: 443\n                    - protocol: UDP\n                      port: 53\n                    - protocol: TCP\n                      port: 53\n                    - port: 5353\n                      protocol: UDP\n                    - port: 5353\n                      protocol: TCP\n      dependency-postgres:\n        podContainerSecurityContextParams:\n          fsGroup: 0\n          runAsUser: 1000\n          runAsNonRoot: true\n      dependency-kafka:\n        kafka:\n            storage:\n              type: persistent-claim\n              size: 250Gi\n              class: gp2\n        zookeeper:\n            storage:\n              type: persistent-claim\n              size: 20Gi\n              class: gp2\n        podContainerSecurityContextParams:\n            fsGroup: 0\n            runAsUser: 1000\n            runAsNonRoot: true\n      dependency-redis:\n        podContainerSecurityContextParams:\n            fsGroup: 0\n            runAsUser: 1000\n            runAsNonRoot: true\n      guardium-agent-cert-generator:\n        podContainerSecurityContextParams:\n          fsGroup: 0\n          runAsUser: 1000\n      mini-snif:\n        podContainerSecurityContextParams:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: false\n          runAsNonRoot: false\n        replicaCount: 1\n      dependency-db2:\n        db2instance:\n          nodes: 2\n          storage:\n              - name: meta\n                spec:\n                  accessModes:\n                  - ReadWriteMany\n                  resources:\n                    requests:\n                      storage: 1000Gi\n                  storageClassName: efs-sc\n                type: create\n              - name: data\n                spec:\n                  accessModes:\n                  - ReadWriteOnce\n                  resources:\n                    requests:\n                      storage: 100Gi\n                  storageClassName: gp2\n                type: template\n              - name: backup\n                spec:\n                  accessModes:\n                  - ReadWriteMany\n                  resources:\n                    requests:\n                      storage: 2000Gi\n                  storageClassName: efs-sc\n                type: create\n              - name: tempts\n                spec:\n                  accessModes:\n                  - ReadWriteOnce\n                  resources:\n                    requests:\n                      storage: 1000Gi\n                  storageClassName: gp2\n                type: template\n              - name: archivelogs\n                spec:\n                  accessModes:\n                  - ReadWriteMany\n                  resources:\n                    requests:\n                      storage: 500Gi\n                  storageClassName: efs-sc\n                type: create\nEOF\nApply the above CR\nkubectl apply -f gdsc-custom-eks.yaml\nThe deployment may take a little while.\n\n\nVerifying instance is up\nCheck to make sure the Guardium Insights custom resource (CR) has been completely reconciled.\nkubectl get gdsc\nOutput should look like this:\ntsx {2} NAME     TYPE    STATUS   REASON     MESSAGE                    DESIRED_VERSION   INSTALLED_VERSION gi-eks   Ready   True     Complete   Completed Reconciliation   3.6.0             3.6.0\nCheck that the tenant has been created\nkubectl get tenantminisnif\nOutput should look like this:\ntsx {2} NAME                           TYPE    STATUS   REASON     MESSAGE                    DESIRED_VERSION   INSTALLED_VERSION gi-eksctqdrippjy9p2gqvvzwhyc   Ready   True     Complete   Completed Reconciliation   3.6.0             3.6.0\nVerify all ingress paths have been created\nkubectl get ingress -n openshift-marketplace\nOutput should look like this:\ntsx {11-17} NAME                                            CLASS    HOSTS                                                          ADDRESS                                                                         PORTS     AGE cncf-common-web-ui                              nginx    cp-console-openshift-marketplace.apps.gi.thinkforward.work     a904f47d90a8c4961ae389b155359368-aa0f265b8588c710.elb.us-east-1.amazonaws.com   80        5d22h cncf-id-mgmt                                    nginx    cp-console-openshift-marketplace.apps.gi.thinkforward.work     a904f47d90a8c4961ae389b155359368-aa0f265b8588c710.elb.us-east-1.amazonaws.com   80        5d22h cncf-platform-auth                              nginx    cp-console-openshift-marketplace.apps.gi.thinkforward.work     a904f47d90a8c4961ae389b155359368-aa0f265b8588c710.elb.us-east-1.amazonaws.com   80        5d22h cncf-platform-id-auth                           nginx    cp-console-openshift-marketplace.apps.gi.thinkforward.work     a904f47d90a8c4961ae389b155359368-aa0f265b8588c710.elb.us-east-1.amazonaws.com   80        5d22h cncf-platform-id-provider                       nginx    cp-console-openshift-marketplace.apps.gi.thinkforward.work     a904f47d90a8c4961ae389b155359368-aa0f265b8588c710.elb.us-east-1.amazonaws.com   80        5d22h cncf-platform-login                             nginx    cp-console-openshift-marketplace.apps.gi.thinkforward.work     a904f47d90a8c4961ae389b155359368-aa0f265b8588c710.elb.us-east-1.amazonaws.com   80        5d22h cncf-platform-oidc                              nginx    cp-console-openshift-marketplace.apps.gi.thinkforward.work     a904f47d90a8c4961ae389b155359368-aa0f265b8588c710.elb.us-east-1.amazonaws.com   80        5d22h cncf-saml-ui-callback                           nginx    cp-console-openshift-marketplace.apps.gi.thinkforward.work     a904f47d90a8c4961ae389b155359368-aa0f265b8588c710.elb.us-east-1.amazonaws.com   80        5d22h cncf-social-login-callback                      nginx    cp-console-openshift-marketplace.apps.gi.thinkforward.work     a904f47d90a8c4961ae389b155359368-aa0f265b8588c710.elb.us-east-1.amazonaws.com   80        5d22h gi-eks-apigateway-api                           nginx    guardium.apps.gi.thinkforward.work                             a904f47d90a8c4961ae389b155359368-aa0f265b8588c710.elb.us-east-1.amazonaws.com   80, 443   23h gi-eks-apigateway-docs-dev                      nginx    guardium.apps.gi.thinkforward.work                             a904f47d90a8c4961ae389b155359368-aa0f265b8588c710.elb.us-east-1.amazonaws.com   80, 443   23h gi-eks-insights                                 nginx    guardium.apps.gi.thinkforward.work                             a904f47d90a8c4961ae389b155359368-aa0f265b8588c710.elb.us-east-1.amazonaws.com   80, 443   23h gi-eks-s3                                       nginx    s3.guardium.apps.gi.thinkforward.work                          a904f47d90a8c4961ae389b155359368-aa0f265b8588c710.elb.us-east-1.amazonaws.com   80, 443   21h gi-eks-ssh-service                              &lt;none&gt;   guardium.apps.gi.thinkforward.work                                                                                                             80, 443   23h gi-eksctqdrippjy9p2gqvvzwhyc-snif-picker-feed   nginx    feed-ctqdrippjy9p2gqvvzwhyc-gi-eks.apps.gi.thinkforward.work   a904f47d90a8c4961ae389b155359368-aa0f265b8588c710.elb.us-east-1.amazonaws.com   80        20h gi-eksctqdrippjy9p2gqvvzwhyc-snif-picker-unix   nginx    unix-ctqdrippjy9p2gqvvzwhyc-gi-eks.apps.gi.thinkforward.work   a904f47d90a8c4961ae389b155359368-aa0f265b8588c710.elb.us-east-1.amazonaws.com   80        20h\nSince we installed everything into the openshift-marketplace namespace, we should see everything there.",
    "crumbs": [
      "Guardium Insights",
      "Guardium Insights Installation"
    ]
  },
  {
    "objectID": "src/guardium_insights/03_gi-install.html#creating-the-qs-tenant",
    "href": "src/guardium_insights/03_gi-install.html#creating-the-qs-tenant",
    "title": "Guardium Insights Installation",
    "section": "Creating the QS tenant",
    "text": "Creating the QS tenant\n\nIn the current release of GI, in order to create the Quantum Security tenant, you need to scale down all the unnecessary services that are spun up by default in Guardium Insights. Future GI revisions will allow for disabling these services in the CR.\n\n\nScale down services\nOur example GI instance cluster is called gi-eks. Let’s export that as an env var.\nIf you’re unsure as to what your instance cluster is called, it can be found with the following command:\nkubectl get gi\nOutput should look like this:\nNAME     TYPE    STATUS   REASON     MESSAGE                    DESIRED_VERSION   INSTALLED_VERSION\ngi-eks   Ready   True     Complete   Completed Reconciliation   3.6.0             3.6.0\nexport cluster_name=\"gi-eks\"\nNow scale down the following deployments\n{/* ${cluster_name}-rm-risk-controller  /} {/ ${cluster_name}-risk-analytics-controller  /} {/ ${cluster_name}-ssh-service  */}\nkubectl scale deploy \\\n${cluster_name}-risk-analytics-engine \\\n${cluster_name}-risk-analytics-ml-classification \\\n${cluster_name}-rm-risk-engine \\\n${cluster_name}-outliers-engine \\\n${cluster_name}-analytics-events \\\n${cluster_name}-assets \\\n${cluster_name}-compliance-accelerator \\\n${cluster_name}-pam-connector-thycotic \\\n${cluster_name}-policy-builder \\\n${cluster_name}-postgres-proxy \\\n${cluster_name}-postgres-sentinel \\\n${cluster_name}-rm-data-collector \\\n${cluster_name}-snif-assist \\\n${cluster_name}-streams \\\n--replicas=0\n\n\nCreate the tenant\nRetrieve the cpadmin password and set it as an env var\nexport CPPASS=$(kubectl get secret platform-auth-idp-credentials -o jsonpath='{.data.admin_password}' | base64 -d)\nCreate an auth header\nexport AUTH=$(echo -ne \"cpadmin:$CPPASS\" | base64)\n{/* Create a JSON payload.json\n{\n  \"async\": false,\n  \"external_id\": \"adv_gi41_qspm_allcom\",\n  \"name\": \"adv_gi41_qspm_allcom\",\n  \"part_number\": \"gi_dedicated_qspm\",\n  \"uid\": \"cpadmin\"\n}\n*/}\nAnd now let’s submit the request. Our API url is https://guardium.apps.gi.thinkforward.work/api/v3/tenants. This is the URL we set in the CR.\n:::note[Using an insecure hostname] If you are using an insecure hostname, make sure to use that value for the API URL. The local resolution to the external IP of the load balancer should have already been configured in your local hosts file.\nFor example\nhttps://guardium.apps.gi.guardium-insights.com/api/v3/tenants\n:::\ncurl -k -X 'POST' \\\n  'https://guardium.apps.gi.thinkforward.work/api/v3/tenants' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -H \"Authorization: Basic $AUTH\" \\\n  -d '{\n  \"async\": false,\n  \"external_id\": \"adv_gi41_qspm_allcom\",\n  \"name\": \"adv_gi41_qspm_allcom\",\n  \"part_number\": \"gi_dedicated_qspm\",\n  \"uid\": \"cpadmin\"\n}'\nSometimes the command runs and nothing happens for awhile. To fix this problem change \"async\": false to \"async\": true.\nWhen the tenant is created, the tenant id should be returned\n{\"tenant_id\":\"TNT_ACCKEBT4ZIRQW9YRRKCRJH\"}\n\n\nVerify the tenant creation\nWe set the URL for Guardium Insights to guardium.apps.gi.thinkforward.work in our CR. Now open this in a browser.\nThe console url should open. Login with cpadmin and the password you retrieved and set to $CPPASS\n\n\n\nGuardiumLogin01\n\n\nSelect the upper rightmost icon and select the QS tenant under Account dropdown. Then click the IBM Guardium logo in the upper left to open the console for the QS tenant.\n\n\n\nGuardiumLogin02\n\n\nThe console should open.\n\n\n\nGuardiumLogin03\n\n\n{/* ### Stupid error tricks\nldap pods are stuck. One is in an error state due to insufficient ephemeral storage (wtf)\nkubectl get pods | grep ldap\nc-gi-eks-db2-ldap-6bdbdf8fc7-flh8b                                0/1     Error                   0                 18h\nc-gi-eks-db2-ldap-6bdbdf8fc7-kmqdx                                0/1     Running                 0                 7h1m\n\nWe describe the errored pod\n```tsx {21-23} kubectl describe pod c-gi-eks-db2-ldap-6bdbdf8fc7-flh8b\nName: c-gi-eks-db2-ldap-6bdbdf8fc7-flh8b Namespace: openshift-marketplace Priority: 0 Service Account: account-openshift-marketplace-gi-eks-db2 Node: ip-192-168-17-40.ec2.internal/192.168.17.40 Start Time: Tue, 03 Sep 2024 15:50:04 -0400 Labels: app=gi-eks-db2 formation_id=gi-eks-db2 pod-template-hash=6bdbdf8fc7 role=ldap Annotations: cloudpakId: 5ed3ce3e032545a0a2a17bf2b6c23dcd cloudpakMetric: RESOURCE_UNIT cloudpakName: IBM Security Guardium Package SW productChargedContainers: All productCloudpakRatio: 1:100 productID: 4c49ee7d8da44cf9bf2da48fc8fabb11 productMetric: MANAGED_VIRTUAL_SERVER productName: IBM Security Guardium Insights Status: Failed Reason: Evicted Message: Pod ephemeral local storage usage exceeds the total limit of containers 5Mi.\n\nThe other pod is hung because this pod hasn't come up. They both have the same resource restrictions. Just WTF.\n\nOther errors\n\n```tsx\nkubectl get pods | grep db2u\nc-gi-eks-db2-db2u-0                                               0/1     Init:CrashLoopBackOff   211 (2m41s ago)   18h\ndb2 pod is stuck in a crash loop.\nStatus:           Pending\nIP:               192.168.31.213\nIPs:\n  IP:  192.168.31.213\nInit Containers:\n  instdb:\n    Container ID:  containerd://28ce603c5c63398d080938e3d37dbfaa7fdea60f5b71ca5e53c0a51a5ecdc5f6\n    Image:         icr.io/db2u/db2u.instdb@sha256:cf2f99358fb6beac6ca2f9553855b8f33d4cfcd7748bd191c66b725180722cab\n    Image ID:      icr.io/db2u/db2u.instdb@sha256:cf2f99358fb6beac6ca2f9553855b8f33d4cfcd7748bd191c66b725180722cab\n    Port:          &lt;none&gt;\n    Host Port:     &lt;none&gt;\n    Command:\n      /bin/sh\n      -c\n      /Db2wh_preinit/instdb_entrypoint.sh\n    State:          Waiting\n      Reason:       CrashLoopBackOff\n    Last State:     Terminated\n      Reason:       Error\n      Exit Code:    1\n      Started:      Wed, 04 Sep 2024 09:54:45 -0400\n      Finished:     Wed, 04 Sep 2024 09:54:54 -0400\n    Ready:          False\n    Restart Count:  211\nthe initdb init container in the pod is failing for… reasons. Just awesome. */}",
    "crumbs": [
      "Guardium Insights",
      "Guardium Insights Installation"
    ]
  },
  {
    "objectID": "src/solution_overview/environment.html",
    "href": "src/solution_overview/environment.html",
    "title": "Step Two",
    "section": "",
    "text": "Step Two Solution\nPhasellus at risus egestas, ultricies tortor efficitur, auctor augue. Suspendisse finibus maximus dui nec condimentum. Proin fringilla efficitur vehicula. Suspendisse sem lacus, iaculis quis erat et, facilisis sagittis est. Sed sapien justo, condimentum vitae aliquet sed, faucibus at ex. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Fusce placerat ante eget diam tincidunt, vitae tincidunt sapien malesuada. Nullam ullamcorper justo eros. Duis ultricies aliquam dui, id aliquam lorem congue vitae. Ut porttitor, tellus eu dignissim semper, turpis nunc cursus libero, sed placerat est dui non enim.\nVestibulum libero dolor, vehicula vitae risus non, consequat gravida neque. Maecenas a posuere sem. Quisque dignissim porta pretium. Nam mattis lacus commodo lobortis consequat. Vestibulum at massa a quam egestas accumsan. Fusce ac neque eu libero maximus aliquet id quis metus. In sodales neque ut turpis iaculis vehicula. Sed volutpat, tellus non laoreet aliquet, risus felis ornare dolor, interdum venenatis turpis metus in lorem. Aliquam quam ligula, porttitor non ultrices ac, lacinia ullamcorper massa. Integer molestie eleifend urna, imperdiet dignissim tellus malesuada ac. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam tristique sem ac purus interdum, vitae pharetra erat fringilla. Integer non nunc a eros porttitor rutrum. Suspendisse ut libero urna. Vestibulum vitae est at diam vestibulum aliquet. Donec porta nunc eget lobortis mollis.\nVestibulum luctus sodales odio, at luctus lacus lobortis a. Aliquam vulputate id turpis sit amet bibendum. Donec a dignissim tellus, vel vehicula erat. Pellentesque hendrerit ex magna, at dictum est pretium a. Vivamus faucibus ipsum lectus, ut elementum diam interdum scelerisque. Cras magna sapien, tincidunt quis eleifend at, tincidunt non eros. Aliquam ac augue turpis."
  },
  {
    "objectID": "src/landing_page/landing_page.html",
    "href": "src/landing_page/landing_page.html",
    "title": "Project Name",
    "section": "",
    "text": "Project Name\nSubtitle\n\n\nOur Documentation \n\n\n\n\n\nNext Steps\n\n\n\nL ink 1\n\n\n\nLink 2"
  },
  {
    "objectID": "src/qsr/performance_test_harness/02_usage.html",
    "href": "src/qsr/performance_test_harness/02_usage.html",
    "title": "Usage",
    "section": "",
    "text": ":::note[Useful Links]\nA lot of info cribbed from here\nFor a complete list of tested algorithms, see above link. :::",
    "crumbs": [
      "QSR",
      "Performance Test Harness",
      "Usage"
    ]
  },
  {
    "objectID": "src/qsr/performance_test_harness/02_usage.html#executing-the-tests",
    "href": "src/qsr/performance_test_harness/02_usage.html#executing-the-tests",
    "title": "Usage",
    "section": "Executing the tests",
    "text": "Executing the tests\nDepending on where you extracted the PTH, change directory to the test_utils folder.\ncd ~/perftest/pqc-performance-test/test_utils\n\nRunning the h2load test suite\nRun the test with the ip address you set as TARGET_IP_LIST. In our example we used 9.46.72.93. Select 11 to test all algorithms.\n./all_h2load.sh -c 6 -n 100000 -s 9.46.72.93 -t 3 -o ~/perftest/perf_results/h2load\n\n/root/perftest/perf_results/h2load exists and will be appended.\nh2load performance tests with a variety of key agreement algorithms\nRetrieving assignments ...\nStaring h2load container ...\nRunning load test using 6 clients for 100000 requests.\nChoose a signature algorithm or select 'all' to test all signatures:\n1. ecdsap256\n2. ecdsap384\n3. ecdsap521\n4. dilithium2\n5. dilithium3\n6. dilithium5\n7. p256_dilithium2\n8. p384_dilithium3\n9. p521_dilithium5\n10. rsa3072\n11. all\n0. Exit\nEnter your choice (1-11): 11\n\nThe test should run for a little while. In our example case, the last lines returned were the following:\n2024/07/31 11:14:45: 100000 total requests with 6 concurrent connections to https://9.46.72.93 from 10.21.71.145/cluster-nfs-nfs01.fyre.ibm.com.\nRequest Time: Time taken for a single request and response.\nConnect Time: Time to connect to the server including TLS handshake.\nFirst Byte Time: Time to get the first byte of decrypted application data from the server.\n\n\nRunning the openssl test suite\nWe’re testing against our docker host again (9.46.72.93). Select 11 for all algorithms.\n./all_openssl.sh -t 30 -s 9.46.72.93 -w / -o ~/perftest/perf_results/openssl\n\nopenssl s_time performance tests with a variety of key agreement algorithms\nRetrieving assignments ...\nStarting PQC client container ...\nRetrieving CA Certificate ...\nChoose a signature algorithm or select 'all' to test all signatures:\n1. ecdsap256\n2. ecdsap384\n3. ecdsap521\n4. dilithium2\n5. dilithium3\n6. dilithium5\n7. p256_dilithium2\n8. p384_dilithium3\n9. p521_dilithium5\n10. rsa3072\n11. all\n0. Exit\nThe test should run for quite a while. In our example case, the last lines returned were the following:\n2024/07/31 12:01:53: 30 seconds of requests to 9.46.72.93 from 10.21.71.145/cluster-nfs-nfs01.fyre.ibm.com using cipher TLS_AES_256_GCM_SHA384.\nTotal Reqeusts: Total number of connections initiaged (no session reuse).\nUser Time: User CPU time consumed durring test loop execution.\nUser Rate: Connections per user CPU second.\nReal Time: Wall clock time of the s_time run rounded to integer seconds.\nTotal Bytes: Total number of payload bytes (actual web response) returned.",
    "crumbs": [
      "QSR",
      "Performance Test Harness",
      "Usage"
    ]
  },
  {
    "objectID": "src/qsr/performance_test_harness/02_usage.html#generate-the-visualizations",
    "href": "src/qsr/performance_test_harness/02_usage.html#generate-the-visualizations",
    "title": "Usage",
    "section": "Generate the visualizations",
    "text": "Generate the visualizations\nRun ./gen_visulizations.sh\nWe’re going to use port 8080 for our example.\n./gen_visulizations.sh \nEnter infrastructure name: Test Infra\nnter the PTH Dashboard target port: 8080\n[+] Building 17.5s (12/12) FINISHED                                                                                            docker:default\n =&gt; [internal] load build definition from Dockerfile                                                                                     0.1s\n =&gt; =&gt; transferring dockerfile: 895B                                                                                                     0.0s\n =&gt; [internal] load metadata for docker.io/library/node:alpine                                                                           0.8s\n =&gt; [auth] library/node:pull token for registry-1.docker.io                                                                              0.0s\n =&gt; [internal] load .dockerignore                                                                                                        0.1s\n =&gt; =&gt; transferring context: 2B                                                                                                          0.0s\n =&gt; [1/6] FROM docker.io/library/node:alpine@sha256:39005f06b2fae765764d6fdf20ad1c4d0890f5ad3e1f39b56a18768334b8ecd6                     6.9s\n =&gt; =&gt; resolve docker.io/library/node:alpine@sha256:39005f06b2fae765764d6fdf20ad1c4d0890f5ad3e1f39b56a18768334b8ecd6                     0.1s\n =&gt; =&gt; sha256:39005f06b2fae765764d6fdf20ad1c4d0890f5ad3e1f39b56a18768334b8ecd6 6.62kB / 6.62kB                                           0.0s\n =&gt; =&gt; sha256:c83e6e8aa2c458cf740b18b7b13e546751fe081d36223aac253b5ec0da2cd89d 1.72kB / 1.72kB                                           0.0s\n =&gt; =&gt; sha256:5c4cc5767575c711b99b1b077ad8afb8cfbf407aca174b7cbc998ce6db1e4f93 6.36kB / 6.36kB                                           0.0s\n =&gt; =&gt; sha256:c6a83fedfae6ed8a4f5f7cbb6a7b6f1c1ec3d86fea8cb9e5ba2e5e6673fde9f6 3.62MB / 3.62MB                                           0.3s\n =&gt; =&gt; sha256:c4f54159f74a5dccd97b9af978fab507483785736e822851638f20f275716fc3 1.39MB / 1.39MB                                           0.3s\n =&gt; =&gt; sha256:8d90f41c769e0bfd90a1e8456db9f590ae8dc42842ffa098693b6ed4bd44eba3 47.36MB / 47.36MB                                         1.0s\n =&gt; =&gt; extracting sha256:c6a83fedfae6ed8a4f5f7cbb6a7b6f1c1ec3d86fea8cb9e5ba2e5e6673fde9f6                                                0.4s\n =&gt; =&gt; sha256:6ecb2bd0d8e8f8628fe4a7cf14404c59d67a5547c6e5faa4a507f4b232bd316e 449B / 449B                                               0.4s\n =&gt; =&gt; extracting sha256:8d90f41c769e0bfd90a1e8456db9f590ae8dc42842ffa098693b6ed4bd44eba3                                                4.9s\n =&gt; =&gt; extracting sha256:c4f54159f74a5dccd97b9af978fab507483785736e822851638f20f275716fc3                                                0.1s\n =&gt; =&gt; extracting sha256:6ecb2bd0d8e8f8628fe4a7cf14404c59d67a5547c6e5faa4a507f4b232bd316e                                                0.0s\n =&gt; [internal] load build context                                                                                                        0.2s\n =&gt; =&gt; transferring context: 1.44MB                                                                                                      0.1s\n =&gt; [2/6] WORKDIR /app                                                                                                                   0.3s\n =&gt; [3/6] COPY dist/ /app/dist/                                                                                                          0.2s\n =&gt; [4/6] RUN npm install -g http-server@14.1.1                                                                                          6.9s\n =&gt; [5/6] RUN addgroup -S appuser && adduser -S appuser -G appuser                                                                       0.6s\n =&gt; [6/6] RUN chown -R appuser:appuser /app                                                                                              0.6s\n =&gt; exporting to image                                                                                                                   0.7s\n =&gt; =&gt; exporting layers                                                                                                                  0.6s\n =&gt; =&gt; writing image sha256:ce54effa6ab8dec6db3ddcee727d065728f5560907d0d6dddb90df5dbf874a2d                                             0.0s\n =&gt; =&gt; naming to docker.io/library/pth_dashboard                                                                                         0.0s\n4b6dd6935d473d0f637fec4f81bf1df03f42a1656e16a4923cf38b5585308487\nAll output files are converted and stored. The script then builds and deploys the dashboard container\ntsx {3} docker ps -a CONTAINER ID   IMAGE                                            COMMAND                  CREATED          STATUS          PORTS                                                                           NAMES 4b6dd6935d47   pth_dashboard                                    \"docker-entrypoint.s…\"   39 seconds ago   Up 38 seconds   0.0.0.0:8080-&gt;8080/tcp                                                          pthdashboard a7011b81d849   localhost:5000/qsc-perf-server-ubuntu:provider   \"/bin/bash /opt/ngin…\"   2 hours ago      Up 2 hours      0.0.0.0:9100-9563-&gt;9100-9563/tcp, 0.0.0.0:9080-&gt;80/tcp, 0.0.0.0:9043-&gt;443/tcp   qsc-perf-server\nIn our case, the dash can now be seen at port 8080 at our docker host\n\n\n\nPTH-Dashboard",
    "crumbs": [
      "QSR",
      "Performance Test Harness",
      "Usage"
    ]
  },
  {
    "objectID": "src/qsr/adaptive_proxy/03_configuration-ssl.html",
    "href": "src/qsr/adaptive_proxy/03_configuration-ssl.html",
    "title": "Configuration with SSL",
    "section": "",
    "text": "Useful links:\nhttps://www.ibm.com/docs/en/quantum-safe/quantum-safe-remediator/1.0.x?topic=proxy-configuring-adaptive"
  },
  {
    "objectID": "src/qsr/adaptive_proxy/03_configuration-ssl.html#adaptive-proxy-configuration",
    "href": "src/qsr/adaptive_proxy/03_configuration-ssl.html#adaptive-proxy-configuration",
    "title": "Configuration with SSL",
    "section": "Adaptive proxy configuration",
    "text": "Adaptive proxy configuration\nThe adaptive proxy configuration consists of the following files:\n\nadaptive.proxy.location.conf\nadaptive.proxy.upstream.conf\nadaptive.proxy.curvemap\nadaptive.proxy.passthrough.conf"
  },
  {
    "objectID": "src/qsr/adaptive_proxy/03_configuration-ssl.html#example-setup",
    "href": "src/qsr/adaptive_proxy/03_configuration-ssl.html#example-setup",
    "title": "Configuration with SSL",
    "section": "Example setup",
    "text": "Example setup\n\nBuild a test site with Astro and Starlight\nSome of the below was pulled out of How To Create a Self-Signed SSL Certificate for Apache in Ubuntu 16.04\n\nPrerequisites\nOn our Linux host (Ubuntu 24.0.4 LTS), let’s install httpd and npm, install corepack, enable yarn, and configure the firewall.\nsudo apt update\n\nsudo apt-get -y install apache2\n\ncurl -fsSL https://deb.nodesource.com/setup_20.x | sudo bash -\n\nsudo apt-get install -y nodejs\n\nsudo corepack enable\n\n\nEnabling mod_ssl\nEnable mod_ssl with the a2enmod command:\nsudo a2enmod ssl\nRestart Apache to activate the module:\nsudo systemctl restart apache2\n\n\nCreate the SSL Certificate\nCreate the SSL key and certificate files with the openssl command:\nsudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/ssl/private/apache-selfsigned.key -out /etc/ssl/certs/apache-selfsigned.crt\nThe entirety of the prompts will look something like this:\nCountry Name (2 letter code) [XX]:US\nState or Province Name (full name) []:Example\nLocality Name (eg, city) [Default City]:Example \nOrganization Name (eg, company) [Default Company Ltd]:Example Inc\nOrganizational Unit Name (eg, section) []:Example Dept\nCommon Name (eg, your name or your server's hostname) []:your_domain_or_ip\nEmail Address []:webmaster@example.com\n\n\nBuild the test site\nNow let’s build an Astro site with a Starlight template.\nsudo yarn create astro --install --no-git --typescript relaxed --template starlight /var/www/html/test-app\nThis will create a directory called /var/www/html/test-app\nGo into test-app and update the astro.config.mjs with the following:\nimport { defineConfig } from 'astro/config';\nimport starlight from '@astrojs/starlight';\n\n// https://astro.build/config\nexport default defineConfig({\n    outDir: './build',\n    build: {\n        assets: 'css'  \n    }, \n    integrations: [\n        starlight({\n            title: 'My Docs',\n            social: {\n                github: 'https://github.com/withastro/starlight',\n            },\n            sidebar: [\n                {\n                    label: 'Guides',\n                    items: [\n                        // Each item here is one entry in the navigation menu.\n                        { label: 'Example Guide', slug: 'guides/example' },\n                    ],\n                },{\n                    label: 'Reference',\n                    autogenerate: { directory: 'reference' },\n                },\n            ],\n        }),\n    ],\n});\nNow run the build\nsudo yarn build\n\nThis will build a viable site in /var/www/html/test-app/build\n\n\nUpdate apache config\nCreate a file called /etc/apache2/sites-available/test-site.conf\nSince we’re running this in IBM Fyre, we’re going to use the FQDN of our host here.\nListen 8080\n\n&lt;VirtualHost *:8080&gt;\n    DocumentRoot \"/var/www/html/test-app/build\"\n    ServerName apache-host01.fyre.ibm.com\n\n    SSLEngine on\n    SSLCertificateFile /etc/ssl/certs/apache-selfsigned.crt\n    SSLCertificateKeyFile /etc/ssl/private/apache-selfsigned.key\n&lt;/VirtualHost&gt;\nEnable the new config\ncd /etc/apache2/sites-enabled\n\nsudo ln -s ../sites-available/test-site.conf\nNow restart apache2\nsudo systemctl restart apache2\n\nVerify the site is now up by navigating to https://apache-host01.fyre.ibm.com:8080.\n\n\n\nQSRAP-config1\n\n\nWe set our test site to port 8080.\n\n\n\nDeploy the proxy\nAfter following the instructions to install the Adaptive Proxy here\nEdit the following file /opt/adaptive-proxy/workdir/adaptive.proxy.location.conf\n:::note[On NGINX configuration]\nThere’s likely better ways to configure this, but for this example, this works fine. :::\n# IBM Confidential\n# PID: 5900-B8I\n# Copyright (c) IBM Corp. 2024\n\n# Nginx Documentation: https://nginx.org/en/docs/http/ngx_http_core_module.html\n# Location directive documentation: https://nginx.org/en/docs/http/ngx_http_core_module.html#location\n# file path in server: /etc/nginx/conf.d/location.conf\n\nlocation ^~ / {\n    proxy_pass https://apache-host01.fyre.ibm.com:8080/;\n    proxy_set_header Host $host;\n    proxy_set_header X-Real-IP $remote_addr;\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n    proxy_set_header X-Forwarded-Proto $scheme;\n}\n\nStart up the adaptive proxy\ncd /opt/adaptive-proxy/workdir\n\n./runAdaptiveProxyServer.sh\nProxy should come up and our test site should be available now\n\n\n\nQSRAP-config2\n\n\nIn our case, we’re pointing the proxy_pass to the FQDN virtual host we configured on our web server.\n:::note[How it works]\nThe magic is the PORT. You must go to the URL at port 8443 which is where the proxy listens.\nIn our case, to get to the proxied site:\nhttps://dockerbot21.fyre.ibm.com:8443/\ndockerbot21 is our docker host in fyre. His FQDN is dockerbot21.fyre.ibm.com\nOur apache host is apache-host01.fyre.ibm.com\n:::\n\n\nVerifying the encryption algorithms\nNow that our site is up and proxied, let’s verify that it’s using a Quantum Safe algorithm for encryption.\n\nCurrently there is no PQC encryption available for any TLS certs available from any legitimate Certificate Authority.\n\nIn a Chrome brower, go to the site and click the three dots.\n\n\n\nQSRAP-config3\n\n\nOpen the Developer Tools\n\n\n\nQSRAP-config4\n\n\nClick the Security panel\n\n\n\nQSRAP-config5\n\n\nHere we can see that X25519Kyber768Draft00 is one of the encryption algorithms and that’s Quantum Safe."
  },
  {
    "objectID": "src/qsr/adaptive_proxy/03_configuration-ssl.html#diagram-of-the-proxied-site",
    "href": "src/qsr/adaptive_proxy/03_configuration-ssl.html#diagram-of-the-proxied-site",
    "title": "Configuration with SSL",
    "section": "Diagram of the Proxied Site",
    "text": "Diagram of the Proxied Site\nd2 sketch pad=50 direction: left dockerbot1: \"dockerbot21:8443\\n(adaptive proxy)\\nTLS encryption\\n happens here\" {   style.fill: transparent   style.font-size: 12   style.font: mono   width: 141   height: 95 } Image: \"\" {   icon: https://icons.terrastruct.com/aws/_General/Users_light-bg.svg   shape: image } Image -&gt; dockerbot1: TLS Square: \"apache-host01\\n(example site)\\nNo TLS\" {   style.fill: transparent   style.font-size: 12   style.font: mono   width: 141   height: 95 } dockerbot1 -&gt; Square: \"No-TLS\\n\" Square -&gt; dockerbot1: \"No-TLS\\n\" dockerbot1 -&gt; Image: TLS"
  },
  {
    "objectID": "src/implementation_methodology/steptwo-imp.html",
    "href": "src/implementation_methodology/steptwo-imp.html",
    "title": "Step Two",
    "section": "",
    "text": "Step Two Implementation\nPhasellus at risus egestas, ultricies tortor efficitur, auctor augue. Suspendisse finibus maximus dui nec condimentum. Proin fringilla efficitur vehicula. Suspendisse sem lacus, iaculis quis erat et, facilisis sagittis est. Sed sapien justo, condimentum vitae aliquet sed, faucibus at ex. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Fusce placerat ante eget diam tincidunt, vitae tincidunt sapien malesuada. Nullam ullamcorper justo eros. Duis ultricies aliquam dui, id aliquam lorem congue vitae. Ut porttitor, tellus eu dignissim semper, turpis nunc cursus libero, sed placerat est dui non enim.\nVestibulum libero dolor, vehicula vitae risus non, consequat gravida neque. Maecenas a posuere sem. Quisque dignissim porta pretium. Nam mattis lacus commodo lobortis consequat. Vestibulum at massa a quam egestas accumsan. Fusce ac neque eu libero maximus aliquet id quis metus. In sodales neque ut turpis iaculis vehicula. Sed volutpat, tellus non laoreet aliquet, risus felis ornare dolor, interdum venenatis turpis metus in lorem. Aliquam quam ligula, porttitor non ultrices ac, lacinia ullamcorper massa. Integer molestie eleifend urna, imperdiet dignissim tellus malesuada ac. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam tristique sem ac purus interdum, vitae pharetra erat fringilla. Integer non nunc a eros porttitor rutrum. Suspendisse ut libero urna. Vestibulum vitae est at diam vestibulum aliquet. Donec porta nunc eget lobortis mollis.\nVestibulum luctus sodales odio, at luctus lacus lobortis a. Aliquam vulputate id turpis sit amet bibendum. Donec a dignissim tellus, vel vehicula erat. Pellentesque hendrerit ex magna, at dictum est pretium a. Vivamus faucibus ipsum lectus, ut elementum diam interdum scelerisque. Cras magna sapien, tincidunt quis eleifend at, tincidunt non eros. Aliquam ac augue turpis."
  },
  {
    "objectID": "src/implementation_methodology/stepthree-imp.html",
    "href": "src/implementation_methodology/stepthree-imp.html",
    "title": "Step Three",
    "section": "",
    "text": "Step Three Implementation\nPhasellus at risus egestas, ultricies tortor efficitur, auctor augue. Suspendisse finibus maximus dui nec condimentum. Proin fringilla efficitur vehicula. Suspendisse sem lacus, iaculis quis erat et, facilisis sagittis est. Sed sapien justo, condimentum vitae aliquet sed, faucibus at ex. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Fusce placerat ante eget diam tincidunt, vitae tincidunt sapien malesuada. Nullam ullamcorper justo eros. Duis ultricies aliquam dui, id aliquam lorem congue vitae. Ut porttitor, tellus eu dignissim semper, turpis nunc cursus libero, sed placerat est dui non enim.\nVestibulum libero dolor, vehicula vitae risus non, consequat gravida neque. Maecenas a posuere sem. Quisque dignissim porta pretium. Nam mattis lacus commodo lobortis consequat. Vestibulum at massa a quam egestas accumsan. Fusce ac neque eu libero maximus aliquet id quis metus. In sodales neque ut turpis iaculis vehicula. Sed volutpat, tellus non laoreet aliquet, risus felis ornare dolor, interdum venenatis turpis metus in lorem. Aliquam quam ligula, porttitor non ultrices ac, lacinia ullamcorper massa. Integer molestie eleifend urna, imperdiet dignissim tellus malesuada ac. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam tristique sem ac purus interdum, vitae pharetra erat fringilla. Integer non nunc a eros porttitor rutrum. Suspendisse ut libero urna. Vestibulum vitae est at diam vestibulum aliquet. Donec porta nunc eget lobortis mollis.\nVestibulum luctus sodales odio, at luctus lacus lobortis a. Aliquam vulputate id turpis sit amet bibendum. Donec a dignissim tellus, vel vehicula erat. Pellentesque hendrerit ex magna, at dictum est pretium a. Vivamus faucibus ipsum lectus, ut elementum diam interdum scelerisque. Cras magna sapien, tincidunt quis eleifend at, tincidunt non eros. Aliquam ac augue turpis."
  },
  {
    "objectID": "src/qse/02_scanning.html",
    "href": "src/qse/02_scanning.html",
    "title": "Scanning",
    "section": "",
    "text": "import { Tabs, TabItem } from ‘@astrojs/starlight/components’;",
    "crumbs": [
      "QSE",
      "Scanning"
    ]
  },
  {
    "objectID": "src/qse/02_scanning.html#scanning-in-vscode",
    "href": "src/qse/02_scanning.html#scanning-in-vscode",
    "title": "Scanning",
    "section": "Scanning in VSCode",
    "text": "Scanning in VSCode\n:::note[Useful Links]\nInitiating scans as described here\n:::\n\nKicking off the scan and seeing results\nLet’s scan the code base for the cli for QSE\nIn our example, we’ve created a directory called QSE and extracted all the installation elements of QSE.\n\n\n\nScanning1\n\n\nOpen that directory in VSCode.\ncd QSE\n\ncode .\n\n\n\nScanning2\n\n\nOn VSCode, open a new terminal in the code window. This can be done with the following key combo: Ctrl-Shift-Backtick(`)\n\n\n\nScanning3\n\n\nIn windows this will open a powershell terminal\nPress Ctlr-Shift-P to open the Command Palette and search for Quantum Safe Explorer. Select Quantum Safe Explorer Scan.\n\n\n\nScanning4\n\n\nThe following notification should pop up\n\n\n\nScanning5\n\n\nLet it scan until it completes.\nWhen the scan completes, you should see the following directories show up in the Explorer window of VSCode\n\n\n\nScanning6\n\n\nWe should now see the QUANTUM SAFE EXPLORER DASHBOARD populated with some graphs and charts.\n\n\n\nScanning7",
    "crumbs": [
      "QSE",
      "Scanning"
    ]
  },
  {
    "objectID": "src/qse/02_scanning.html#executing-the-ibm-quantum-safe-explorer-cli",
    "href": "src/qse/02_scanning.html#executing-the-ibm-quantum-safe-explorer-cli",
    "title": "Scanning",
    "section": "Executing the IBM Quantum Safe Explorer CLI",
    "text": "Executing the IBM Quantum Safe Explorer CLI\n\nMake sure that Git Bash is installed and use a Git Bash terminal window for the instructions below.\n\nChange directory to M0GT6EN where ever you had it extracted and run the following command:\n/usr/bin/sed -i 's/^RSP_LICENSE_ACCEPTED=.*/RSP_LICENSE_ACCEPTED=true/' LicenseAcceptance.config\nTo make sure you have the correct permissions to run the shell script first run:\nchmod +x cli.sh\nNext, run the following help command\n./cli.sh -h \n\nNote:\nThe run will also point out that you have already accepted the licensing agreement.\n\nBefore you officially perform the script run, it is helpful to know the list of required parameters:\n\n\n\nQSEmacos1\n\n\nAs well as the optional parameters:\n\n\n\nQSEmacos2\n\n\nYou initiate a scan using the required program package path. The command can be run for single or multiple languages. Here is an example:\n/cli.sh -i &lt;package-path&gt; -clean -nmo -ccdir resource:class-catalog -l .java\n\nNote:\nIf you are scanning more than one language, then the command needs to be run in double quotation marks. Use the Help command by running ./cli.sh -h for more details on the various parameters.\n\nHere is an example of a completed scan with a specified program package path:\n$ ./cli.sh -i ~/Virtualenvs/java-projects -clean -nmo -ccdir resource:class-catalog -l .java\nLicense Agreement have been already completed.\ncanonical pathname &gt;&gt; /Users/gerald.trotmanibm.com/Virtualenvs/java-projects\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\nbuild_class_catalog  : \nclass-catalogs_directory : resource:class-catalog\nclear_results        : \ndart_class_catalog   : ../quantum-safe-sca-tng/class-catalog/dart-class-catalog.json\ndata_analytics       : false\ndata_mine_class_catalog : \nenable_detail_log    : \nexact_match          : true\ninput_folder         : /Users/gerald.trotmanibm.com/Virtualenvs/java-projects\ninternal_utility     : \njava_class_catalog   : ../quantum-safe-sca-tng/class-catalog/java-class-catalog.json\nlanguage_list        : .java\nlibrary_usage        : \nname_matching_only   : \noutput_folder        : /Users/gerald.trotmanibm.com/Virtualenvs/java-projects/qs_scan_result\npath_exclusion_filter : \npath_filter          : \npython_class_catalog : ../quantum-safe-sca-tng/class-catalog/python-class-catalog.json\nregression_test      : \nsingle_steps         : multi-language-selection\nsource_filter        : \ntab_size             : 4\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\nVersion 1.23.0.1065 -- Generated On Tue Jul 23 13:22:12 PDT 2024\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n+                PARAMETERS SENT TO SCA AND ANALYTICS\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n+ error-log-file                          : qs_scan_result/log/multi-language-err.log\n+ parameter-class-catalogs-directory      : resource:class-catalog\n+ parameter-demo-mode                     : true\n+ parameter-detail-results                : true\n+ parameter-language-processing-list      : .java\n+ parameter-library-selection             : pycrypto\n+ parameter-path-exclusion-filter         : \n+ parameter-path-filter                   : \n+ parameter-run-single-step               : delete-project-root,crypto-name-matching-only,multi-language-selection\n+ parameter-source-filter                 : \n+ parameter-source-filter-exact-match     : true\n+ parameter-tab-size                      : 4\n+ parameter-use-absolute-source-paths     : true\n+ project-root                            : qs_scan_result\n+ root                                    : /Users/gerald.trotmanibm.com/Virtualenvs/java-projects\n+ run-log-file                            : qs_scan_result/log/multi-language.log\n+ source-dart-class-catalog-json          : ../quantum-safe-sca-tng/class-catalog/dart-class-catalog.json\n+ source-java-class-catalog-json          : ../quantum-safe-sca-tng/class-catalog/java-class-catalog.json\n+ source-knowledge-base-db                : kb/cd_kb.db\n+ source-python-class-catalog-json        : ../quantum-safe-sca-tng/class-catalog/python-class-catalog.json\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n[23/07/2024 13:22:12.119] Setting Log File To           : /Users/gerald.trotmanibm.com/Virtualenvs/java-projects/qs_scan_result/log/multi-language.log\n[23/07/2024 13:22:12.121] Setting Error File To         : /Users/gerald.trotmanibm.com/Virtualenvs/java-projects/qs_scan_result/log/multi-language-err.log\n[23/07/2024 13:22:12.125] Running Task                  : com.ibm.quantumsafe.sca.lang.base.task.impl.LoadCryptoKBImpl\n[23/07/2024 13:22:12.544] Running Task                  : com.ibm.quantumsafe.sca.lang.general.task.impl.DeleteProjectRootDirectoryImpl\n[23/07/2024 13:22:12.573] Running Task                  : com.ibm.quantumsafe.sca.lang.general.task.impl.ClearScanResultsImpl\n[23/07/2024 13:22:12.611] Running Task                  : com.ibm.quantumsafe.sca.lang.java.task.impl.JavaPreprocessImpl\n[23/07/2024 13:22:12.639] Running Task                  : com.ibm.quantumsafe.sca.lang.java.task.impl.JavaTokenizeImpl\n[23/07/2024 13:22:12.671] Running Task                  : com.ibm.quantumsafe.sca.reporting.task.impl.AddLineNumbersImpl\n[23/07/2024 13:22:12.695] Running Task                  : com.ibm.quantumsafe.sca.lang.java.task.impl.JavaStructureImpl\n[23/07/2024 13:22:12.732] Running Task                  : com.ibm.quantumsafe.sca.framework.common.BuildLocalClassCatalogImpl\n[23/07/2024 13:22:13.191] Class Catalog Size            : 11010\n[23/07/2024 13:22:15.175] Running Task                  : com.ibm.quantumsafe.sca.lang.java.task.impl.ExternalizeNestedCallsmpl\n[23/07/2024 13:22:15.198] Running Task                  : com.ibm.quantumsafe.sca.framework.common.JavaBuildGraphModelImpl\n[23/07/2024 13:22:15.234] Running Task                  : com.ibm.quantumsafe.sca.reporting.task.impl.BuildRcgSummaryAndCBOMFromModelImpl\n[23/07/2024 13:22:15.253] Running Task                  : com.ibm.quantumsafe.sca.framework.common.AnalyzeClasssCatalogImpl\n[23/07/2024 13:22:15.739] Running Task                  : com.ibm.quantumsafe.sca.reporting.task.impl.BuildDashboardFromFindings\n[23/07/2024 13:22:15.745] Running Task                  : com.ibm.quantumsafe.sca.lang.general.task.impl.SendScanResultsImpl\n[23/07/2024 13:22:15.756] Running Task                  : com.ibm.quantumsafe.sca.framework.common.impl.ProfileProcessImpl\n\nNote: When scanning large applications (e.g., more than 500,000 lines of code) on macOS systems, IBM Quantum Safe Explorer consumes a large amount of disk space. It is advised to archive and clear the results folder and to reboot the machine before initiating a scan on another application.",
    "crumbs": [
      "QSE",
      "Scanning"
    ]
  },
  {
    "objectID": "src/qse/02_scanning.html#accessing-the-ibm-quantum-safe-explorer-cli-scan-results",
    "href": "src/qse/02_scanning.html#accessing-the-ibm-quantum-safe-explorer-cli-scan-results",
    "title": "Scanning",
    "section": "Accessing the IBM Quantum Safe Explorer CLI scan results",
    "text": "Accessing the IBM Quantum Safe Explorer CLI scan results\nThe results of your scan are written to a findings.json file and can be found nested within your project path. It resemble the following:\n/&lt;package-path&gt;/qs_scan_result/scan-results/\nThe file is quite verbose. Here’s a snippet of the result:\n\n\n\nAccessing1",
    "crumbs": [
      "QSE",
      "Scanning"
    ]
  },
  {
    "objectID": "src/qse/02_scanning.html#viewing-the-cli-scan-results-in-the-ibm-quantum-safe-explorer-visual-studio-code-extension",
    "href": "src/qse/02_scanning.html#viewing-the-cli-scan-results-in-the-ibm-quantum-safe-explorer-visual-studio-code-extension",
    "title": "Scanning",
    "section": "Viewing the CLI scan results in the IBM Quantum Safe Explorer Visual Studio Code Extension",
    "text": "Viewing the CLI scan results in the IBM Quantum Safe Explorer Visual Studio Code Extension\n\nOpening the scanned folder\nAs of IBM Quantum Safe Explorer 1.0.1 release, you can now view the CLI scan results in Visual Studio Code.\nWhen you execute a scan, a qs_explorer_result folder is generated along with the qs_scan_result folder.\n\n\n\nOpening1\n\n\n\nNote: To view the results, you need to have the Visual Studio Code extension installed on your system.\n\nNavigate to the File menu drop down, select the Open Folder to locate your /&lt;package-path&gt; folder:\n\n\n\nOpening2\n\n\nOnce you have found your /&lt;package-path&gt; simply open it to populate the project in the Visual Studio Code Explorer view:\n\n\n\nOpening3\n\n\nTo populate the SCAN RESULTS, we must first navigate to the QUANTUM SAFE EXPLORER tab. This then populates the SCAN RESULTS menu. That menu drop down then brings you to the Crypto Artifacts. For example:\n\n\n\nOpening4\n\n\nTo view the results dashboard, you simply toggle to the QUANTUM SAFE EXPLORER DASHBOARD tab and click the + symbol on the right hand side to expand the view:\n\n\n\nOpening5\n\n\nThis should then expose the dashboard as shown below:\n\n\n\nOpening6",
    "crumbs": [
      "QSE",
      "Scanning"
    ]
  }
]