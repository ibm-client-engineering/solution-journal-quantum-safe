---
title: Optional Services
format:
  html:
    toc: true
lightbox: true
date: last-modified
---

## Export the following vars. 

These should be relevant to your environment, therefore clustername should be your clustername and same with your hostname. Below this is an example.

```bash
export clustername=gi-east
export region=us-east-1
export NAMESPACE=openshift-marketplace
export HOSTNAME=apps.gi.thinkforward.work
```

## (Optional) Install the ALB ingress controller

Unlike NGINX Ingress, ALB generates a load balancer when a external service is created in the cluster. So DNS is not necessary. See example [here](https://docs.aws.amazon.com/eks/latest/userguide/alb-ingress.html#application-load-balancer-sample-application).

Download an IAM policy for the AWS Load Balancer Controller that allows it to make calls to AWS APIs on your behalf.

::: {.callout-note}

As of this writing, the latest version of AWS Load Balancer Controller is [v2.9.0](https://github.com/kubernetes-sigs/aws-load-balancer-controller/releases/tag/v2.9.0)

:::

```bash
curl -O https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.9.0/docs/install/iam_policy.json
```

Create an IAM policy using the policy downloaded in the previous step. 

```bash
aws iam create-policy \
    --policy-name AWSLoadBalancerControllerIAMPolicy \
    --tags '{"Key": "Product","Value": "Guardium"}' \
    --policy-document file://iam_policy.json
```

Should return

```bash
{
    "Policy": {
        "PolicyName": "AWSLoadBalancerControllerIAMPolicy",
        "PolicyId": "ANPA3WENOYSATHNEI5OIR",
        "Arn": "arn:aws:iam::<ACCOUNT ID>:policy/AWSLoadBalancerControllerIAMPolicy",
        "Path": "/",
        "DefaultVersionId": "v1",
        "AttachmentCount": 0,
        "PermissionsBoundaryUsageCount": 0,
        "IsAttachable": true,
        "CreateDate": "2024-09-30T21:08:03+00:00",
        "UpdateDate": "2024-09-30T21:08:03+00:00",
        "Tags": [
            {
                "Key": "Product",
                "Value": "Guardium"
            }
        ]
    }
}

```

Letâ€™s export that policy arn as another env var

```bash
export alb_policy_arn=$(aws iam list-policies --query 'Policies[?PolicyName==`AWSLoadBalancerControllerIAMPolicy`].Arn' --output text)
```

Export the role name as a env var. We're going to append the cluster name to the role name to help identify it in AWS and in case we have multiple clusters in the same account.
```bash
export alb_role_name=AWSLoadBalancerControllerRole-${clustername}
```

Create a Kubernetes service account named aws-load-balancer-controller in the kube-system namespace for the AWS Load Balancer Controller and annotate the Kubernetes service account with the name of the IAM role.

```bash
eksctl create iamserviceaccount \
    --cluster ${clustername} \
    --namespace kube-system \
    --name aws-load-balancer-controller \
    --role-name ${alb_role_name} \
    --attach-policy-arn ${alb_policy_arn} \
    --tags "Product=Guardium" \
    --approve \
    --region ${region}

```

Now let's use helm to install the AWS Load Balancer Controller

Install the helm repo

```bash
helm repo add eks https://aws.github.io/eks-charts
helm repo update
```

Now install the ALB controller

```bash
helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
  -n kube-system \
  --set clusterName=${clustername} \
  --set serviceAccount.create=false \
  --set serviceAccount.name=aws-load-balancer-controller 

```

Should return something like this
```bash
NAME: aws-load-balancer-controller
LAST DEPLOYED: Mon Sep 30 17:21:43 2024
NAMESPACE: kube-system
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
AWS Load Balancer controller installed!
```

Verify the ingress class has been created

```bash
kubectl get ingressclass
```

should return

```bash {2}
NAME    CONTROLLER             PARAMETERS   AGE
alb     ingress.k8s.aws/alb    <none>       79s
nginx   k8s.io/ingress-nginx   <none>       35d
```

## Community Cert Manager (optional)

### Installing community Cert Manager

Install the helm repo
```bash
helm repo add jetstack https://charts.jetstack.io
```

Create the namespace
```bash
kubectl create ns cert-manager
```

Now install the Community Cert Manager

```bash
helm install cert-manager jetstack/cert-manager \
--namespace cert-manager \
--set installCRDs=true \
--version v1.16.1
```


## Configure Security Constraints (Optional)

For some environments there might be security constraints applied to the EKS cluster. Follow
these instructions to apply security constraints using OPA gatekeeper.

If the operating environment is a production or pre-production enterprise environment, then
you can skip this section as it's meant for development environments.

### Install OPA Gatekeeper

Deploy OPA Gatekeeper using prebuilt docker images

::: {.callout-note}
By default, the violations audit runs every 60 seconds and will report a maximum of 20 audits in the constraint yaml.
If you want to increase the number of violations shown on the constraint yaml to 500 (not recommended to increase further),
update the gatekeeper.yaml below before applying and add the following argument to the audit deployment.

```yaml
  template:
    metadata:
      labels:
        control-plane: audit-controller
        gatekeeper.sh/operation: audit
        gatekeeper.sh/system: "yes"
    spec:
      automountServiceAccountToken: true
      containers:
      - args:
        - --operation=audit
        - --operation=status
        - --operation=mutation-status
        - --logtostderr
        - --disable-opa-builtin={http.send}
        - --disable-cert-rotation
        - --constraint-violations-limit=500
```
:::

```bash
kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper/release-3.17/deploy/gatekeeper.yaml
```

Check the pods in gatekeeper-system namespace

```bash
kubectl get pods -n gatekeeper-system
```

The output will be similar to:

```bash
NAME                                             READY   STATUS    RESTARTS   AGE
gatekeeper-audit-5bc9b59c57-9d9hc                1/1     Running   0          25s
gatekeeper-controller-manager-744cdc8556-hxf2n   1/1     Running   0          25s
gatekeeper-controller-manager-744cdc8556-jn42m   1/1     Running   0          25s
gatekeeper-controller-manager-744cdc8556-wwrb6   1/1     Running   0          25s
```

### Install kustomize
To ease in the deployment of the templates and constraints, install [kustomize](https://kubectl.docs.kubernetes.io/installation/kustomize/).

The following script detects your OS and downloads the appropriate kustomize binary to your current working directory.

```bash
curl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh"  | bash
```

### Apply Templates

There are 2 parts to OPA gatekeeper policy enforcment, the first part is the `ConstraintTemplate`.
This is where the logic of the policy exists. The templates must be deployed prior to deploying the 
constraints.

If you haven't yet, clone this github repository.

```bash
git clone https://github.com/ibm-client-engineering/engineering-journal-quantum-safe.git
```

Go to the opa library templates directory.

```bash
cd engineering-journal-quantum-safe/opa/library/templates
```

Apply the templates using `kustomize`

```bash
kustomize build . | kubectl apply -f -
```

Sample output

```bash
constrainttemplate.templates.gatekeeper.sh/k8sallowedrepos created
constrainttemplate.templates.gatekeeper.sh/k8sblockendpointeditdefaultrole created
constrainttemplate.templates.gatekeeper.sh/k8sblockloadbalancer created
constrainttemplate.templates.gatekeeper.sh/k8sblocknodeport created
constrainttemplate.templates.gatekeeper.sh/k8sblockwildcardingress created
constrainttemplate.templates.gatekeeper.sh/k8sdisallowanonymous created
constrainttemplate.templates.gatekeeper.sh/k8sdisallowedrepos created
constrainttemplate.templates.gatekeeper.sh/k8sdisallowedtags created
constrainttemplate.templates.gatekeeper.sh/k8sdisallowinteractivetty created
constrainttemplate.templates.gatekeeper.sh/k8sexternalips created
constrainttemplate.templates.gatekeeper.sh/k8shttpsonly created
constrainttemplate.templates.gatekeeper.sh/k8simagedigests created
constrainttemplate.templates.gatekeeper.sh/k8spoddisruptionbudget created
constrainttemplate.templates.gatekeeper.sh/k8spspautomountserviceaccounttokenpod created
constrainttemplate.templates.gatekeeper.sh/k8srequiredresources created
constrainttemplate.templates.gatekeeper.sh/k8sstorageclass created
constrainttemplate.templates.gatekeeper.sh/k8suniqueingresshost created
constrainttemplate.templates.gatekeeper.sh/k8suniqueserviceselector created
```

### Apply Constraints

The 2nd part to OPA gatekeeper policy enforcment is the `Constraint`. The constraints
are a specific application of a template. This instructs the OPA gatekeeper to constrain
a resource according to specific parameters and applying one of the templates. For example,
a constraint can specify that every namespace must have a specific label applied.

Part of a constraint definition is the enforcement action. By default, OPA gatekeeper will enforce
constraints by denying the action. For the constraints here, we are using `dryrun`, which
does not deny the resources that violate constraints, but rather just logs the violation. This
allows us to see what resources are violating the constraints without the resources being denied.

Go to the opa library constraints directory.

```bash
cd engineering-journal-quantum-safe/opa/library/constraints
```

Apply the templates using `kustomize`

```bash
kustomize build . | kubectl apply -f -
```

Sample output

```bash
k8sallowedrepos.constraints.gatekeeper.sh/repo-is-amazonaws created
k8sallowedrepos.constraints.gatekeeper.sh/repo-is-openpolicyagent created
k8sblockendpointeditdefaultrole.constraints.gatekeeper.sh/block-endpoint-edit-default-role created
k8sblockloadbalancer.constraints.gatekeeper.sh/block-load-balancer created
k8sblocknodeport.constraints.gatekeeper.sh/block-node-port created
k8sblockwildcardingress.constraints.gatekeeper.sh/block-wildcard-ingress created
k8sdisallowanonymous.constraints.gatekeeper.sh/no-anonymous-no-authenticated created
k8sdisallowinteractivetty.constraints.gatekeeper.sh/no-interactive-tty-containers created
k8sdisallowedrepos.constraints.gatekeeper.sh/repo-must-not-be-k8s-gcr-io created
k8sdisallowedtags.constraints.gatekeeper.sh/container-image-must-not-have-latest-tag created
k8sexternalips.constraints.gatekeeper.sh/external-ips created
k8shttpsonly.constraints.gatekeeper.sh/ingress-https-only created
k8simagedigests.constraints.gatekeeper.sh/container-image-must-have-digest created
k8spspautomountserviceaccounttokenpod.constraints.gatekeeper.sh/psp-automount-serviceaccount-token-pod created
k8spoddisruptionbudget.constraints.gatekeeper.sh/pod-distruption-budget created
k8srequiredresources.constraints.gatekeeper.sh/container-must-have-limits-and-requests created
k8suniqueingresshost.constraints.gatekeeper.sh/unique-ingress-host created
```


### List Violations

Use the following command to list the violations.

```bash
kubectl get constraints
```

Sample output

```bash
NAME                                                                                     ENFORCEMENT-ACTION   TOTAL-VIOLATIONS
k8srequiredresources.constraints.gatekeeper.sh/container-must-have-limits-and-requests   dryrun               4

NAME                                                                 ENFORCEMENT-ACTION   TOTAL-VIOLATIONS
k8suniqueingresshost.constraints.gatekeeper.sh/unique-ingress-host   dryrun               0

NAME                                                                                ENFORCEMENT-ACTION   TOTAL-VIOLATIONS
k8sdisallowinteractivetty.constraints.gatekeeper.sh/no-interactive-tty-containers   dryrun               0

NAME                                                        ENFORCEMENT-ACTION   TOTAL-VIOLATIONS
k8shttpsonly.constraints.gatekeeper.sh/ingress-https-only   dryrun               0

NAME                                                                                         ENFORCEMENT-ACTION   TOTAL-VIOLATIONS
k8sblockendpointeditdefaultrole.constraints.gatekeeper.sh/block-endpoint-edit-default-role   dryrun               0

NAME                                                                       ENFORCEMENT-ACTION   TOTAL-VIOLATIONS
k8sdisallowedrepos.constraints.gatekeeper.sh/repo-must-not-be-k8s-gcr-io   dryrun               0

NAME                                                                                   ENFORCEMENT-ACTION   TOTAL-VIOLATIONS
k8sdisallowedtags.constraints.gatekeeper.sh/container-image-must-not-have-latest-tag   dryrun               0

NAME                                                         ENFORCEMENT-ACTION   TOTAL-VIOLATIONS
k8sblocknodeport.constraints.gatekeeper.sh/block-node-port   dryrun               0
```

### Violation Details

To see the details of the violations, you can look at the constraint yaml. For example, to see the 
`container-must-have-limits-and-requests` constraint violations, run the following command.

```bash
kubectl get k8srequiredresources container-must-have-limits-and-requests -o yaml
```

Near the end of the yaml output, you will see violations listed.

```yaml
  violations:
  - enforcementAction: dryrun
    group: ""
    kind: Pod
    message: container <manager> does not have <{"cpu"}> limits defined
    name: gatekeeper-controller-manager-865cc64485-kgkkj
    namespace: gatekeeper-system
    version: v1
  - enforcementAction: dryrun
    group: ""
    kind: Pod
    message: container <manager> does not have <{"cpu"}> limits defined
    name: gatekeeper-controller-manager-865cc64485-h2x7d
    namespace: gatekeeper-system
    version: v1
  - enforcementAction: dryrun
    group: ""
    kind: Pod
    message: container <manager> does not have <{"cpu"}> limits defined
    name: gatekeeper-controller-manager-865cc64485-g4b72
    namespace: gatekeeper-system
    version: v1
  - enforcementAction: dryrun
    group: ""
    kind: Pod
    message: container <manager> does not have <{"cpu"}> limits defined
    name: gatekeeper-audit-c794d5f69-s2nmd
    namespace: gatekeeper-system
    version: v1
```

If you want a CSV of all of the violations (up to the constraints violation limit set at install),
run the following command. Note that the header will be repeated.

```bash
kubectl get constraints -o json | yq -o=json eval '.items[].status.violations' | jq -r '(.[0] | keys_unsorted) as $keys | $keys, map([.[ $keys[] ]])[] | @csv' > violations.csv 2>/dev/null
```

## Configure OPA Mutations

::: {.callout-note}
If you are deploying to an **EKS** environment that is airgapped and using a private registry, the following mutations would need to be configured for the environment. This is not applicable to an Openshift environment as that can use an `ImageContentSourcePolicy`
:::

These mutations essentially work like an Openshift `ImageContentSourcePolicy` and rewrite the domain paths in pods.

Export the registry domain. This should be the url of your private registry. `my.registry.io` is an example.


```bash
export myprivatereg=my.registry.io
```

Apply the mutations

```bash
cat <<EOF | kubectl apply -f -
apiVersion: mutations.gatekeeper.sh/v1alpha1
kind: AssignImage
metadata:
  name: assign-container-domain
spec:
  applyTo:
  - groups: [ "" ]
    kinds: [ "Pod" ]
    versions: [ "v1" ]
  location: "spec.containers[name:*].image"
  parameters:
    assignDomain: "${myprivatereg}"
  match:
    source: "All"
    scope: Namespaced
    namespaces:
      - openshift-marketplace
      - ibm-cert-manager
    kinds:
    - apiGroups: [ "*" ]
      kinds: [ "Pod" ]
---
apiVersion: mutations.gatekeeper.sh/v1alpha1
kind: AssignImage
metadata:
  name: assign-initcontainer-domain
spec:
  applyTo:
  - groups: [ "" ]
    kinds: [ "Pod" ]
    versions: [ "v1" ]
  location: "spec.initContainers[name:*].image"
  parameters:
    assignDomain: "${myprivatereg}"
  match:
    source: "All"
    scope: Namespaced
    namespaces:
      - openshift-marketplace
      - ibm-cert-manager
    kinds:
    - apiGroups: [ "*" ]
      kinds: [ "Pod" ]
---
apiVersion: mutations.gatekeeper.sh/v1alpha1
kind: AssignImage
metadata:
  name: assign-ephemeralcontainer-domain
spec:
  applyTo:
  - groups: [ "" ]
    kinds: [ "Pod" ]
    versions: [ "v1" ]
  location: "spec.ephemeralContainers[name:*].image"
  parameters:
    assignDomain: "${myprivatereg}"
  match:
    source: "All"
    scope: Namespaced
    namespaces:
      - openshift-marketplace
      - ibm-cert-manager
    kinds:
    - apiGroups: [ "*" ]
      kinds: [ "Pod" ]
EOF
```